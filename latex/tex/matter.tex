\chapter{Machinelles Lernen}

Der Themenbereich des \keyword{Machinellen Lernens} beschäftigt sich mit Algorithmen und mathematischen Modellen, welche von selber lernen, Probleme zu lösen.
Hierbei wird nicht explizit einprogrammiert, wie das Modell das Problem zu lösen
hat, stattdessen wird das Modell trainiert, optimiert sich von selbst und findet selber
einen Weg, das Problem zu lösen.
Die Grundidee dabei ist, dass man Daten erfasst, generiert oder misst, welche
analysiert werden sollen. Innerhalb dieser Daten existieren gewisse
Gesetzmässigkeiten und Muster. Diese Muster sollen vom Modell
erkannt werden und verallgemeinert werden. Nach dem erfolgreichen Lernen,
kann das Modell Vorhersagen für neuen Daten machen.
\para{}
Man unterscheidet zwischen zwei Arten von Maschinellem Lernen.
\begin{itemize}
\item{
    \keyword{Überwachtes Lernen} (engl.:\ supervised learning) ist ein
    Lernverfahren, bei welchem die Daten aus zwei Teilen bestehen, aus Inputs und
    Outputs. Man bezeichnet dabei die Outputs als Labels. Die Aufgabe des Modells
    ist es eine \keyword{Korrelation} zwischen den Inputs und den Labels zu
    erlernen und so ihre Beziehung züinander zu verstehen.
    Anhand der Informationen, welche in den Inputs enthalten
    sind, sind dann die Labels vorherzusagen. Man gleicht dann die
    vorhergesagten Labels des Modells mit den wahren Labels ab (man
    ``überwacht'' sie) und bewertet so die Fähigkeiten des Modells..
    \para{}
    Voraussetztung dafür ist, dass die Daten eben ``gelabelt'' sein müssen.
    Man muss schon im vorhinein Daten haben, bei welchen man die gewünschten
    Labels hat. Natürlich muss auch die erwähnte Korrelation bestehen. Falls
    kein Zusammenhang zwischen den Inputs und den Outputs besteht, kann das
    Modell auch keine Vorhersagen machen und so auch nichts erlernen.
  }
\item{
    \keyword{Unüberwachtes Lernen} (engl.:\ unsupervised learning) ist ein anderes
    Lernverfahren, bei welchem eben diese Labels nicht vorhanden sind. Dem
    Modell stehen nur die Inputdaten zur Verfügung. Diese werden ebenfalls analysiert
    und das Modell soll Muster herausextrahieren, welche sich von sonstigem
    züfälligen Rauschen unterscheiden.
  }
\end{itemize}

Der Grossteil der Modelle des Maschinellen Lernens benutzen überwachtes
Lernen, da es auch deutlich mehr Anwendungsmöglichkeiten besitzt. Es gibt nur wenige Arten von
unüberwachtem Lernen, dazu gehören: Hauptkomponentenanalysen und Generative
Adversial Networks. Aus diesem Grund, werden wir im folgenden uns grundsätzlich
mit überwachtem Lernen beschäftigen.
\para{}
\cite{wiki:supervised_learning}
\cite{wiki:unsupervised_learning}

\section{Allgemeine Begriffe}

\subsection{Daten}

Um ein Modell zu trainieren, braucht man Daten. Diese Daten bestehen immer aus
\keyword{Inputs} und \keyword{Outputs}. Man unterscheidet hierbei zwischen zwei Arten
von Outputs. Die \keyword{Labels} sind die erwarteten Outputs, welche die
gewüschten Zielwerte sind. Die \keyword{Vorhersagen} sind die Outputs, welche
das Modell produziert und hoffentlich möglichst genau mit den Labels
übereinstimmen.
\para{}
Diese Daten für das Training kommen in der Form eines \keyword{Trainingsdatensatzes} $\set{X}$.
Dabei handelt es sich um eine Menge von \keyword{Samples},
welche jeweils aus Inputs und Labels bestehen.
Die Inputs werden in einem Vektor
\[ \vec{x} = \trans{\begin{pmatrix} x_1 & x_2 & \cdots & x_m \end{pmatrix}} \]
und die Labels in einem Vektor
\[ \vec{\hat{y}} = \trans{\begin{pmatrix} \hat{y}_1 & \hat{y_2} & \cdots & \hat{y}_n \end{pmatrix}} \]
zusammengefasst.Somit ist ein Trainingssample nichts anderes als ein Paar
$(\vec{x}_i,\vec{\hat{y}}_i)$ eines Inputsvektor $\vec{x}$ und eines Labelvektor
$\vec{\hat{y}}$.
Die Vorhersagen werden ebenfalls in einem Vektor
\[\vec{y} = \trans{\begin{pmatrix} y_1 \quad y_2 \quad \cdots \quad y_n \end{pmatrix}} \]
zusammengefasst.
\para{}
Die Inputs beinhalten sogennanten \keyword{Features} (deutsch: Merkmale). Sie
zeichnen die Inputs aus und umfassen ihren ganzen Informationsgehalt.
Der Algorithmus soll
anhand dieser Features seine Vorhersagen machen.
Diese Vorhersagen werden dann mit den Labels abgeglichen und so bewertet.
Anhand der Bewertungen wird dann eine Optimierung des Modells vorgenommen.
Unter korrekten Bedingungen (kein Overfitting) findet kein Auswendiglernen der Trainingsdaten statt,
sondern ein Generalisieren des Zusammenhangs anhand von Mustern und Gesetzmassigkeiten.
\para{}
Um eine endgültige Bewertung des Modells durchzuführen, wird ein Testdatensatz
$\set{T}$, welcher nicht Teil des Trainingsdatensatz ist, genutzt, um Vorhersagen zu machen.
Dies garantiert, dass kein Auswendiglernen möglich ist.
\para{}
BEISPIEL


\subsection{Modelle}
Ein \keyword{Modell} ist eine mathematische Funktion $\mathit{h}\colon \set{R}^m
\to \set{R}^n$, Hypothesenfunktion genannt, welche die Inputs auf die Outputs abbildet $\vec{y}=\mathit{h}(\vec{x})$.
Man kann diese Funktion gewissermassen als die Hypothese, welche das Modell bezüglich der Beziehung zwischen
den Inputs und den Labels aufgestellt hat, betrachten.
Diese Modelle können verwendet werden um vorallem entweder
Klassifizierungsprobleme oder Regressionsprobleme zu lösen. Falls es sich um
letzteres handelt, spricht man von einem Regressionsmodell. Ein solches
Regressionsproblem werden wir in dieser Arbeit lösen.
\para{}
Das Verhalten eines Modells wird bestimmt durch seine \keyword{Modellparameter}
$\param_1, \param_2,\ldots,\param_k$. Sie bestimmen, wie die Hypothese des Modells lautet.
Das Ziel ist es, die Modellparameter so einzustellen, dass die Vorhersagen
$\vec{y}$ des Modells besser mit den Labels $\vec{\hat{y}}$ der Trainingsdaten übereinstimmen.
Dies wird iterativ gemacht, indem immer wieder leichte Anpassungen an den
Parametern vorgenommen werden, bis das Modell die gewünschten Resultate liefert.
\para{}
Neben den gelernten Parametern, gibt es auch noch sogenannte \keyword{Hyperparameter}.
Diese können nicht erlernt werden, sondern müssen manüll vor dem Training gewählt werden und können den Lernvorgang erheblich beeinflussen.
Dies bedeutet, dass man ausprobieren muss, welche Werte für die Hyperparameter
die besten Resultate liefern.
\para{}
Das wohl einfachste Regressionsmodell ist eine Regressionsgerade. Diese ist
angemessen, falls ein einfacher linearen Zusammenhang der Form $y=\param_1x +
\param_0$ zwischen den Features und den Labels besteht.
Nun müssten nur noch $\param_0$ und $\param_1$ bestimmt werden.
BEISPIELMODELL AUTO
\para{}
Für Machine Learning haben sich gewisse Modelle besonders gut etabliert,
darunter wären: Support Vector Machines, Evolutionäre Algorithmen, und Künstliche Neuronale Netze.
Diese Arbeit wird sich vorwiegend mit Neuronalen Netzen auseinandersetzen.
\\
\begin{figure}[h!]
  \centering


  \caption{Schema eines Modells}
\end{figure}

\section{Training}
\subsection{Verlust- und Kostenfunktionen}
Einsicht ist der erste Schritt zur Besserung. Das gilt auch beim Machine Learning, deshalb muss beim Training zürst die Genauigkeit des Modells bewertet werden.
Dies wird mithilfe von sogennaten Kostenfunktionen bzw. Verlustfunktionen erreicht.
\para{}
Eine \keyword{Verlustfunktion} $L(y,\hat{y})$ soll ein Mass für die
Abweichung der Vorhersage $y$ von dem Label $\hat{y}$ sein.
Aus ihr bildet man die \keyword{Kostenfunktion} $C(\vec{y},\vec{\hat{y}})$, indem man die
Verluste der einzelnen Ouputs aufsummiert. Somit erhält man die Kosten
einer gesamten Vorhersage mit $m$ Outputs (siehe Gl. (\ref{eq:errorfunc})). Bei
gewissen Kostenfunktionen gibt es keine Verlustfunktion, da sie nicht auf die
einzelnen Outputpaare aufgebrochen werden kann. \\
Der Fehler $\bar{C}(\set{X})$ des gesamten Trainingsdatensatzes $\set{X}$, der
Grösse $p$, ergibt sich aus dem arithmetischen Mittel der einzelnen Kosten der
Vorhersagen (siehe Gl. (\ref{eq:meanerrorfunc})).
\\
\begin{minipage}[h!]{0.5\textwidth}
  \begin{equation}\label{eq:errorfunc}
    C \left(\vec{y},\vec{\hat{y}} \right) = \ds\sum_{i=1}^{m} L(y_i, \hat{y}_i)
  \end{equation}
\end{minipage}
\begin{minipage}[h!]{0.5\textwidth}
  \begin{equation}\label{eq:meanerrorfunc}
    \bar{C}(\set{X}) = \frac{1}{p}\ds\sum_{j=1}^{p} C\left(\vec{y}_j,\vec{\hat{y}}_j\right)
  \end{equation}
\end{minipage}
\para{}
Eine eine Kostenfunktion $C$ sollte folgende Eigenschaften aufweisen:
\begin{itemize}
\item{$C$ ist minimal, wenn $\vec{y} = \vec{\hat{y}}$}
\item{$C$ wächst mit $|\vec{y} - \vec{\hat{y}}|$}
\item{$C$ ist nach jedem $y_n$ partiell differenzierbar (erklärt in Sektion (\ref{ref:partielle_ableitungen}))}
\end{itemize}
\para{}

\cite{Nielsen}

\subsubsection{Mittlere quadratische Abweichung}
Die bekannteste Kostenfunktion ist die ``Mittlere quadratische Abweichung''.
(engl.:\ mean squared error). Sie ist definiert als das arithmetische Mittel
aller quadrierten Differenzen zwischen den Vorhersagen und Labels.
Zusätzlich halbiert man noch das arithmetische Mittel, damit bei der Ableitung der Faktor
$2$ wegfällt. Sie hat keine entsprechene Verlustfunktion, da man so das
arithemtische Mittel nicht schreiben kann.
\\
Die Kostenfunktion kann mithilfe einer Summe berechnen werden, oder in
diesem Fall mithilfe einer Vektorsubtraktion Vorhersagen $\vec{y}$ von den Labels $\vec{\hat{y}}$.
\\
\begin{equation}\label{eq:MSE}
  C_{MSE} = \frac{1}{2n}\sum_{i=1}^{n}{(\hat{y}_i - y_i)}^2 = \frac{1}{2n}{(\vec{\hat{y}} - \vec{y})}^2
\end{equation}
\\
Sie erfüllt alle Anforderungen an eine Kostenfunktion:
\begin{itemize}
\item{Ihr Funktionwert ist 0 und minimal für $\vec{y} = \vec{\hat{y}}$}
\item{Sie ist proportional zu ${(\vec{\hat{y}}-\vec{y})}^2$}
\item{Ihre partielle Ableitung nach einem $y_i$ lautet: $C_{MSE}'=\frac{1}{n}(y_i-\hat{y_i})$}
\end{itemize}


\subsubsection{Cross-entropy}


\subsection{Gradientenverfahren}\label{sec:gradientenverfahren}
% Um ein Modell nun zu trainieren, muss man verstehen, dass es sich um ein Optimierungsproblem handelt.
% Das Modell ist am besten, also macht die besten Vorraussagen, wenn die
% Funktionswerte der Kostenfunktion am kleinsten sind.
% Deshalb muss diese Kostenfunktion $C$ minimiert werden.
% Hierbei muss die Funktion $C$ nicht mehr in Abhängigkeit der Inputs und Outputs betrachtet
% werden, sondern als Funktion der Modellparameter
% $C(\param_1, \param_2, \ldots, \param_k)$, denn diese sollen angepasst werden,
% um das Modell zu verbessern.
% Für diese Optimiertung wird das sogennante \keyword{Gradientenverfahren} (engl.: Gradient descent) verwendet.
% \footnote{Im Gymnasium wird beigebracht die lokalen Extrema (inklusive lokalen Minimas) zu
%   bestimmen, indem die erste Ableitung $f'$ gebildet wird und gleich null gesetzt
%   wird.Dies ist hier nicht möglich, da die Funktion
%   $C'(\param_1,\param_2,\ldots,\param_n)$ deutlich zu komplex ist, um die
%   Nullstellen analytisch zu bestimmen. Deshalb wird das Gradientenverfahren
%   verwendet.
% }
% Allgemein können mithilfe des Gradientenverfahrens Funkionen $f(x_1, x_2, \ldots, x_n)$ vom Typ $\set{R}^n \to \set{R}$ (wie z.B. die Fehlerfunktion) minimiert werden.
% Dies geschieht, indem ein Startpunkt (Ortsvektor) $\vec{p}_0$, dessen
% Komponenten den Input von $f(\vec{p}_0)$ darstellen, gewählt wird.
% Nun werden iterativ neue Punkte $\vec{p}_t$ gesucht, welche immer näher beim lokalen Minimun liegen, also Punkte, die den Funktionswert $f(\vec{p}_t)$ immer kleiner werden lassen.
% Dies wird durchgeführt, bis der Punkt genügend nahe beim lokalen Minimun ist.
% \para{}
% Dafür muss ein Vektor $\vec{b}_t$ bestimmt werden, welcher auf den Punkt $\vec{p}_t$ addiert einen neuen Punkt $\vec{p}_{t+1}$ bildet,
% bei dem der Funktionswert $f(\vec{p}_{t+1})$ kleiner ist als der von $f(\vec{p}_t)$.
Dies geschieht am effizientesten, wenn $\vec{b}_t$ in die Richtung der stärksten Funktionswertabnahme zeigt.

Hierzu braucht man den sogennanten \keyword{Gradient} $\vecf{\nabla}$, wofür man wiederum partielle Ableitungen braucht.
\para{}
\begin{defbox}{Partielle Ableitungen}\label{ref:partielle_ableitungen}
  Partielle Ableitungen sind eine Erweiterung der ``normalen'' Ableitungen auf
  multidimensionale Funktionen $f(\vec{x}) = f(x_1,\ldots,x_n): \set{R}^n \to \set{R}$.
  Man leitet dabei nur nach einem Argument $x_i$ ab und betrachtet die restlichen Argumente als Konstanten.
  Es gelten die gleichen Ableitungsregeln wie bei der nicht-partiellen Ableitung.
  Die partielle Ableitung einer Funktion $f(x_1,\ldots,x_n)$ bezüglich einer
  Variable $x_i$ in einem Punkt $\vec{a} = \trans{\begin{pmatrix} a_1 & \cdots & a_n \end{pmatrix}}$
  ist analog zur normalen Ableitung folgendermassen definiert:
  \begin{equation*}
    \partderiv{f}{x_i}(\vec{a}) \coloneqq \lim_{h \to 0} \frac{f(a_1,\ldots,a_i + h,\ldots,a_n)-f(a_1,\ldots,a_i,\ldots,a_n)}{h}
  \end{equation*}
  Geometrisch ist dies die Steigung der Tangente an die Funktion $f$ im Punkt
  $\vec{a}$. Die Tangente liegt in der Richtung der Achse des Parametern $x_i$, nach dem man ableitet.
\end{defbox}
\\
\begin{defbox}{Gradient}
  Der Gradient $\vecf{\nabla}$ ist ein Differentialoperator, den man auf eine
  Funktion $f(\vec{x}) = f(x_1,\ldots,x_n): \set{R}^n \to \set{R}$ anwendet kann, welcher ein Skalarfeld auf ein Vektorfeld (das sogennante Gradientenfeld) abbildet.\\
  Um den Gradienten $\vecf{\nabla}f$ der Funktion $f$ zu bilden, fasst man alle partiellen Ableitungen der Funktion $f$, nach jedem
  Argument $x_i$, in einem Vektor zusammen. Meistens bestimmt man den Gradienten
  $\vecf{\nabla} f(\vec{p})$ in einem spezifischen Punkt $\vec{p} =
  \trans{\begin{pmatrix} p_1 & \cdots & p_n \end{pmatrix}}$.
  \begin{equation*}
    \vecf{\nabla}_{\vec{x}} f(\vec{p}) = \ds\partderiv{f}{\vec{x}}(\vec{p}) =
    \begin{pmatrix}
      \ds\partderiv{f}{x_1}(\vec{p}) \\
      \vdots \\
      \ds\partderiv{f}{x_n}(\vec{p}) \\
    \end{pmatrix}
  \end{equation*}

  Geometrisch ist der Gradient $\vecf{\nabla}f(\vec{p})$ einer Funktion $f$ in
  einem Punkt $\vec{p}$ dann der Vektor, welcher in die Richtung des steilsten
  Anstiegs von $f$ zeigt. Sein Betrag gibt die Stärke des Anstiegs an.
\end{defbox}
\para{}
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.7\textwidth]{grad.pdf}
  \caption{ein Skalarfeld als Konturdiagramm mit zugehörigem Gradientenfeld}
\end{figure}
\para{}
Da der Gradient in die Richtung des steilsten Anstiegs zeigt, sollte der Vektor
$\vec{b}_t$ gerade in die entgegengesetze Richtung zeigen. Also sollte er in die Richtung des negierten Gradienten der Funktion $f$ im Punkt $\vec{p}_t$ zeigen.
Also kann jetzt das iterative Annähern an das lokale Minimum folgendermassen beschrieben
werden (wobei $\eta$ eine später erklärte Schrittgrösse darstellt).
\\
\begin{equation}\label{eq:gradientdescent}
  \vec{p}_{t+1} = \vec{p}_t - \eta \cdot \vecf{\nabla} \mathit{f}(\vec{p}_t)
\end{equation}
\\

\ifcp%
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.7\textwidth]{gd.pdf}
  \caption{Visualisierung des Gradientenabstiegs: ein Ball rollt das
    Gradientenfeld hinab in das lokale Minimum.}
\end{figure}
\fi%

Während des Gradientenverfahren konvergiert der Punkt $\vec{p}_t$ zu einem
beliebigen \textit{lokalen} Minimum, abhängig davon wie der Startpunkt
$\vec{p}_0$ gewählt wurde.
Wie man diesen Startpunkt zu wählen hat, wird in Sektion
(\ref{sec:parameter_initalisieren}) erlaütert.
Da dieser meist zufällig bestimmt wird, ist es eine Glückssache ein sehr niedriges Minimum zu finden.
\para{}
Die sogennante \keyword{Lernrate} $\eta$ aus Gleichung (\ref{eq:gradientdescent}) ist ein Hyperparameter.
Sie ist ein positiver Proportionalitatsfaktor, welcher die Schrittgrösse des
Gradientenabstiegs bestimmt. Sie muss je nach zu minimierender Funktion anders gewählt werden.
Dabei hilft nur Ausprobieren. Falls $\eta$ nicht gut gewählt wurde, gibt es Probleme beim Training:
\begin{itemize}
\item{Falls $\eta$ zu klein ist, verlaüft das Trainings unnötig langsam und braucht sehr lange.
    Ausserdem kann es passieren, dass man bei einem hohen lokalen Minimum stecken bleibt.}

\item{Falls $\eta$ zu gross ist, kann es passiert, dass man über das lokale
    Minimum hinaus schiesst und somit nur darum herum springt.}
\end{itemize}

\begin{figure}[h!]
  \centering
  \begin{tikzpicture}
    \draw[gray] (0,0) grid[step=0.5cm] (5,5);
    \draw[ultra thick,black,->] (-0.5,0) -- (5.5,0) node[right] {$x_1$};
    \draw[ultra thick,black,->] (0,-0.5) -- (0,5.5) node [above left] {$f(x_1)$};
  \end{tikzpicture}
  \caption{Visualisierung verschiedener Lernraten}
\end{figure}

\para{}
\cite{Nielsen}

\subsection{Stochastisches Gradientenverfahren}
Wie oben erklärt, wird für das Trainieren eines Modells das Gradientenverfahren benutzt.
Konkret wird die Kostenfunktion $C(\vec{y},\vec{\hat{y}}\ |\ \param_0,\ldots,\param_n)$
($\param_i$ sind die Modellparameter, $\vec{y}$ die Vorhersagen und $\vec{\hat{y}}$
die Labels) minimiert, indem die Parameter angepasst werden. Dadurch macht das Modell immer bessere Vorhersagen.
Für nur eine Iteration des Gradientenverfahren, müsste man den Gradienten für den
\textit{gesamten} Trainingsdatensatz berechnen.
Dies wäre zwar ein exakter Prozess, aber ein extrem langsamer zugleich.
Bei grossen Datensätzen würde es eine Ewigkeit daürn, bis das Modell nur annahernd güte Vorhersagen machen würde.
\para{}
Aus diesem Grund verwendet man eine leicht abgeänderte Variante dieses
Verfahren, nämlich das \keyword{Stochastische Gradientenverfahren} (engl.:
Stochastic Gradient Descent) (SGD).
Hierfür wird der ``echte'' Gradient des gesamten Datensatzes mit dem Gradienten einiger Trainingssamples approximiert.
Dazu wird der Trainingsdatensatz in sogennante \keyword{Mini-Batches} eingeteilt und der Gradient jeweils pro Mini-Batch berechnet.
Als Konseqünz finden deutlich mehr Iterationen in \textit{einer}
Durchkämmung der Trainingsdaten statt. Eine solche Durchkämmung bezeichnet man als
\keyword{Epoche}. Oft wird mehrere Epochen lang trainiert.
Der Gradient eines genug grossen Mini-Batches ist zwar nicht ganz exakt, aber approximiert den Gradieten des gesamten Datensatzen genügend gut.
Sowohl die Mini-Batch Grösse, wie auch die Anzahl Epochen sind weitere Hyperparameter.
\para{}
Die partiellen Ableitungen der gesamten Trainingsdaten wird mit dem
arithmetischen Mittel der partiellen Ableitungen eines Mini-Batches der Grösse $q$ approximiert.
\\
\begin{equation}\label{eq:minibatch_deriv}
  \partderiv{\bar{C}}{\param_k} \approx \frac{1}{q}\sum_{i=1}^{q} \partderiv{C_i}{\param_k}
\end{equation}
\\
Eine Iteration des Stochastischen Gradientenverfahren wird analog zu Gleichung (\ref{eq:gradientdescent}) folgendermassen durchgeführt.
\\
\begin{equation}\label{eq:sgd}
  \param_{k,t+1} = \param_{k,t} - \frac{\eta}{q} \sum_{i=1}^{q} \partderiv{C_i}{\param_{k,t}}
\end{equation}

\cite{Nielsen}

\section{Trainingsphänomene}

\subsection{Konvergenz und Divergenz}
Beim Training eines Modells, kann es entweder zu \keyword{Konvergenz} oder \keyword{Divergenz} kommen.
Falls die Vorhersagen im Verlaufe des Trainings immer besser mit den Labels
übereinstimmen, bzw. die Kostenfunktion immer kleiner wird, sagt man, dass das
Modell konvergiert. Also findet das Gradientenverfahren erfolgreich ein lokales Minimum.
\para{}
Es passiert aber auch oft, dass ein Modell nicht konvergiert oder vielleicht
sogar divergiert; die Kosten werden also grösser im Verlaufe des Training.
Dies kann verschiedene Gründe haben. Einige davon können sein:
\begin{itemize}
  \item{zu wenig Trainingsdaten}
  \item{zu schwache Korrelation zwischen Inputs und Labels}
  \item{falsche Hyperparameter}
  \item{falsches Modell}
\end{itemize}


\subsection{Underfitting und Overfitting}
Falls ein Modell konvergiert, heisst das noch nicht, dass es die
Gesetzmässigkeiten innerhalb der Trainingsdaten richtig erlernt hat.
Es kann dabei zu zwei Problemen kommen: Overfitting und Underfitting.
\para{}
\keyword{Overfitting} bezeichnet das Problem, dass ein Modell Vorhersagen
erzeugt, welche zu stark an die Trainingssamples angepasst sind.
Meistens liegt das daran, dass das Modell zu viele Modellparameter besitzt oder
der Trainingsdatensatz zu wenige Samples besitzt.
Somit übersteigt die Komplexität des Modells gewissermassen die des Problems.
\para{}
Um das Problem zu verdeutlichen betrachten wir ein lineares Regressionmodell.
Dieses besitzt eine Hypothesenfunktion $h = a_0 + a_1 x + a_2 x^2 + \ldots + a_n
x^n$, welcche einfach ein Polynom $n$-ter Ordnung ist. Falls der
Trainingsdatensatz nun aus $n$ oder weniger Samples besteht, kann das Modell die
Regressionkurve der Hypothesenfunktion exakt durch jeden Datenpunkt legen.
Dies entspricht dem Verhalten, dass ein Modell mit $n$ Modellparametern, welches
mit $n$ Samples trainiert wird, exakt jedes Samples auswendig lernt, anstatt die
Gesetzmässigkeiten zu erlernen.
Beim Auswendiglernen
wiegt das Modell dem Datenrauschen, welches durch die natürliche Varianz
innerhalb der Trainingsdaten entsteht, zu viel Gewicht zu. Somit nutzt es
eigentlich unrelevante Modellparameter dazu dieses Rauschen zu erlernen. \\
Dadurch kann das Modell zwar sehr gute Vorhersagen zum Trainingsdatensatz
$\set{X}$ machen, jedoch würde es sehr schlechte Vorhersagen für einen
zusätzlichen Testdatensatz $\set{T}$ liefern.
\para{}
Somit können wir Overfitting folgendermassen definieren. Eine
Hypothesenfunktion overfittet dann, wenn eine alternative Hypothesenfunktion
$h'$ exisitert, für welche die Kosten bezüglich dem Trainingsdatensatz
grösser sind $\bar{C}_{h'}(\set{X}) > \bar{C}_h(\set{X})$, jedoch die Kosten
für Testdatensatz kleiner sind $\bar{C}_{h'}(\set{T}) < \bar{C}_h(\set{T})$.
\para{}
Das Gegenteil von Overfitting ist \keyword{Underfitting}. Dabei handelt es sich um das Problem, dass eine
Hypothesenfunktion $h$ zu
wenige Modelparameter $\param_k$ besitzt, um die Komplexität eines Problems zu bewältigen.
Die Parameter reichen nicht aus, damit das Modell die Korrelation zwischen
Inputs und Labels erlernen kann, beziehungsweise durch die Hyptothesenfunktion
die gewünschte Abbildung durchzuführen.
\para{}
Bezogen auf das lineare Regressionmodell, bedeutet dies, dass der Grad $n$ des
polynomen Hypothesenfunktion $h$ zu gering ist, um sich an die Datenpunkte der
Sampels anzuschmiegen.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{overunderfitting.jpg}
  \caption{Visalierung von Under- und Overfitting}
\end{figure}

\cite{wiki:overfitting}

% ------------------------------------------------------------

\chapter{Deep Learning und Künstliche Neuronale Netze}
Nun betrachten wir ein spezifisches Modell, welches die wohl besten Resultate
für die meisten Problemstellungen des Maschinellen Lernens (Bilderkennung,
Spracherkennung, etc.) liefert. Es handelt sich um das \keyword{Künstliche Neuronale Netz} (eng.: Neural Network) kurz KNN.
Der Bereich des Maschinellen Lernens, welcher sich mit Neuronalen Netzen
beschäftigt, bezeichnet man als \keyword{Deep Learning}.
\para{}
Künstliche Neuronale Netze sind zum Teil biologisch von Nervensystemen vieler
Lebewesen inspiriert.
Sie sind aber lediglich eine Abstraktion ihrer Informationverarbeitung und versuchen nicht eine möglichst genaü biologische Abbildung darzustellen.
Es gibt nicht nur eine Art von Neuronalem Netz, sondern es exisitieren die
verschiedensten Architekturen, welche je nach Problemstellung ausgewählt werden
müssen. Diese Arbeit wird vorallem von zwei solchen Architekturen gebrauch machen:
Convolutional Neural Networks und sogenannte Autoencodern.

\para{}
\cite{wiki:kuenstliches_neuronales_netz}

\section{Perzeptron}
Um den Aufbau und die Funktion eines Künstlichen Neuronalen Netzes besser zu
verstehen, wird im folgenden ein Vorgänger des KNN erklärt: das \keyword{Perzeptron}.
\para{}
Das einlagige Perzeptron wurde erstmals 1958 von Frank Rosenblatt vorgestellt. Dieses
besteht aus einem einzigen Künstlichen Neuron. Dieses Künstliche Neuron
hat mehrere binäre Inputs und einen einzigen binären Output. Binär
bedeutet, dass der Wert nur entweder 0 (\textit{aus}) oder 1 (\textit{ein}) sein
kann. Des weiteren besitzt es mehrere sogenannte \keyword{Gewichte} $w_1, \ldots,
w_m \in \set{R}$, für jeden Input $x_i$ ein Gewicht $w_i$.
Diese sind reelle Zahlen, welche das Verhalten des Perzeptron bestimmen.
Die \keyword{gewichtete Summe}, also die Summe aller Produkte der Inputs mit
ihrem Gewicht, wird mit $\tilde{z}$ bezeichnet.
Sie ist das gleiche wie das Skalarprodukt des Gewichtevektors
$\vec{w} = \trans{\begin{pmatrix} w_1 & \cdots & w_n \end{pmatrix}}$ mit dem
Inputvektor $\vec{x}$. \\
\begin{equation*}
  \tilde{z} = \sum_{i=1}^{m} w_i x_i = \vec{w} \cdot \vec{x}
\end{equation*} \\
Zusätzlich besitzt das Perzeptron einen \keyword{Schwellenwert} $\tilde{b}$.
Zusammen mit den Gewichten, bilden sie die Modellparameter.
Das Perzeptron verhält sich so, dass falls die gewichtete Summe $\tilde{z}$ grösser als der
Schwellenwert $\tilde{b}$ ist, das Neuron feürt, d.h.\ der Output 1 beträgt.
Andernfalls ist er 0 (siehe erster Teil der Hypothesenfunktion $h$ in Gleichung (\ref{eq:perzeptron_1})).
Es ist gängig die Ungleichung der Bedingung in die Nullstellenform zu bringen
und $\tilde{b}$ durch die \keyword{Neigung} (engl.: bias)
$b = -\tilde{b}$ zu ersetzten. Somit lautet die Ungleichung: $\tilde{z} + b
> 0$. Der neue Term $\tilde{z} + b$ wird mit $z$ bezeichnet (siehe Rest der Gl. (\ref{eq:perzeptron_1})).
Die Neigung gibt an wie stark das Neuron dazu neigt zu feürn. Eine grosse
Neigung lässt ein Neuron auch dann noch feürn, wenn nur wenige Inputs
eingeschaltet sind. SCHLECHT \\
\begin{equation}\label{eq:perzeptron_1}
  h(\vec{x}) =
  \begin{cases}
    1 & \quad \text{falls } \tilde{z} > \tilde{b}\\
    0 & \quad \text{ansonsten}
  \end{cases}
  \quad =
  \begin{cases}
    1 & \quad \text{falls } \tilde{z} + b > 0\\
    0 & \quad \text{ansonsten}
  \end{cases}
  \quad =
  \begin{cases}
    1 & \quad\text{falls } \vec{w} \cdot \vec{x} + b > 0\\
    0 & \quad\text{ansonsten}
  \end{cases}
\end{equation}
\para{}
\begin{figure}[h!]
  \centering
  \begin{tikzpicture}[>=latex]
    \path (3,0) node [circle,draw](neuron){Neuron};
    \path[red] (0,1.5) node(x1){$x_1$} (0,0) node(x2){$x_2$} (0,-1.5) node(x3){$x_3$};
    \path[black!40!green] (5,0) node(y1){$y$};
    \draw[->] (x1) -- node[above,sloped]{$w_1$} (neuron);
    \draw[->] (x2) -- node[above,sloped]{$w_2$} (neuron);
    \draw[->] (x3) -- node[above,sloped]{$w_3$} (neuron);
    \draw[->] (neuron) -- (y1);
  \end{tikzpicture}
  \caption{Perzeptron mit drei Inputs}
  \label{fi:perzeptron}
\end{figure}
\para{}
Für das Trainieren des Perzeptrons existieren spezielle Verfahren, welche hier
aber nicht relevant sind. Das Gradientenverfahren kann nämlich nicht verwendet
werden. Der Grund dafür sollte später in Sektion (\ref{sec:künstlicheNeuronen}) einleuchtend werden.

\cite{wiki:perzeptron}
\cite{Nielsen}

\subsection{Lernpotenzial eines Perzeptrons}
Nun stellt sich die Frage, was ein Perzeptron erlernen kann und wofür es genutzt werden kann.
Das Perzeptron ist lediglich ein \keyword{linearer Klassifikator} der Form
$y = w_1x_1 + \cdots + w_m x_m$. Es ist also ein Klassifizierungsmodell, kein Regressionsmodell.
Es kann die Features in zwei Klassen 0 oder 1 einordnen, wobei der Output der
Hypothesenfunktion, diese Klassifizierung angibt.
Überschreitet $y$ den Schwellenwert, werden die Features der Klasse 1 zugeordnet, sonst
der Klasse 0.
Jedoch müssen diese Klassen linear separierbar sein.
\para{}
Lineare Separierbarkeit bedeutet, dass alle Featurevektoren $\vec{x}_1,\ldots,\vec{x}_p \in \set{R}^m$
innerhalb ihres Vektorraums $\set{R}^m$ durch eine Hyperebene in ihre Klassen aufteilbar sein müssen.
Falls das Perzeptron zwei Inputs hat, bedeutet dies, dass die Ortsvektoren
einfach durch eine Gerade voneinander trennenbar sein müssen (siehe Abb.
(\ref{fig:linearer_Klassifikator})). \\
Falls die Features nicht linear separierbar sind, bedeutet dies, dass das
Perzeptron die Klassifizierung nicht erlernen kann.
BEISPIEL
\\
\begin{figure}[h!]
  \caption{erfolgreiche lineare Separierung (links) und das Versagen bei XOR (rechts)}
  \label{fig:linearer_Klassifikator}
\end{figure}
\para{}
\cite{wiki:perzeptron}
\cite{wiki:linear_separability}

\section{Erweiterung der Künstlichen Neuronen}\label{sec:künstlicheNeuronen}
Ein Perzeptron ist, wie vorhin erklärt, nur in der Lage, lineare Klassifikationen
durchzuführen. Um nun auch kompliziertere Probleme zu lösen, muss das Prinzip
ausgebaut werden. Ausserdem brauchen wir ein Künstliches Neuron, welches sich
besonders gut als Baustein für KNNs eignet.


\subsection{Künstliche Neuronen im Allgemeinen}
Künstliche Neuronen sind immer so aufgebaut, dass sie einen oder mehrere Inputs
haben und einen einzigen Output. Zu jedem Input $x_i$ ist ein Gewicht
$w_{i}$ assoziert. Zürst wird die gewichtete Summe der Inputs $\tilde{z}$ gebildet.
Die Neigung $b$ wird ebenfalls draufaddiert, um $z$ zu erhalten. Nun muss
die sogenannte \keyword{Aktivierung} $a$ gebildet werden. Sie ist der Output des Neurons.
Die Aktivierung $a = \varphi(z)$ ist das Resultat der
\keyword{Aktivierungsfunktion} $\varphi: \set{R} \to \set{R}$ angewendet
auf $z$. Die verschiedenen Künstlichen Neuronen unterscheiden
sich fast nur in ihrer Aktivierungsfunktion.
\\
\begin{figure}[h!]

  \caption{ein künstliches Neuron und seine Bestandteile}
\end{figure}
\\

\para{}
\cite{Nielsen}
\cite{wiki:kuenstliches_neuron}

\subsection{Perzeptronen als Künstliche Neuronen}
Zürst noch einmal ein Blick auf das Perzeptron im Angesicht der Aktivierungsfunktion.
Ein wesentlicher Unterschied des Perzeptron gegenüber sonstigen Künstlichen
Neuronen, besteht darin, dass seine Inputs und Outputs nur binäre Werte
annehmen können. Um dieses Verhalten des Perzeptrons zu erhalten,
muss eine Stufenfunktion als Aktivierungfunktion verwendet werden: die Heaviside-Funktion $\Theta$.
Sie hat einen einzigen Stufensprung bei $x=0$ vom Wert 0 auf 1 (siehe Abb. (\ref{fig:heaviside})).
\\
\begin{figure}[h!]
  \begin{minipage}[h!]{0.5\textwidth}
    \begin{equation*}
      \varphi^{\text{hlim}}(z) = \Theta(z) =
      \begin{cases}
        1 & \quad \text{falls } z \geq 0\\
        0 & \quad \text{falls } z < 0
      \end{cases}
    \end{equation*}
  \end{minipage}
  \begin{minipage}[h!]{0.5\textwidth}
    \centering
    \begin{tikzpicture}[scale=2.5]
      \draw[->] (-1.5,0) -- (1.5,0) node[right] {$x$}; % x-axes
      \draw[->] (0,-0.2) -- (0,1.2) node [above] {$y$}; % y-axes
      \draw[style=help lines,step=0.5] (-1.4,0) grid (1.4, 1.1);

      \foreach \x in {-1,-0.5,0.5,1}
      \draw[shift={(\x,0)}] (0pt,2pt) -- (0pt,-2pt) node[below,fill=pagecolor] {$\x$};

      \foreach \y in {0.5,1}
      \draw[shift={(0,\y)}] (2pt,0pt) -- (-2pt,0pt) node[left,fill=pagecolor] {$\y$};

      \draw[shift={(0,0)}] (0pt,0pt) node[below left,fill=pagecolor] {$O$};

      \draw[red,ultra thick] (-1.5,0) -- (0,0); % 0-red
      \draw[red,ultra thick] (0,1) -- (1.5,1); % 1-red
      \draw[red,ultra thick,dashed] (0,0) -- (0,1); % y-red
      \draw[draw=red,fill=white] (0,0) circle (0.05);
      \draw[draw=red,fill=red] (0,1) circle (0.05);
    \end{tikzpicture}
  \end{minipage}
  \caption{Definition und Graph der Heaviside-Funktion $\Theta$}
  \label{fig:heaviside}
\end{figure}

\cite{wiki:kuenstliches_neuron}
\cite{wiki:perzeptron}


\subsection{ReLU Neuronen}\label{sec:ReLU}
Der nächste Schritt nach einer Stufenfunktion als Aktivierungsfunktion, waren
lineare Aktivierungsfunktionen. Für diese können die Inputs nun belibige reelle
Zahlen sein.
Jedoch sind solche lineare Neuronen in einem KNN von keinem Nutzen.
Das hat den Grund, dass eine Verkettung von linearen Neuronen,
immer auf eine einzige lineare Funktion reduziert werden kann. Somit hat
die Verkettung keinen Mehrwert.
\para{}
Stattdessen verwendet man sogennante ReLU Neuornen. Sie benutzen die
\keyword{Rectified Linear Unit} (\keyword{ReLU}) Aktivierungsfunktion.
Diese ist
eine nur teilweise lineare Aktiverungsfunktion. Die Werte grösser als 0 werden
auf sich selber linear abbgebildet und die Werte kleiner als 0 werden auf 0
abbgebildet (siehe Abb. (\ref{fig:relu})).
Eine sehr wichtige Eigenschaft von ihr ist, dass sie - im Gegensatz zu den vorhin
genannten Aktivierungsfunktionen - fast überall differenzierbar%
\footnote{%
  Eigentlich ist die ReLU-Funktion in $x=0$ wegen des Knicks nicht
  differenzierbar. Für die Gradientenberechnung definiert man jedoch einfach
  die Ableitung $\varphi'^{\text{ReLU}}(0) \coloneqq 0$. Dies ist mathematisch zwar nicht
  korrekt, löst aber das Problem.
}%
und strikt monoton
steigend ist. Erst für diese Aktivierungsfunktion, kann das Gradientenverfahren
angewendet werden und somit das KNN trainiert werden.
\para{}
Da sie nur teilweise linear ist, gehört sie genaugenommen den
Nicht-linearen-Aktiverungsfunktionen an. Diese Nicht-Linearität erlaubt es dem
Neuronen deutlich komplexere Systeme zu modellieren und so deutlich komplexere
Probleme zu lösen. Wir werden in Sektion (\ref{sec:UAT}) sogar erfahren, dass
eine Kompostion von nicht-linearen Neurön jede beliebige Funktion approximieren kann.
\para{}
Wir werden diese Art von Neuron vorerst zur Seite legen und erst wieder in
Kapitel (\ref{sec:CNN}) im Zusammenhang mit KNNs zur Bilderkennung betrachten.
\para{}
\begin{figure}[h!]
  \begin{minipage}[h!]{0.5\textwidth}
    \begin{equation}
      \varphi^{\text{ReLU}}(z) =
      \begin{cases}
        z & \quad \text{falls } z > 0\\
        0 & \quad \text{falls } z \leq 0
      \end{cases}
      = \max(z,0)
    \end{equation}
  \end{minipage}
  \begin{minipage}[h!]{0.5\textwidth}
    \centering
    \begin{tikzpicture}[scale=2.5]
      \draw[->] (-1.5,0) -- (1.5,0) node[right] {$x$}; % x-axes
      \draw[->] (0,-0.2) -- (0,1.2) node [above] {$y$}; % y-axes
      \draw[style=help lines,step=0.5] (-1.4,0) grid (1.4, 1.1);

      \foreach \x in {-1,-0.5,0.5,1}
      \draw[shift={(\x,0)}] (0pt,2pt) -- (0pt,-2pt) node[below,fill=pagecolor] {$\x$};

      \foreach \y in {0.5,1}
      \draw[shift={(0,\y)}] (2pt,0pt) -- (-2pt,0pt) node[left,fill=pagecolor] {$\y$};

      \draw[shift={(0,0)}] (0pt,0pt) node[below left,fill=pagecolor] {$O$};

      \draw[red,ultra thick] (-1.5,0) -- (0,0);
      \draw[red,ultra thick] (0,0) -- (1,1);
      \draw[red,ultra thick,dashed] (1,1) -- (1.2,1.2);

      \draw[draw=red,fill=red] (0,0) circle (0.03);
    \end{tikzpicture}
  \end{minipage}
  \caption{Formel und Graph der ReLU-Funktion}
  \label{fig:relu}
\end{figure}

\para{}
\cite{wiki:kuenstliches_neuron}
\cite{Nielsen}

\subsection{Sigmoid Neuronen}
Ein weiteres nicht-lineares Neuron, sind sogennante Sigmoid-Neuronen.
Den Namen haben sie von ihrer Aktivierungsfunktion: der Sigmoidfunktion $\sigma$.
\para{}
Auch sie kann aufgrund ihrer Nicht-linearität zur Approximation jeder Funktion
in einem KNN verwendet werden. Sie ist die Aktivierungsfunktion, welche am
meisten in KNNs eigesetzt wird. Da sie wirklich sehr stark von Lineartiät
abweicht und so am schnellsten komplexe Sachverhalte modellieren kann.
\para{}
Die Sigmoid-Funktion besitzt eine einzige Wendestelle $\sigma''(x=0)=0$ und hat
zwei Asymptoten, eine $\ds\lim_{x \to -\infty} \sigma(x)=0$
und eine zweite $\ds\lim_{x \to \infty} \sigma(x)=1$ (siehe Abb.
(\ref{fig:sigmoid})). Desweiteren zeichnet sie sich durch ihre simple Ableitung aus.
\\
\begin{figure}[h!]
  \begin{minipage}[h!]{0.5\textwidth}
    \begin{align*}
      \varphi^{\text{sig}}(z) &= \sigma(z) = \frac{1}{1 + e^{-z}}\\
      \sigma'(z)&=\sigma(z)(1-\sigma(z))
    \end{align*}
  \end{minipage}
  \begin{minipage}[h!]{0.5\textwidth}
    \centering
    \begin{tikzpicture}[scale=2.5]
      \draw[->] (-1.5,0) -- (1.5,0) node[right] {$x$}; % x-axes
      \draw[->] (0,-0.2) -- (0,1.2) node [above] {$y$}; % y-axes
      \draw[style=help lines,ystep=0.5,xstep=0.25] (-1.4,0) grid (1.4, 1.1);

      \foreach \x/\xtext in {-1/-4,-0.5/-2,0.5/2,1/4}
      \draw[shift={(\x,0)}] (0pt,2pt) -- (0pt,-2pt) node[below,fill=pagecolor] {$\xtext$};

      \foreach \y in {0.5,1}
      \draw[shift={(0,\y)}] (2pt,0pt) -- (-2pt,0pt) node[left,fill=pagecolor] {$\y$};

      \draw[shift={(0,0)}] (0pt,0pt) node[below left,fill=pagecolor] {$O$};

      \draw[red,ultra thick,x=0.25cm] plot[domain=-6.0:6.0] (\x,{1/(1+exp(-\x)) });
    \end{tikzpicture}
  \end{minipage}
  \caption{Definition, Ableitung und Graph der Sigmoid-Funktion $\sigma$}
  \label{fig:sigmoid}
\end{figure}

\para{}
\cite{wiki:kuenstliches_neuron}
\cite{wiki:sigmoidfunktion}


\section{Topologie der Künstlichen Neuronalen Netzen}
Nun sollten diese Sigmoiden Neuronen als Bausteine verwendet werden, um ein Künstliches
Neuronales Netz zu bilden. Dazu werden sie miteinander verbunden und bilden so ein Netz,
ähnlich wie ein Nervensystem.
\para{}
Diese Neuronen sind in verschieden Schichten (engl.: Layers)
arangiert. Die erste ist die \keyword{Inputschicht}. Sie beinhaltet die
Inputneuronen. Diese sind eigentlich keine richtigen
Neuronen sondern eher Platzhalter für ihr jeweiliges Feature $x_i$. Als letztes kommt die
\keyword{Outputschicht} mit den Outputneuronen, welche jeweils einen Outputwert $y_i$
besitzen. Dazwischen liegen die \keyword{Zwischenschichten} (engl.: hiddenlayers). Von ihnen kann es
beliebig viele geben und in ihnen beliebig viele Neuronen.
Falls viele Zwischenschichten verwendet werden, bezeichnet man das Netzwerk als
``deep''. Daher rürt auch der Name des Deep Learnings.
Der Aufbau eines KNN bezeichnet man als \keyword{Topologie} des Netzes. Die
Topologie umfasst viele Hyperparameter. Darunter sind zum Beispiel die Anzahl Zwischenschichten, wie auch
die Anzahl Neuronen pro Schicht.
\para{}
Jedes Neuron aus einer Schicht ist mit jedem Neuron aus der nächsten Schicht über
Verbindungen gekoppelt. Alle Verbindungen besitzten ein Gewicht analog zu den Inputs des
Perzeptrons. Die Aktivierung, also der Output, eines Neurons wandert entlang den jeweiligen
Verbindung zu allen Neuronen der nächsten Schicht und dient als deren Input.
Die söben beschriebe Art von KNN nennt man \keyword{Feedforward-Netz}, da alle Werte
ausschliesslich nach vorne propagiert werden.
\para{}
In Abbildung (\ref{fig:nn_layers}) ist ein Beispiel eines Neuronalen Netzes
abgebildet. In diesem Fall besitzt es sowohl 4 Inputs, als auch 4 Outputs. Es hat
ausserdem 3 Zwischenschichten. Die erste und die dritte haben jeweils 3 Neuronen
und die zweite besitzt 4 Neuronen. \\

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{knn1.pdf}
  \caption{Schichten eines KNNs}
  \label{fig:nn_layers}
\end{figure}

\cite{wiki:kuenstliches_neuronales_netz}
\cite{Nielsen}

\section{Lernverhalten}
Die Hoffnung beim Trainieren von KNNs besteht darin, dass das Modell für jede
weiter Schicht eine höheres Abstraktionsniveau erreicht. Würde man zum
Beispiel ein Netzwerk zur Gesichtserkennung trainieren, könnte man sich den
Erkennungsprozess folgendermassen vorstellen: Die erste Zwischenschicht erkennt
Kanten und Konturen. Die zweite vereint diese Merkmale zu Ecken und primitiven
geometrischen Formen. Die dritte Schicht sollte dann schon komplexere
geometrische Formen erkennen, welche gewissen Gesichtmerkmalen, wie der Nase, ähneln. Die letzten Schichten soll dann alle diese
Merkmale zusammensetzen und so ein Gesicht erkennen.
WEITERFÜHREN

\section{Vorwärtspropagierung}
Jetzt, da der Aufbau eines KNNs erklärt wurde, sollte nun die mathematische Funktionsweise
des Modells erklärt werden. Hierfür müssen einige Konventionen zur
Bezeichnung der Teile eines KNNs getroffen werden. Es sollten vorallem noch
Abbildungen (\ref{fig:nomenklatur1}) und (\ref{fig:nomenklatur2}) zum
Verstäntnis der Nomenklatur studiert werden.
\begin{itemize}
\item{$l$ ist der Index einer Schicht. Die Indexierung beginnt bei 0.}
\item{$L$ ist der letzte Schichtindex und somit auch die gesamte Anzahl an
    Schichten (ohne die Inputschicht).}
\item{$|l|$ ist die Anzahl Neuronen in der $l$-ten Schicht.
    \footnote{
      Diese Schreibweise hat nichts mit dem Betrag zu tun, sondern wird einfach
      gewählt, da sie sehr platzsparend ist.
    }
  }
\item{$n_j^l$ bezeichnet das $j$-te Neuron in der $l$-ten Schicht.}
\item{$z_j^l$ ist die gewichtete Summe der Inputs des $j$-ten Neuron in der $l$-ten Schicht.}
\item{$a_j^l$ ist die Aktivierung/Output des $j$-ten Neurons in der $l$-ten Schicht.}
\item{$b_j^l$ ist die Neigung für das $j$-te Neuron in der ($l+1$)-ten Schicht.
    \footnote{
      Diese Konvention wurde gewählt, damit die folgenden Gleichungen simpler sind.
    }
  }
\item{$w_{j,k}^l$ ist das Gewicht der Verbindung vom $k$-ten Neuron
    in der $l$-ten Schicht zum $j$-ten Neuron in der ($l+1$)-ten Schicht.
    \footnote{
      Man beachte die Reihenfolge!\\
      Diese Konvention scheint zwar auf den ersten Blick unintuitiv, macht jedoch
      Sinn für die Matrixindezierung.
      (\ref{sec:backpropagation}).
    }
  }
\item{$\varphi$ ist die gewählte Aktivierungsfunktion (diese ist immer eine
    nicht-lineare Aktivierungsfunktion)}
\end{itemize}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{knn2.pdf}
  \label{fig:nomenklatur1}
  \caption{zum Verstäntniss der Nomenklatur}
\end{figure}
\para{}
\begin{figure}[h!]
  \centering
  \begin{tikzpicture}[>=latex]
    \path (-2,1.5) node [draw,circle,inner sep=0,minimum size=1.25cm](n11){$n^1_1$};
    \path (-2,-1.5) node [draw,circle,inner sep=0,minimum size=1.25cm](n12){$n^1_2$};
    \path (2,1.5) node [draw,circle,inner sep=0,minimum size=1.25cm](n21){$n^2_1$};
    \path (2,-1.5) node [draw,circle,inner sep=0,minimum size=1.25cm](n22){$n^2_2$};
    \draw[->] (n11) -- node[above,sloped]{$w^1_{1,1}$} (n21);
    \draw[->] (n11) -- node[above,pos=0.75,sloped]{$w^1_{2,1}$} (n22);
    \draw[->] (n12) -- node[above,pos=0.75,sloped]{$w^1_{1,2}$} (n21);
    \draw[->] (n12) -- node[above,sloped]{$w^1_{2,2}$} (n22);
  \end{tikzpicture}
  \label{fig:nomenklatur2}
  \caption{zum Verstäntnis der Gewichtebeschriftungen}
\end{figure}
\para{}
Die Vorwärtspropagierung beginnt bei den Inputneuronen, welche jeweils
einen Inputwert in sich tragen. Diese Werte werden, um für eine kohärente Nomenklatur zu sorgen,
analog zu den Aktivierungen der anderen Neuronen mit $a_j^0$ bezeichnet, wobei
$j$ der Index des Neurons ist. \\
Nun müssen die restlichen Aktivierungen der Neuronen bis und mit den Ouputneuronen berechnet werden. Dies geschieht rekursiv, anhand der
Aktivierungen der voherigen Schicht und zwar folgendermassen (ersichtlich in
Gleichung (\ref{eq:gewichtete_summe_normal})).
\para{}
Zürst läuft eine Summe über alle Neuronen $n_k^{l}$ der jetztigen Schicht
$l$. Dabei wird die gewichtete Summe der Aktivierungen $a_k^{l}$ mit den
assozierten Gewichten $w_{j,k}^l$ gebildet. ZU KOMPLIZIERT RELEVANT?: Hierbei ist das Gewicht jenes, welches das
$k$-te Neuron der $l$-ten Schicht mit dem $j$-ten Neuron der ($l+1$)-ten Schicht verbindet.
Zusätzlich gehört zu der gewichteten Summe auch die jeweilige Neigung $b_j^l$, welche
dazuaddiert wird. Diese gewichtete Summe wird mit $z_j^{l+1}$ bezeichnet.
\\
\begin{equation}\tag{FP1}\label{eq:gewichtete_summe_normal}
  z_j^{l+1} = \sum_{k=1}^{|l|} w_{j,k}^l a_k^l + b_j^l
\end{equation}
\\
Auf diese Summe wird dann die Aktivierungsfunktion $\varphi$ angewandt.
Das ist dann die Aktivierung $a_j^{l+1}$ des $j$-ten Neurons in der ($l+1$)-ten Schicht.
\\
\begin{equation}\tag{FP2}\label{eq:aktivierung_normal}
  a_j^{l+1} = \varphi\left(\sum_{k=1}^{|l|} w_{j,k}^l a_k^{l} + b_j^l \right) = \varphi \left( z_j^{l+1} \right)
\end{equation}
\par\bigskip
Für Deep Learning braucht man vorallem sogenannte Deep Neural Networks. Diese
zeichnen sich dadurch aus, dass sie sehr viele Zwischenschichten besitzen.
Deshalb bezeichnet man sie auch als ``deep''.
Bei solchen Netzwerken ist es nicht unüblich,
dass diese sehr viele Neuronen und Verbindungen (über 100'000) besitzen.
Um hierbei nicht den Überblick zu verlieren und um nicht in den Indizes zu
ertrinken, macht man Gebrauch von \keyword{Linearer Algebra}. Man verwendet
Matrizen und Vektoren um die vielen Variabeln zusammenzufassen.
Ausserdem besteht ein weiterer Vorteil darin, dass Computer mithilfe von Vektor-
und Matrixoperationen die Berechnungen paralellisieren können und in kürzerer
Zeit und mit weniger Ressourcen viele Berechnungen gleichzeitig ausführen können.
Dies beschleunigt das Training der Modelle um
ein Vielfaches. Dies wird später in Sektion
(\ref{sec:tensorflow}) noch weiter thematisiert.
\para{}
Die Inputs $\vec{x}$, Vorhersagen $\vec{y}$ und Labels $\vec{\hat{y}}$ haben wir schon von Anfang an als Vektoren geschrieben.
Nun sollen noch die Modellparameter und die restlichen Komponenten eines KNNs als Vektoren und Matrizen zusammengefasst werden.
Sowohl alle gewichteten Summen $z_j^l$, wie auch alle Aktivierungen $a_j^l$
einer Schicht $l$, werden in Vektoren $\vec{z}^l \in \set{R}^{|l|}$ und
$\vec{a}^l \in \set{R}^{|l|}$ zusammengefasst.
Auch alle Neigung $b_j^l$ für eine Schicht ($l+1$) bilden einen Vektor
$\vec{b}^l \in \set{R}^{|l+1|}$.
\para{}
Zu guter letzt, wird noch eine \keyword{Gewichtsmatrix} $\mat{W}^l \in
\set{R}^{|l+1| \times |l|}$
definiert. Sie enthält alle Gewichte welche die $l$-te
Schicht \textit{zu} der ($l+1$)-ten Schicht verbindet.
Das heisst der Eintrag in der $j$-ten Zeile und in
der $k$-ten Spalte ist $w_{j,k}^l$ und verbindet so das Neuron $n_k^{l}$ zu
dem Neuron $n_j^{l+1}$.
\\
\begin{align*}
  \vec{z}^l &=  \trans{\begin{pmatrix} z_1^l & z_2^l & \cdots & z_{|l|}^l \end{pmatrix}} \\
  \vec{a}^l &=  \trans{\begin{pmatrix} a_1^l & a_2^l & \cdots & a_{|l|}^l \end{pmatrix}} \\
  \vec{b}^l &=  \trans{\begin{pmatrix} b_1^l & b_2^l & \cdots & b_{|l+1|}^l \end{pmatrix}} \\
\end{align*}
\begin{equation*}
  \mat{W}^l =
  \begin{pmatrix}
    w_{1,1}^l & w_{1,2}^l & \cdots & w_{1,|l|}^l \\[0.3em]
    w_{2,1}^l & w_{2,2}^l & \cdots & w_{2,|l|}^l \\[0.3em]
    \vdots & \vdots & \ddots & \vdots \\[0.3em]
    w_{|l+1|,1}^l & w_{|l+1|,2}^l & \cdots & w_{|l+1|,|l|}^l
  \end{pmatrix}
\end{equation*}
\\
Mit diesen Definitionen kann nun fast Gleichung (\ref{eq:gewichtete_summe_normal}) in
Matrixform geschrieben werden, denn die Matrixmultiplikation von $\mat{W}^l$ mit
$\vec{a}^{l}$ ergibt einen Vektor $\vec{\tilde{z}}^{l+1}$, welcher alle gewichteten
Summen $\tilde{z}_j^{l+1}$ ohne die jeweilige Neigung enthält.
\\
\begin{equation*}
  \mat{W}^l \vec{a}^{l} = \trans{\begin{pmatrix}\ds \sum_{j=1}^{|l|} w_{1,j}^l a_j^l &\ds \sum_{j=1}^{|l|} w_{2,j}^l a_j^l & \cdots &\ds \sum_{j=1}^{|l|} w_{|l+1|,j} a_j^l \end{pmatrix}} = \vec{\tilde{z}}^{l+1}
\end{equation*}
\\
Nun müssen wir noch den Neigungsvektor $\vec{b}^l$ draufaddieren und wir
erhalten Gleichung (\ref{eq:gewichtete_summe_matrix}), mithilfe der wir den
Vektor der gewichteten Summen $\vec{z}^{l+1}$ bilden können.
\\
\begin{equation}\tag{FP1a}\label{eq:gewichtete_summe_matrix}
  \vec{z}^{l+1} = \mathbf{W}^{l} \vec{a}^{l} + \vec{b}^{l}
\end{equation}
\\
Der letzte Schritt besteht noch darin die Aktivierungsfunktion auf $\vec{z}^{l+1}$
anzuwenden um den Aktivierungsvektor $\vec{a}^{l+1}$ zu bilden.
Hierfür muss aber noch ein neues mathematisches
Konzept eingeführt werden: die Vektorisierung einer Funktion.
\para{}

\begin{defbox}{Vektorisierung einer Funktion}
  Die Vektorisierung einer skalaren Funktion $f$, geschrieben als
  $\vecf{f}[\vec{v}]$ hat als Argument einen Vektor $\vec{v}$, auf dessen
  Komponenten jeweils \textit{einzeln} die Funktion $f$ angewandt wird. Dieser neue
  Vektor ist der Rückgabewert der Funktion. Er besitzt die gleichen Dimensionen
  wie der Argumentvektor.
  \\
  \begin{equation*}
    \vecf{f}[\vec{v}]=
    \begin{pmatrix}
      f(v_1)\\
      \vdots \\
      f(v_n)\\
    \end{pmatrix}
  \end{equation*}
\end{defbox}

Nun kann ganz einfach die vektorisierte Aktivierungsfunktion $\vecf{\varphi}$ auf
$\vec{z}^{l+1}$ angewandt werden.

\begin{equation}\tag{FP2a}\label{eq:aktivierung_matrix}
  \vec{a}^{l+1} = \vecf{\varphi} \left[\mat{W}^{l} \vec{a}^{l} + \vec{b}^{l} \right] = \vecf{\varphi} \left[ \vec{z}^{l+1} \right]
\end{equation}

\para{}
\cite{Nielsen}

\subsection{Modellparameter initialisieren}\label{sec:parameter_initalisieren}
Ein Schritt der getan werden muss, bevor das Training beginnt, ist das
Initialisieren aller Modellparameter, in diesem Fall die Gewichte und Neigungen.
Dies ist ein sehr essentieller Schritt, denn diese Startwerte entscheiden
erheblich über die Leistungsfähigkeit des Modells.
\para{}
Wie in Sektion (\ref{sec:gradientenverfahren}) gezeigt, muss am Anfang des
Gradientenverfahrens ein Startpunkt $\vec{p}_0$ innerhalb des Gradientenfeldes
$\vecf{\nabla}C$ gewählt werden, von welchem aus der Gradientenabstieg beginnt.
Da wir das Gradientenverfahren zur Optimierung der Modellparameter verwenden,
ist unser Startpunkt der Vektor aller Hyperparameter
$\vec{\param} = \trans{\begin{pmatrix} \param_1 & \cdots & \param_k \end{pmatrix}}$.
Dieser Startpunkt entscheidet darüber, in welches lokale Minimum konvergiert
wird und bestimmt somit auch die bestmögliche Exaktheit der Vorhersagen. Falls
schlechte Initialwerte gewählt werden, konvergiert der Punkt in ein hohes lokales
Minimum, was grosse Kostenfunktionswerte und schlechte Vorhersagen verursacht.
\para{}
Es ist nicht möglich im Vorhinein zu wissen, welche Initialwerte gute Resultate
liefern. Man muss ausprobieren, deshalb initialisert man gängierweise die
Hyperparameter mit Zufallswerten. Dafür nimmt man aber nicht irgendwelche
Zufallsvariabeln sondern man benutzt die Gauss'sche Normalverteilung
$\mathcal{N}(\mu,\sigma^2)$ bzw. ihre Dichtefunktion.
\[\ds \phi(x\ |\ \mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \text{exp} \left\{-\frac{{(x-\mu)}^2}{2\sigma^2}\right\} \]
\para{}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{gauss.pdf}
  \caption{Graph der Dichtefunktion $\phi(x\ |\ \mu=0,\sigma^2=1)$ mit ihren
    wichtigsten Eigenschaften}%
\end{figure}

Um die Neigungen $b_{t=0}$ zu initialisieren benutzt man eine Normalverteilung
$\mathcal{N}(0,1)$ mit Erwartungswert $\mu = 0$ und Varianz $\sigma^2 =
1$. Um die Gewichte $w_{t=0}$ zu initialisieren benutzt man ebenfalls eine
Normalverteilung mit Erwartungswert $\mu = 0$, jedoch wird die Varianz
$\sigma^2$ so skaliert, dass die Summe aller Zufallesvariabeln der Gewichte
einer Schicht eine Varianz von $\sigma^2_{tot} = 1$ hat. ERKLÄREN
\begin{align}
  w_{t=0}^l &\sim \mathcal{N}(\mu = 0, \sigma^2 = \frac{1}{|l|}) \\
  b_{t=0}^l &\sim \mathcal{N}(\mu = 0, \sigma^2 = 1)
\end{align}


\para{}
\cite{wiki:normal_distribution}
\cite{Nielsen}

\section{Rückwärtspropagierung}\label{sec:backpropagation}
Die wahre Herausforderung besteht beim Gradientenverfahren darin,
die partiellen Ableitungen der Modellparameter,
also die Komponenten des Gradienten, zu bestimmen.
Anders gesagt müssen alle Terme
$\ds\partderiv{C}{w_{j,k}^l}$, wie auch alle Terme $\ds\partderiv{C}{b_k^l}$
berechnet werden.
Das Verfahren zum bestimmen dieser Ausdrücke ist so spezifisch und aufwendig,
dass das Gradientenverfahren für KNNs einen eigenen Namen hat: die
\keyword{Rückwärtspropagierung} (engl.: backpropagation) (auch Fehlerrückführung).
Im Verlauf der Arbeit ist es nicht notwendig die Rückwärtspropagierung bis ins
Detail zu verstehen. Man sollte lediglich ein Gespür dafür entwickeln, wie
der Gradient berechnet wird.
\para{}
Da ein KNN, wie der Name es schon sagt, vernetzt ist, können die partiellen
Ableitungen einer Schicht bezüglich seiner Nachbarsschichten berechnet werden.
Dies ist auch der namensgebende Grundgedanke der Rückwärtsprogagierung: Man
beginnt in der letzten Schicht die partiellen Ableitungen zu bestimmen und
berechnet dann im Rückwärtsgang Schicht für Schicht die vorherigen
partiellen Ableitungen, bis zur Inputsschicht. \\
Dies macht man mithilfe der Kettenregel der Ableitungen.
Es ist sinnvoll für das Aufstellen dieser Gleichungen das Netzwerk als
\keyword{Computation Graph} zu betrachten.
\para{}
Ein Computation Graph ist eine Darstellung einer Verkettung von Funktionen als Netzwerk von Operationen.
Die Knoten im Graph stellen Variabeln dar und die Pfade, welche die Knoten
verbinden, sind die Funktionen, welche die Variabeln aufeinander abbilden. Die
Funktion wird auf die Variable angewandet, von der der Pfad ausgeht. Der Knoten
in welchem der Pfad endet, nimmt dann den Funktionswert an. Falls
mehrere Pfade in einem Knoten enden, werden die einzelnen Werte der Pfade
zusammenaddiert, um die Variable zu bilden. KETTENREGEL NOCHMALS ERWÄHNEN ODER
IN ABBILDUNG (LEIBNIZ-SCHREIBWEISE!)\\
In Abbildung (\ref{fig:computation_graph}) ist ein Beispiel eines Computation
Graphs zusammen mit der Herleitung der partiellen Ableitungen dargestellt.
\para{}
\begin{figure}[h!]
  \begin{minipage}[h!]{0.5\textwidth}
    \centering
    \begin{tikzpicture}[>=latex]

      \path (0,1) node [circle,draw](var_x){$x$};
      \path (0,-1) node [circle,draw](var_y){$y$};
      \path (2,0) node [circle,draw](var_a){$a$};
      \path (4,0) node [circle,draw](var_z){$z$};
      % \path[red] (0,1.5) node(x1){$x_1$} (0,0) node(x2){$x_2$} (0,-1.5) node(x3){$x_3$};
      \draw[->] (var_x) -- node[above,sloped]{$\sin(x)$} (var_a);
      \draw[->] (var_y) -- node[above,sloped]{$e^y$} (var_a);
      \draw[->] (var_a) -- node[above,sloped]{$a^3$} (var_z);

    \end{tikzpicture}

  \end{minipage}
  \begin{minipage}[h!]{0.5\textwidth}
    \begin{align*} % or align*
      a(x,y) &= \sin(x) + e^y \\
      z(a(x,y)) &= a^3(x,y) = \left(\sin(x) + e^y\right)^3 \\[3ex]
      \partderiv{z}{x} &= \partderiv{a}{x} \cdot  \partderiv{z}{a} = \cos(x) \cdot 3a^2 \\[0.5ex]
      \partderiv{z}{y} &= \partderiv{a}{y} \cdot \partderiv{z}{a} = e^y \cdot 3a^2
    \end{align*}
  \end{minipage}

  \caption{Computation Graph einer exemplarischen Verkettung von Funktionen}
  \label{fig:computation_graph}
\end{figure}
\para{}
Der erste Schritt der Rückwärtsprogagierung besteht darin, dass man die partiellen Ableitungen $\ds\partderiv{C}{z_j^l}$
der Kostenfunktion $C$ bezüglich den gewichteten Summen $z_j^l$ aller Schichten
berechnet. Daraus lässt sich dann später sehr einfach die partiellen Ableitungen
bezüglich den Gewichten $\ds\partderiv{C}{w_{j,k}^l}$ und bezüglich den Neigung
$\ds\partderiv{C}{b_j^l}$ berechnen.
\para{}
Zur Übersichtlichkeit definiert man einen \keyword{Fehler} $\delta_j^L$ für
jedes $j$-te Neuron in jeder $l$-ten Schicht, welcher die partielle Ableitung bezüglich der
gewichteten Summe dieses Neurons ist (siehe Gl. (\ref{eq:BP0}). Ebenfalls definiert man analog einen Fehlervektor
$\vec{\delta}^l$, welcher alle Fehler $\delta_j^l$ einer Schicht $l$
zusammenfasst (siehe Gl. (\ref{eq:BP0a})). Nun heisst es diesen für jedes Neuron jeder Schicht zu
berechnen.
\\
\begin{gather}
  \tag{BP0}\label{eq:BP0} \delta_j^l \coloneqq \partderiv{C}{z_j^l} \\
  \tag{BP0a}\label{eq:BP0a} \vec{\delta}^l \coloneqq \trans{\begin{pmatrix} \ds\partderiv{C}{z_1^l} & \ds\partderiv{C}{z_2^l} & \cdots & \ds\partderiv{C}{z_{|l|}^l} \end{pmatrix}}
\end{gather}
\\
Da die Kostenfunktion unmittelbar auf die letzte Schicht $L$ angewandt wird, beginnt
man auch dort mit der Berechnung des Fehlers $\vec{\delta}^L$.
Wir stellen nun einen Computation Graph auf, um die partiellen Ableitungen zu bilden.
\para{}
\begin{figure}[h!]
  \centering
  \begin{tikzpicture}[>=latex]
    % \colorlet{sumcolor}{red!70!textcolor}
    % \colorlet{actcolor}{green!70!textcolor}
    % \tikzstyle{act} = [fill=actcolor]
    % \tikzstyle{sum} = [fill=sumcolor]

    \path (0,2) node [circle,draw](z1){$z_1^L$};
    \path (0,0) node [circle,draw](z2){$z_2^L$};
    \path (0,-2) node [circle,draw](z3){$z_3^L$};
    \path (3,2) node [circle,draw](a1){$a_1^L$};
    \path (3,0) node [circle,draw](a2){$a_2^L$};
    \path (3,-2) node [circle,draw](a3){$a_3^L$};
    \path (6,0) node [circle,draw](C){$C$};

    \draw[->] (z1) -- node[above,sloped]{$\varphi(z_1^L)$} (a1);
    \draw[->] (z2) -- node[above,sloped]{$\varphi(z_2^L)$} (a2);
    \draw[->] (z3) -- node[above,sloped]{$\varphi(z_3^L)$} (a3);

    \draw[->] (a1) -- node[above,sloped]{$c(a_1^L,\hat{y}_1$)} (C);
    \draw[->] (a2) -- node[above,sloped,pos=0.4]{$c(a_2^L,\hat{y}_2$)} (C);
    \draw[->] (a3) -- node[above,sloped]{$c(a_3^L,\hat{y}_3$)} (C);

    % endings
    \node (beg1) at (-3,2) {};
    \node (beg2) at (-3,0) {};
    \node (beg3) at (-3,-2) {};

    \draw[dashed] ($(beg1)!0.6!(z1)$) -- (z1);
    \draw[dashed] ($(beg1)!0.6!(z2)$) -- (z2);
    \draw[dashed] ($(beg1)!0.6!(z3)$) -- (z3);
    \draw[dashed] ($(beg2)!0.6!(z1)$) -- (z1);
    \draw[dashed] ($(beg2)!0.6!(z2)$) -- (z2);
    \draw[dashed] ($(beg2)!0.6!(z3)$) -- (z3);
    \draw[dashed] ($(beg3)!0.6!(z1)$) -- (z1);
    \draw[dashed] ($(beg3)!0.6!(z2)$) -- (z2);
    \draw[dashed] ($(beg3)!0.6!(z3)$) -- (z3);
  \end{tikzpicture}
  \caption{Computation Graph zur Berechnung von $\vec{\delta}^L$}
  \label{fig:cg_L}
\end{figure}
\para{}
Wir können dem Computation Graph aus Abbildung (\ref{fig:cg_L}) entnehmen, dass die Kosten $C$ eine Funktion
in Abhängigkeit von den letzten Aktivierungen $a_j^L$ ist, welche wiederum in
Abhängigkeit von der jeweiligen gewichteten Summe $z_j^L$ berechnet werden.
Somit können wir mithilfe der Kettenregel die Beziehung (\ref{eq:BPh1}) aufstellen.
\\
\begin{equation}\label{eq:BPh1}
  \delta_j^L = \partderiv{C}{z_j^L} = \partderiv{C}{a_j^L} \cdot \partderiv{a_j^L}{z_j^L}
\end{equation}
\\
Da $a_j^L$ durch die Anwendung der Aktivierungsfunktion $\varphi$ auf $z_j^L$
gebildet wird, ist $\ds\partderiv{a_j^L}{z_j^L}$ einfach die Ableitung der Aktivierungsfunktion
$\varphi'(z_j^L)$. Somit erhalten wir die erste (\ref{eq:BP1}) von vier
wichtigen Gleichungen für die Rückwärtsprogagierung.
\\
\begin{equation}\tag{BP1}\label{eq:BP1}
  \delta_j^L = \partderiv{C}{a_j^L} \cdot \varphi'(z_j^L)
\end{equation}
\\
Nun möchten wir diese Ausdrücke wieder in Matrixschreibweise realisieren,
welche die ganze Schicht $L$ zusammenfasst. Dazu
muss eine neue Operation eingeführt werden: das Hadamard-Produkt.

\begin{defbox}{Hadamard-Produkt}
  Das Hadamard-Produkt (auch elementweises Produkt) ist ein spezielles Produkt zweier gleichgrossen Matrizen
  $\mat{A} \in \set{R}^{m \times n}$ und $\mat{B} \in \set{R}^{m \times n}$.
  Die resultierende Matrix ergibt sich aus der elementweisen Multiplikation der Ausgangsmatrizen.

  \begin{minipage}{0.5\textwidth}
    \begin{equation*}
      \mat{A} \odot \mat{B} =
      \begin{pmatrix}
        \matelem{A}_{1,1} \matelem{B}_{1,1} & \cdots & \matelem{A}_{1,n} \matelem{B}_{1,n} \\[0.3em]
        \vdots & \ddots & \vdots \\[0.3em]
        \matelem{A}_{m,1} \matelem{B}_{m,1} & \cdots & \matelem{A}_{m,n} \matelem{B}_{m,n} \\[0.3em]
      \end{pmatrix}
      \in \set{R}^{m \times n}
    \end{equation*}
  \end{minipage}
  %
  \begin{minipage}{0.5\textwidth}
    \begin{equation*}
      \vec{v} \odot \vec{w} =
      \begin{pmatrix}
        v_1 w_1 \\
        \vdots \\
        v_n w_n
      \end{pmatrix}
    \end{equation*}

  \end{minipage}
\end{defbox}
\para{}

Mit $\vecf{\varphi}'$ als die vektorisierte Ableitung der Aktivierungsfunktion,
könne wir den Fehlervektor der letzten Schicht nach Gleichung (\ref{eq:BPh0}) berechnen.

\begin{equation}\label{eq:BPh0}
  \vec{\delta}^L = \trans{\begin{pmatrix} \ds\partderiv{C}{a_1^L} & \ds\partderiv{C}{a_2^L} & \cdots & \ds\partderiv{C}{a_{|L|}^L}\end{pmatrix}} \odot \vecf{\varphi}'[\vec{z}^L]
\end{equation}

Dabei ist der erste Operand des Hadamard-Produkts nichts anderes als
der Gradient $\vecf{\nabla}_{\vec{a}^L} C$ der Kostenfunktion $C$ bezüglich dem Aktivierungsvektor
$\vec{a}^L$ der letzten Schicht. Dieser Gradient kann einfach berechnet werden, indem man die
vektorisierte Ableitungsfunktion für die gewählte Kostenfunktion bildet. Würde man die
Mittlere quadratischen Abweichung $C = \frac{1}{2|L|}(\vec{\hat{y}} - \vec{a}^L)^2$ als Kostenfunktion wählen, hätten wir
$\vecf{\nabla}_{\vec{a}^L} C = (\vec{a}^L - \vec{\hat{y}}) \cdot \frac{1}{|L|}$.
\para{}
Daraus folgt die kompakte matrix-version (\ref{eq:BP1a}) der Gleichung
(\ref{eq:BP1}), welche den Fehlervektor für die letzte Schicht berechnet.
\\
\begin{equation}\tag{BP1a}\label{eq:BP1a}
  \vec{\delta}^L = \vecf{\nabla}_{\vec{a}^L}C \odot \vecf{\varphi}'(\vec{z}^L)
\end{equation}
\\
Nun müssen wir eine rekursive Berechnungsmethode des Fehlers $\delta_j^{l-1}$
der vorherigen Schicht anhand des Fehlers $\delta_j^l$ der jetzigen Schicht
erarbeiten. Dafür stellen wir zuallererst wieder einen Computation Graph auf
(siehe Abb. (\ref{fig:cg_L-1})).
\para{}
\begin{figure}[h!]
  \centering
  \begin{tikzpicture}[>=latex]
    \path (0,1) node [circle,draw](z1-){$z_1^{l-1}$};
    \path (0,-1) node [circle,draw](z2-){$z_2^{l-1}$};
    \path (3,1) node [circle,draw](a1-){$a_1^{l-1}$};
    \path (3,-1) node [circle,draw](a2-){$a_2^{l-1}$};
    \path (7,2) node [circle,draw](z1){$z_1^l$};
    \path (7,0) node [circle,draw](z2){$z_2^l$};
    \path (7,-2) node [circle,draw](z3){$z_3^l$};

    \draw[->] (z1-) -- node[above,sloped]{$\varphi(z_1^L)$} (a1-);
    \draw[->] (z2-) -- node[above,sloped]{$\varphi(z_2^L)$} (a2-);

    \draw[->] (a1-) -- node[sloped,above]{$\times w_{1,1}$} (z1);
    \draw[->] (a1-) -- node[sloped,above,pos=0.8]{$\times w_{2,1}$} (z2);
    \draw[->] (a1-) -- node[sloped,below,pos=0.65]{$\times w_{3,1}$} (z3);
    \draw[->] (a2-) -- node[sloped,above,pos=0.65]{$\times w_{1,2}$} (z1);
    \draw[->] (a2-) -- node[sloped,above,pos=0.65]{$\times w_{2,2}$} (z2);
    \draw[->] (a2-) -- node[sloped,below]{$\times w_{3,2}$} (z3);

    % endings
    \node (beg1) at (-3,1) {};
    \node (beg2) at (-3,-1) {};
    \draw[dashed] ($(beg1)!0.6!(z1-)$) -- (z1-);
    \draw[dashed] ($(beg1)!0.6!(z2-)$) -- (z2-);
    \draw[dashed] ($(beg2)!0.6!(z1-)$) -- (z1-);
    \draw[dashed] ($(beg2)!0.6!(z2-)$) -- (z2-);

    \draw[dashed] (z1) -- ++(1.5,0);
    \draw[dashed] (z2) -- ++(1.5,0);
    \draw[dashed] (z3) -- ++(1.5,0);

  \end{tikzpicture}
  \caption{Computation Graph zur Berechnung von $\delta_j^{l-1}$}
  \label{fig:cg_L-1}
\end{figure}
\para{}
Es gilt erneut die Gleichung (\ref{eq:BPh1}) für die Berechnung des Fehlers $\delta_j^{l-1}$.
\\
\begin{equation}\tag{\ref{eq:BPh1}}
  \delta_j^{l-1} = \partderiv{C}{z_j^{l-1}} = \partderiv{a_j^{l-1}}{z_j^{l-1}} \cdot \partderiv{C}{a_j^{l-1}}
\end{equation}
\\
Zürst einmal ist der erste Faktor wieder die Ableitung $\varphi'(z_j^{l-1})$ der Aktiverungsfunktion.
Desweiteren beeinflusst beim Übergang der Schicht ($l-1$) zur Schicht $l$ eine Aktivierungen
$a_j^{l-1}$ alle gewichteten Summen $z_k^l$. Mit der Kettenregel folgt daher,
dass die partielle Ableitung $\ds\partderiv{C}{a_j^{l-1}}$ die Summe aller
$\ds\partderiv{C}{z_k^l} \cdot \ds\partderiv{z_k^l}{a_j^{l-1}}$ sein muss. Wir
erhalten Gleichung (\ref{eq:BPh2}).
\\
\begin{equation}\label{eq:BPh2}
  \delta_j^{l-1} = \varphi'(z_j^{l-1}) \cdot \sum_{k=1}^{|l|} \left( \partderiv{C}{z_k^l} \cdot \partderiv{z_k^l}{a_j^{l-1}} \right)
\end{equation}
\\
Um die gewichtete Summe $z_k^l$ zu bilden, multipliziert man einfach die
Aktivierung $a_j^{l-1}$ der vorherigen Schicht mit den entsprechenden Gewichten $w_{k,j}^{l-1}$.
Dadurch ist diese partielle Ableitung $\ds\partderiv{z_k^l}{a_j^{l-1}}$ gerade das
Gewicht selbst. Desweiteren ist $\ds\partderiv{C}{z_k^l}$ per Definition der
Fehler $\delta_k^l$. Mit dieser Erkenntnis erhalten wir die zweite essentielle
Gleichung (\ref{eq:BP2}) für die Rückwärtsprogagierung.
\\
\begin{equation}\tag{BP2}\label{eq:BP2}
  \delta_j^{l-1} = \varphi'(z_j^{l-1}) \cdot \sum_{k=1}^{|l|} \left( \delta_k^l \cdot w_{k,j}^{l-1} \right)
\end{equation}
\\
Auch diese Gleichung hätten wir gerne in der Matrixschreibweise. Wir beginnen
mit der Erweiterung auf alle gewichteten Summen.
\\
\begin{equation*}
  \vec{\delta}^{l-1} = \vecf{\varphi}'[\vec{z}^{l-1}] \odot \trans{\begin{pmatrix} \ds\sum_{k=1}^{|l|} w_{k,1}^{l-1} \cdot \delta_k^l & \cdots & \ds\sum_{k=1}^{|l|} w_{k,|l-1|}^{l-1} \cdot \delta_k^l \end{pmatrix}}
\end{equation*}
\\
Der zweite Operant des Hadamard-Produkts ist hierbei gerade das Produkt der
Matrixmultiplikation zwischen
der transponierten Gewichtsmatrix $\trans{(\mat{W}^{l-1})}$ der Schicht ($l-1$)
und dem Fehlervektor $\vec{\delta}^l$ der Schicht $l$ (ersichtlich in folgender Gleichung).

\begin{gather*}
  \trans{\begin{pmatrix} \ds\sum_{k=1}^{|l|} w_{k,1}^{l-1} \cdot \delta_k^l & \cdots & \ds\sum_{k=1}^{|l|} w_{k,|l-1|} \cdot \delta_k^l \end{pmatrix}} =
  \begin{pmatrix}
    w_{1,1}^{l-1} & w_{2,1}^{l-1} & \cdots & w_{|l|,1}^{l-1} \\
    w_{1,2}^{l-1} & w_{2,2}^{l-1} & \cdots & w_{|l|,2}^{l-1} \\
    \vdots & \vdots & \ddots & \vdots \\
    w_{1,|l-1|}^{l-1} & w_{2,|l-1|}^{l-1} & \cdots & w_{|l|,|l-1|}^{l-1}
  \end{pmatrix}
  \trans{\begin{pmatrix} \delta_1^l & \cdots & \delta_{|l|}^l \end{pmatrix}} \\=
  \trans{\begin{pmatrix}
      w_{1,1}^{l-1} & w_{1,2}^{l-1} & \cdots & w_{1,|l-1|}^{l-1} \\
      w_{2,1}^{l-1} & w_{2,2}^{l-1} & \cdots & w_{2,|l-1|}^{l-1} \\
      \vdots & \vdots & \ddots & \vdots \\
      w_{|l|,1}^{l-1} & w_{|l|,2}^{l-1} & \cdots & w_{|l|,|l-1|}^{l-1}
    \end{pmatrix}}
  \vec{\delta}^l = \trans{(\mat{W}^{l-1})} \vec{\delta}^l
\end{gather*}

Nun haben wir unsere rekursive Fehlerdefinition in Matrixschreibweise und somit
die kompakte Version (\ref{eq:BP2a}) der zweiten wichtigen Formel (\ref{eq:BP2}).
\\
\begin{equation}\tag{BP2a}\label{eq:BP2a}
  \vec{\delta}^{l-1} = (\trans{(\mat{W}^{l-1})} \vec{\delta}^l) \odot \vecf{\varphi}'[\vec{z}^{l-1}]
\end{equation}
\\
In einem letzten Schritt müssen wir jetzt noch Formeln herleiten, mit welchen
man anhand des Fehlers $\delta_j^l$ die partiellen Ableitungen der Gewichte und
der Neigungen berechnen kann.
\para{}
Eine Neigung $b_j^l$ ist Funktionsbestandteil der entsprechenden gewichteten
Summe $z_j^{l+1}$. Somit gilt für die Neigung Formel (\ref{eq:BPh3}).
\\
\begin{equation}\label{eq:BPh3}
  \partderiv{C}{b_j^l} = \partderiv{C}{z_j^{l+1}} \cdot \partderiv{z_j^{l+1}}{b_k^l}
\end{equation}
\\
Der erste Term ist hierbei per Definition unser Fehler $\delta_j^{l+1}$ und der
zweite Term lässt sich zu 1 evaluieren, da die Summe $z_k^{l+1}$ nur aus
$b_k^l$ besteht und aus Summanden, welche für die partielle Ableitung als konstant gelten.
Somit ist die Ableitung der Neigung gerade unser Fehler und wir erhalten die
dritte (\ref{eq:BP3}) von vier essentielen Gleichungen.
\\
\begin{equation}\tag{BP3}\label{eq:BP3}
  \partderiv{C}{b_j^l} = \delta_j^{l+1}
\end{equation}
\\
Somit ist der Kostengradient bezüglich der Neigung gerade der Fehlervektor
(siehe Gl. (\ref{eq:BP3a})).
\\
\begin{equation}\tag{BP3a}\label{eq:BP3a}
  \vecf{\nabla}_{\vec{b^l}} C =  \vec{\delta}^{l+1}
\end{equation}
\\
Ein Gewicht $w_{j,k}^l$ ist ebenfalls ein Funktionsbestandteil der assozierten
gewichteten Summe $z_j^{l+1}$. Dadurch gilt für die partiellen Ableitungen der
Kosten nach dem Gewicht Gleichung (\ref{eq:BPh4}).
\\
\begin{equation}\label{eq:BPh4}
  \partderiv{C}{w_{j,k}^l} = \partderiv{C}{z_j^{l+1}} \cdot \partderiv{z_j^{l+1}}{w_{j,k}^l}
\end{equation}
\\
Dabei lässt sich der erste Teil wieder zum Fehler $\delta_j^{l+1}$ evaluieren.
Die zweite partielle Ableitung ist gerade die Aktivierung $a_k^l$, da sich die
gewichtete Summe aus der Multiplikation des Gewichtes mit der Aktivierung ergibt.
Somit erhalten wir die letzte der vier essentiellen Gleichungen (\ref{eq:BP4}).
\begin{equation}\tag{BP4}\label{eq:BP4}
  \partderiv{C}{w_{j,k}^l} = \delta_j^{l+1} \cdot a_k^l
\end{equation}
\\
Die Matrix-version davon ist Gleichung (\ref{eq:BP4a}).
\begin{equation}\tag{BP4a}\label{eq:BP4a}
  \vecf{\nabla}_{\mat{W}^l} C = \vec{\delta}^{l+1} \trans{(\vec{a^l})}
\end{equation}

ALLES ZUSAMMENFASSEN

\cite{Nielsen}


\section{Universal Approximation Theorem}\label{sec:UAT}
Es stellt sich nun die Frage, was ein Neuronales Netz alles erlernen kann.
Diese Frage kann man mit dem \keyword{Universal Approximation Theorem} (UAT)
beantworten. Dieser Satz ist ein mathematischer Beweis dafür, dass ein KNN
grundsätzlich in der LAge ist jede kontinürliche Funktion beliebig gut zu
approximieren. Er wurde von Kurt Hornik in 1991 beweisen.

Etwas genaür besagt der UAT, dass ein Feed-Forward Netzwerk mit einer einzigen
Zwischenschicht, welche eine endliche Anzahl Neuron hat, jede Kontinuierliche
Funktion approximieren kann. Voraussetzung dafür ist, dass es sich bei den
Neuronen und Nicht-lineare-Neuronen handelt. Dies ist nur eine theoretische
Ausage über das Lernpotenzial eines KNNs. Jedoch macht es keinerlei Aussage
darüber, ob ein KNN wirklich die Funtkionen erlernen wird.

IAN GOODFELLOW:
A feedworward network with a single layer is sufficient to represent any
function, but the layer may be infeasibly large and may fail to lean and
generalize correctly.

Die meisten Klassen von Probleme können einfach als eine Funktion formuliert
werden. Somit bedeutet der UAT, dass ein KNN theoretisch jedes Problem lösen kann.

Der eigentliche Beweis ist mathematisch ziemlich anspruchsvoll, deshalb werden
wir diesen nicht behandeln.

AUF ANDERE RESOURCE (QR-CODE) VERWEISEN FÜR ILLUSTRIEVEN BEWEIS ODER ANHANG VON
MIR GESCHRIEBEN

\cite{wiki:uat}


\pagebreak
\chapter{Convolutional Neural Networks}
Viele Anwendungen von Machine Learning sind verbunden mit Bild- oder
Audioverarbeitung, wie z.B Bildklassifizierung, Gesichtserkennung oder
Spracherkennung.
Vorallem aber für hochauflösende Bilder sind die KNNs, die wir söben
kennengelernt haben, nicht geeignet. Sie sind zum Teil gar nicht in der
Lage eine Korrelation zwischen den Inputs und Outputs zu erlernen.
Um diesen Umstand zu erklären, wird nun ein kleines Beispielmodell erlaütert:
\para{}
\label{sec:CNN_parameter_problem}
Es soll ein KNN designed werden, welches eine Photographie klassifizieren
soll, ob darauf ein Hund sichtbar ist oder nicht. Wir nehmen für dieses
Gedankenexperiment bereits ein relativ niedrig aufgelöstes Bild mit $256 \times 256$
Pixel, dies entspricht weniger als $0.07$ Megapixel (ein iPhone XS hat eine Kamera mit
12 Megapixel). Um die verschiedenen Farben zu codieren besitzt jeder Pixel drei Komponten: R, G
und B. Somit hat dieses Bild insgesamt $256 \times 256 \times 3 = 196'608$
Komponenten. Jede Komponente ist ein Feature welches das KNN zu verrechnen hat. Somit bestünde
die erste Schicht des Netzwerkes aus fast $200'000$ Neuronen. Um diese Schicht
nun mit seiner Nachbarsschicht, welche gleiche Dimensionen besitzt, zu verbinden, braucht
es $196'608 \times 196'608 = 38'654'705'664$ Verbindungen und damit gleich so
viele Gewichte! Für ein Netwerk ohne eine einzige Zwischenschichten gäbe es
also über 38 Milliarden Modellparameter zu erlernen! Dass dies nicht realistisch ist,
sollte auf der Hand liegen.
\para{}
Nicht nur die Anzahl Modellparameter sind ein Problem für KNNs in der
Bildverarbeitung, sondern es bestehen noch weiter Probleme.
Trotzdem sollte nun klar sein, dass eine andere Modellarchitektur nötig ist, um Machine
Learning auf Bilder anzuwenden. Für genau solche Anwendungen wurde eine modifizierte
Version eines KNNs entwickelt: das \keyword{Convolutional Neural Network} (CNN).
Im Allgemeinen sind CNNs immer dann geeignet, wenn es Daten zu verarbeiten gibt, welche eine
rasterartige Form haben, wie eben z.B Bilder.
Diese Art von Netzwerk macht Gebrauch von Konzepten aus der klassischen
Bildverarbeitung, wie sie mit beispielsweise Photoshop gemacht werden kann.
Wie beim Perzeptron und bei den klassischen KNNs auch, ist die Architektur
biologisch inspiriert.
Der folgende Abschnitt wird die Funktionsweise eines solchen CNNs erklären.
\para{}
\cite{Goodfellow-et-al-2016}
\cite{deeplearning.ai:cnn}
\cite{wiki:cnn}


\section{Bilder als Tensoren}\label{sec:tensor}
CNNs operieren an Bildern. Diese Bilder stellen den Input für die Modelle dar.
Um mit diesen Bildern rechnen zu können, ist es sinnvoll, sie als sogenannte
\keyword{Tensoren} vom Rang 3 zu untersuchen, anstatt sie als Anordnungen von
Pixeln zu betrachten. \\
Um zu verstehen, was ein Tensor dritten Ranges ist, müssen wir zürst verstehen, was ein Tensor im Allgemeinen ist.

\begin{defbox}{Tensor}
  Ein Tensor $\ten{T}$ ist eine Verallgemeinerung von Skalaren, Vektoren und Matrizen auf
  $n$ Dimensionen. Es handelt sich wie bei Matrizen um
  eine Zahlenanordnung. Dabei wird die Anzahl Dimensionen, innerhalb welchen die
  Zahlen liegen als Rang oder Stufe $n$ des Tensors bezeichnet. Vorstellen kann man sich einen Tensor
  als ein Hyperrechteck mit $n$ Dimensionen, innerhalb dessen die Zahlen in
  einem Raster angeordnet sind. Diese Zahlen sind die Elemente des Tensors.
  Ein Tensor nullten Ranges ist ein Skalar, einer erster Stufe ein Vektor und
  einer mit Rang 2 ist eine normale Matrix.
  \begin{gather*}
    1 \in \set{R} \text{ (Skalar)} \quad \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix}
    \in \set{R}^3 \text{ (Vektor)} \quad
    \begin{pmatrix}
      1 & 2 & 3 \\
      4 & 5 & 6 \\
    \end{pmatrix} \in \set{R}^{2 \times 3} \text{ (Matrix)}
  \end{gather*}
\end{defbox}

\begin{defbox}{Tensor 3. Ranges}
  Ein Tensor $\ten{T} \in \set{R}^{h \times w \times d}$ mit Rang 3 ist eine 3-dimensionale Zahlenanordnung. Man kann sich
  diesen Tensor einfach als eine 3D-Matrix vorstellen; ein Volumen innerhalb
  dessen Zahlen in einem Raster angeordet sind.
  Analog zum Volumen bezeichnet man die Form des Tensors mit Höhe, Breite und
  Tiefe. \\
  Ein Tensor von der Form $\set{R}^{3 \times 3 \times 3}$ könnte folgendermassen
  aussehen:
  \para{}



  % \begin{tikzpicture}[x=(15:.5cm), y=(90:.5cm), z=(330:.5cm), >=stealth]
  %   \draw (0, 0, 0) -- (0, 0, 10) (4, 0, 0) -- (4, 0, 10);
  %   \foreach \z in {0, 5, 10} \foreach \x in {0,...,3}
  %   \foreach \y [evaluate={\b=random(0, 1);}] in {0,...,3}
  %   \filldraw [fill=white] (\x, \y, \z) -- (\x+1, \y, \z) -- (\x+1, \y+1, \z) --
  %   (\x, \y+1, \z) -- cycle (\x+.5, \y+.5, \z) node [yslant=tan(15)] {\b};
  %   \draw [dashed] (0, 4, 0) -- (0, 4, 10) (4, 4, 0) -- (4, 4, 10);
  %   \draw [->] (0, 4.5, 0)  -- (4, 4.5, 0)   node [near end, above left] {Column};
  %   \draw [->] (-.5, 4, 0)  -- (-.5, 0, 0)   node [midway, left] {Row};
  %   \draw [->] (4, 4.5, 10) -- (4, 4.5, 2.5) node [near end, above right] {Channel};
  % \end{tikzpicture}

\end{defbox}
\newcommand{\arrayfilling}[2]{
  \fill[#2!30, opacity=.5] ([shift={(1mm,1mm)}]#1.north west) coordinate(#1auxnw)--([shift={(1mm,1mm)}]#1.north east)coordinate(#1auxne) to[out=-75, in=75] ([shift={(1mm,-1mm)}]#1.south east)coordinate(#1auxse)--([shift={(1mm,-1mm)}]#1.south west)coordinate(#1auxsw) to[out=105, in=-105] cycle;
  \fill[#2!80!black, opacity=1] (#1auxne) to[out=-75, in=75] (#1auxse) to[out=78, in=-78] cycle;
  \fill[#2!80!black, opacity=1] (#1auxnw) to[out=-105, in=105] (#1auxsw) to[out=102, in=-102] cycle;
}

\begin{tikzpicture}[font=\ttfamily,
  mymatrix/.style={
    matrix of math nodes, inner sep=0pt, color=#1,
    column sep=-\pgflinewidth, row sep=-\pgflinewidth, anchor=south west,
    nodes={anchor=center, minimum width=5mm,
      minimum height=3mm, outer sep=0pt, inner sep=0pt,
      text width=5mm, align=right,
      draw=none, font=\small},
  }
  ]

  \matrix (C) [mymatrix=green] at (6mm,5mm)
  {0 & 1 & 0 \\ -1 & 0 & 0\\ 0 & 0 & 0\\};
  \arrayfilling{C}{green}

  \matrix (B) [mymatrix=red] at (3mm,2.5mm)
  {0 & 0 & -1 \\ 0 & 0 & 0\\ 1 & 0 & 0\\};
  \arrayfilling{B}{red}

  \matrix (A) [mymatrix=purple] at (0,0)
  {0 & 0 & 0 \\ 0 & 0 & 1\\ 0 & -1 & 0\\};
  \arrayfilling{A}{purple}

  \foreach \i in {auxnw, auxne, auxse, auxsw}
  \draw[brown, ultra thin] (A\i)--(C\i);

  \node[left=1mm of B.west] {$\ten{T} =$};
  \node[right=3mm of B.east] {$\in \set{R}^{3 \times 2 \times 3}$};
\end{tikzpicture}
\para{}
Ein Bild kann somit einfach als Tensor dritten Ranges $\ten{B} \in \set{R}^{h
  \times w \times c}$, der Form $(\text{Bildhöhe} \times \text{Bildbreite}
\times \text{Anzahl Farbkomponenten})$ betrachtet werden.
Elemente der Matrix nehmen dann einfach die Werte der Pixelkomponenten an.
Ein schwarzweiss Bild hat nur eine Komponente, welche die Helligkeit angibt.
Somit wäre es eine normale 2D-Matrix $\mat{B} \in \set{R}^{h \times w}$ (siehe
Abb. (\ref{fig:bildmatrix})).
\para{}
\begin{figure}[h!]
  \begin{tikzpicture}

  \end{tikzpicture}
  \caption{Beispiel einer Bildmatrix}
  \label{fig:bildmatrix}
\end{figure}

\cite{deeplearning.ai:cnn}
\cite{wiki:tensor}

\section{Topologie von CNNs}
Ein CNN besteht, wie auch ein KNN, aus Schichten. Jede dieser Schichten erhält
als Input ein Bild und hat auch wieder eines als Output. \\
Ein wichtiger Unterschied eines CNNs ist, dass es aus unterschiedlichen Typen von
Schichten besteht. Man unterscheiden zwischen vier Arten von Schichten:
\begin{itemize}
\item{\keyword{Fully-connected-Schichten},}
\item{\keyword{Convolutional-Schichten},}
\item{\keyword{Pooling-Schichten}, und}
\item{\keyword{Upsampling-Schichten}.}
\end{itemize}
Die Fully-Connected-Schicht ist altbekannt und ist einfach die klassische Schicht
eines KNNs, bestehend aus Neuronen. Aus diese werden wir nicht weiter eingehen,
da wir sie schon im vorherigen Kapitel kennengelernt haben. \\
Die Convolutional-Schicht ist eine neuartige Schicht, welche es im KNN nicht
gibt. Sie ist die Schicht, welche für das Training relevant ist,
da sie die Modellparameter beinhaltet. Sie extrahiert die relevanten Features
aus den Inputbildern und lernt so die Bilder zu verstehen. \\
Die Pooling-Schicht, wie auch die Upsampling-Schicht, beinhaltet keine
Modellparameter und sind deshalb für das Training nicht direkt relevant.
Diese Schichten werden lediglich gebraucht, um die verarbeiteten Bilder neu zu
skalieren. Dies ist sinnvoll, da durch die Extraktion gewisser Features, die
restlichen Features wegfallen. Somit muss weniger Information pro Bild gehalten
werden und die Bilder sollten schrumpfen, um Overfitting vorzubeugen. Die Pooling-
Schichten reduzieren die Bilder und die Upsampling-Schichten erweitern die
Bilder um mehr Pixel. Dies wird in Sektion (\ref{sec:autoencoder}) für die Autoencoder
hilfreich werden. \\
Grundsätzlich folgt auf eine Convolutional-Schicht immer entweder eine Pooling-
oder eine Upsampling-Schicht. Somit bilden sie gewissermassen eine Einheit.
\para{}
In Abbildung (\ref{fig:cnn_topology}) ist ein Schema eines CNNs abbgebildet.
\begin{figure}[h!]

  \caption{Schichtung eines CNNs}
  \label{fig:cnn_topology}
\end{figure}

\para{}
\cite{Goodfellow-et-al-2016}
\cite{deeplearning.ai:cnn}
\cite{wiki:cnn}


\section{Convolutional-Schichten und Filter}
Zürst betrachten wir die Convolutional Schicht. Diese Schicht macht ausgiebig
Gebrauch von sogenannten Filtern. Diese werden in diesem Abschnitt behandelt.

\subsection{Filter in der Bildverarbeitung}
\keyword{Filter} (auch Kerne) sind in der Bildverarbeitung sehr verbreitet. Jeder kennt sie entweder
von Photoshop, von Instagram oder sonstigen Bildbearbeitungsprogrammen.
Auch CNNs machen Gebrauch von solchen Filtern, in diesem Fall um die Features eines Bildes zu
erlernen.
\begin{figure}[h!]

  \caption{der Instagramfilter: Blablabla}
\end{figure}

\para{}
Ein Filter ist einfach eine Region, deutlich kleiner als das verarbeitete Bild
$\ten{B}$, welche
über alle Pixel wandert, diese manipuliert und so wieder ein neues Bild
$\tilde{\ten{B}}$ liefert.
Mathematisch gesehen handelt es sich bei einem Filter einfach um einen Tensor,
den sogennanten \keyword{Filtertensor} $\ten{F}$ oder Faltungstensor (siehe Abb.
(\ref{fig:filtermatrix})). Solche Filter sind immer quadratisch, wobei ihre
Zeilen- und Spaltenlänge mit $f$ bezeichnet wird. Dabei ist $f$ immer eine ungerade Zahl, damit ein
Element, der sogennante \keyword{Zentralelement} (engl.: Center element), bezeichnet mit $(\ten{F})_C$,
immer im Zentrum des Filters liegt. \\

\begin{figure}[h!]
  % Sourcepixel kennzeichnen
  \caption{eine 2D-Filtermatrix}
  \label{fig:filtermatrix}
\end{figure}
\para{}
Das Verhalten des Filters wird durch seine Matrixeinträge bestimmt.
Die Elemente können also so gewählt werden, dass wenn der Filter auf ein Bild
angewandt wird, er bestimmte Features aus dem Bild hervorhebt. Solche Features
könnten zum Beispiel Ränder oder Kanten sein, welche betont werden.
\para{}
Ein gutes Beispiel ist der Sobel-Filter. Er ist ein typischer 2D-Kantendetektions-Filter mit einer
Grösse von $f = 3$. Wir betrachten nun den Sobelfilter $\mat{G}_x$, welcher
horizontale Kanten akzentuiert. Die Einträge dieses Filters lauten wie folgt:
\begin{equation*}
  \mat{G}_x =
  \begin{pmatrix}
    -1 & 0 & +1 \\
    -2 & 0 & +2 \\
    -1 & 0 & +1 \\
  \end{pmatrix}
\end{equation*}

Angewandt auf ein schwarz-weisses Bild, erhält man ein neues Bild, bei welchem alle erkannten
Kanten weiss eingefärbt sind und die restlichen Bereiche schwarz sind.
Der Filter erkennt dabei jede Stelle als Kante, welche zwei Regionen mit
genug grossem Kontrast voneinander trennt.
In Abbildung (\ref{fig:sobel_filter}) wurde der horizontale Sobelfilter auf ein
Beispielbild angewandt. Links ist das Ursprungsbild und rechts ist das
prozessierte Bild.


\begin{figure}[h!]

  \caption{Sobel-Filter angewandt auf Beispielsbild}
  \label{fig:sobel_filter}
\end{figure}

\para{}
\cite{wiki:sobel_operator}
\cite{deeplearning.ai:cnn}
\cite{wiki:kernel}


\subsection{Filter in CNNs}
Aus didaktischen Gründen betrachten wir nun zürst
nur 2D-Filter, welche sich für graustufige Bilder
$\mat{B} \in \set{R}^{h \times w}$ eignen. Somit ist der hierfür geeignete
Filter einfach eine 2D-Matrix, bezeichnet mit $\mat{F} \in \set{R}^{f \times
  f}$. Man bedenke bei jeder Filtermatrix einfach immer, dass es sich
eigentlich im allgemeinen Fall um einen 3D-Tensor handelt.
\para{}
Wie wir vorhin erfahren haben, können Filter genutzt werden um gewisse
Features eines Bildes hervorzuheben und andere Features zu ignorieren. Dabei
bestimmen die Filtereinträge, welche Features extrahiert werden. Somit besteht
die Aufgabe darin, die richtigen Filtereinträge zu bestimmen, damit die
gewünschten Features erkannt werden und auf die gewünschten Outputs abgebildet
werden. Naheliegend sollte jetzt der Gedanke sein, die Filtereintreage als die
Modellparameter eines CNNs zu definieren.
Mithilfe des Gradietenverfahrens sollten erneut diese Parameter
erlernt werden. \\
Da die Filter die gleiche Funktion haben, wie die Gewichte in einem KNN,
bezeichnet man die Filter in einem CNN mit $\mat{W}$. Die Grösse des Filters
stellt dabei einen Hyperparameter dar.

\begin{equation*}
  \mat{W} = \begin{pmatrix}
    w_{1,1} & \cdots & w_{1,f} \\
    \vdots & \ddots & \vdots \\
    w_{f,1} & \cdots & w_{f,f}
  \end{pmatrix}
\end{equation*}


\subsection{Filteroperation intutitiv}\label{sec:filteroperation_intuitiv}
Wie wendet man nun einen solchen Filter auf ein Bild an? Um das Prozedere einer
Filteroperation leichter verständlich zu machen, wird es erneut für ein
graustufiges Bild $\mat{B} \in \set{R}^{h \times w}$ erlaütert. Später werden auch noch farbige
Bilder erklärt.
\para{}
Bei einer Filteroperation, wendet man einen Filter $\ten{W}$ auf einen Bildtensor
$\ten{B}$ an und erhält so ein neues Bild $\tilde{\ten{B}}$. Man schreibt dafür:
$\tilde{\ten{B}} = \ten{W} * \ten{B}$.
\para{}
Um nun diese Operation zu erklären, verwenden wir als Beispiel eine
2D-Filtermatrix $\mat{W} \in \set{R}^{3 \times 3}$, bei welcher
$f = 3$ gewählt wurde.
Wie bereits oben erwähnt, wandert der Filter über das Bild. Dabei befindet
sich der Filter immer über einer Region des Bilder, welche gleich gross ist,
wie der Filter selber (in diesem Fall ($3 \times 3$)), sodass den Filtermatrixeinträgen immer eindeutig
Bildmatrixeinträge entsprechen. Diese Region des Bildes bezeichnet man als das
\keyword{rezeptives Feld} des Filters (siehe Abb. (\ref{fig:receptive_field})).
Das Rezeptive Feld, bezeichnet mit $\hat{\mat{B}} \in \set{R}^{3 \times 3}$ ist also einfach eine
Untermatrix der Obermatrix $\mat{B}$.
\para{}
Jedem Element des Filters entspricht ein Element des rezeptiven Feldes. Dem
Zentralelement $(\mat{W})_c$ entspricht dabei gerade dem sogenannten Qüllenpixel
(engl.: source pixel) $(\ten{B})_s$ des Bildes. Ihn verwenden wir für eine
Namenskonvention für das rezeptive Feld. Wir bezeichnen das rezeptive Feld mit
$\hat{\mat{B}}^{(y,x)}$, wobei die hochgestellten Indizes $(y,x)$ die Position
des Qüllenpixel $(\mat{B})_S$ im Bild $\mat{B}$ angibt.

\begin{figure}[h!]

  % Zentralelement einzeichnen mit Beschriftung
  \caption{ein Filter und sein rezeptives Feld}
  \label{fig:receptive_field}
\end{figure}

\para{}
Der Filter beginnt nun oben links über das Bild zu wandern. Somit
hat der Filter zu beginn das rezeptive Feld $\hat{\mat{B}}^{(2,2)}$, denn der
Filter darf nicht über die Ränder des Bildes hinausragen.
Nun werden die Elemente der Filters mit den Elementen des rezeptiven Feldes
verrechnet, welche die gleichen Dimensionen besitzen. Dabei wird einfach das
Hadamard-Produkt (das elementweise Produkt) des beiden Matrizen miteinander
gebildet. Nun erhalten wir eine neue Matrix $\mat{W} \odot
\hat{\mat{B}}^{(2,2)} = \mat{P} \in \set{R}^{3 \times 3}$.
Nun werden noch alle Elemente der neuen Matrix $\mat{P}$
aufsummiert. Diese Zahl ist dann der Grauwert des ersten Pixels $(\tilde{\mat{B}})_{1,1}$ des bearbeiteten
Bildes (siehe Gl. (\ref{eq:filter1})). In diesem Sinne stellt der Filter eine Gewichtung der Nachbarspixel dar, welche
bestimmt, wie die neuen Pixel aussehen.
\\
\begin{equation}\label{eq:filter1}
  (\tilde{\mat{B}})_{1,1} = \sum_{y=1}^3 \sum_{x=1}^3 (\mat{P})_{y,x}
\end{equation}
\\
Nun wird der Filter um ein Element nach rechts verschoben und die gleiche
Prozedur angewandt, um den zweiten Pixel zu berechnen. Dies
wird durchgezogen, bis eine ganze Zeile der Bildmatrix durchstreift wurde.
Danach wird der Filter wieder nach ganz links verschoben und er bewegt sich ein
Element nach unten. Das geschieht, bis das ganze Bild verrechnet wurde.
Dabei werden die Elemente des ursprünglichen Bildes durchaus mehrmals
verrechnet, da es zu einer Überlappung des vorherigen Filters und des
verschobenen Filters kommt.
\para{}

\ifcp
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.7\textwidth]{conv.pdf}
  \caption{Schema, wie ein Filter über ein Bild laüft}
\end{figure}
\fi

Es ist zu bemerken, dass das neue Bild $\tilde{\mat{B}} \in \set{R}^{(h-2) \times (w-2)}$ nicht mehr die gleichen Masse
hat, wie das Ursprungsbild $\mat{B} \in \set{R}^{h \times w}$. Das liegt daran, dass pro
Lage des Filters jeweils nur ein Pixel des neuen Bildes berechnet wird. Man kann
sich vorstellen, dass an der Position des Qüllenpixel jeweils ein neuer
Pixel entsteht. Dabei muss der Filter immer eine vollständige Region als
rezeptives Feld haben. Dadurch kommt das Zentralelement nie auf die Ränder des
Bildes zu liegen. Somit fallen diese weg.
\para{}
\cite{deeplearning.ai:cnn}
\cite{wiki:convolution}

\subsection{Filteroperationen als diskrete Faltungen}
Eigentlich handelt es sich bei einer Anwendung eines Filters auf ein Bild um
eine spezifische mathematische Operation: eine \keyword{diskrete Faltung} (engl.:
\keyword{convolution}).
\footnote{
  Streng genommen wird in CNNs nicht eine diskrete Faltung durchgeführt, sondern
  eine sogenannte \keyword{Kreuzkorrelation}. Der Unterschied dabei ist, dass bei einer
  Kreuzkorrelation die Faltungsmatrix nicht zürst horizontal, wie auch vertikal
  gespiegelt wird, im Gegensatz zu der eigentlichen Faltung. Jedoch verwendet man
  begrifflich das Wort Faltung für beide Operationen. Im Verlauf dieser Arbeit
  wird mit dem Wort Faltung immer die Kreuzkorrelation gemeint sein.
}
Daher rührt auch der Name des Convolutional Neural Networks.
Wir werden die Faltung als mathematische Operation nicht im vollen Umfang
betrachten, da sie relativ kompliziert ist. Wir werden uns lediglich auf die
Bedeutung der Faltung für Filter beschränken.
\para{}
\begin{defbox}{Diskrete Faltung}
  Definintion Faltung
  \begin{equation}
    (f * g)(x) = \int_{-\infty}^{\infty} f(\tau) g(x-\tau) \text{d}\tau
  \end{equation}
\end{defbox}
\para{}
Mithilfe der Faltungsoperation können wir die Schritte welche in Sektion
(\ref{sec:filteroperation_intuitiv}) erklärt wurden zusammenfassen.
Bei der Filteroperation handelt es sich lediglich um eine diskrete Faltung des
Filtertensors $\ten{W}$ über den Bildtensor $\ten{B}$. Somit können wir
folgende Formel verwenden, um einen Pixel $\tilde{\mat{B}}(x,y)$ des neuen Bildes zu berechnen.
\\
\begin{equation}
  (\tilde{\mat{B}})_{y,x} = (\mat{W} * \mat{B})_{y,x} = \sum_{v=1}^{f} \sum_{u=1}^{f} (\mat{W})_{v,u}(\mat{B})_{y+v-1,x+u-1}
\end{equation}
\para{}
\begin{examplebox}{Faltung Beispielrechnung}
  \begin{equation*}
    \mat{B} =
    \begin{pmatrix}
      10 & 10 & 10 & 0 & 0 & 0 \\
      10 & 10 & 10 & 0 & 0 & 0 \\
      10 & 10 & 10 & 0 & 0 & 0 \\
      10 & 10 & 10 & 0 & 0 & 0 \\
      10 & 10 & 10 & 0 & 0 & 0 \\
      10 & 10 & 10 & 0 & 0 & 0 \\
    \end{pmatrix}
    \text{ und } \mat{W} =
    \begin{pmatrix}
      1 & 0 & -1 \\
      1 & 0 & -1 \\
      1 & 0 & -1 \\
    \end{pmatrix}
  \end{equation*}

  Für die Berechnung des ersten Pixels $(\tilde{\mat{B}})_{1,1}$ führen wir
  folgende Rechnung aus.
  \begin{align*}
    (\tilde{\mat{B}})_{1,1} &= (\mat{W} * \mat{B})_{1,1} = \sum_{v=1}^{f} \sum_{u=1}^{f} (\mat{W})_{u,v} (\mat{B})_{u,v} \\
                            &= 10 \cdot 1 + 10 \cdot 0 + 10 \cdot -1 + 10 \cdot 1 + 10 \cdot 0 + 10 \cdot -1 + 10 \cdot 1 + 10 \cdot 0 + 10 \cdot -1 \\
                            &= 0
  \end{align*}

  Das Gesamtergebniss lautet dann:
  \begin{equation*}
    \tilde{\mat{B}} = \mat{W} * \mat{B} =
    \begin{pmatrix}
      0 & 30 & 30 & 0 \\
      0 & 30 & 30 & 0 \\
      0 & 30 & 30 & 0 \\
      0 & 30 & 30 & 0 \\
    \end{pmatrix}
  \end{equation*}
\end{examplebox}
\para{}
Für farbige Bilder ist das Vorgehen praktisch das Selbe. Es werden nun
lediglich 3D-Tensoren anstatt 2D-Tensoren verwendet. Wir betrachten den
allgemeinen Bildtensor $\ten{B}_{h \times w \times c}$, wobei $c$ die Anzahl
Farbkomponenten (engl.: channels) ist.
Nun benutzt man einen Filter $\ten{W}_{f \times f \times c}$ mit beliebiger Grösse
$f$ aber mit der gleicher Tiefe $c$ wie das Ursprungsbild $\ten{B}$.
Dadurch muss der Filter nicht entlang der Tiefe des Bildes wandern, da er
gerade gleich tief ist wie das Bild. Das hat zur Folge, dass das verarbeitete
Bild $\tilde{\mat{B}}$ immer eine 2D-Matrix ist.
Die allgemeine Filtergleichung lautet:
\\
\begin{equation}\tag{FO}
  (\tilde{\mat{B}})_{y,x} = (\ten{W} * \ten{B})_{y,x} = \sum_{v=1}^{f} \sum_{u=1}^{f} \sum_{w=1}^c (\ten{W})_{v,u,w} (\ten{B})_{x+u-1,y+v-1,w}
\end{equation}

\para{}
\cite{Goodfellow-et-al-2016}
\cite{deeplearning.ai:cnn}
\cite{wiki:cnn}

\subsection{Mehrere Filter}
Eine Filteroperation ist nicht auf einen einzigen Filter beschränkt. Es können
mehrere Filter auf das gleiche Ausgangsbild angewandt werden und zusammen ein
Endbild erzeugen.
\para{}
Die Anzahl Filter bezeichnen wir mit $c$.
Nun macht man pro Filter $\ten{W}_i$ eine Faltung über das Ursprungsbild $\ten{W}$, wobei
jede Faltung einen neue Matrix $\tilde{\mat{B}}_i = \ten{W}_i * \ten{B}$ liefert, wobei $i$ der Index des
Filters ist. All diese gefalteten Bilder $\tilde{\mat{B}}_i$ sind zweidimensionale Matrizen, unabhängig davon, wieviele
Komponenten das Ursprungsbild hatte. Aus diesem Grund können die einzelnen
Matrizen $\tilde{\mat{B}}_i$ aufeinander gelegt werden und somit einen grossen 3D-Endtensor
$\tilde{\ten{B}}$ bilden.
Hierfür müssen alle Filter $\ten{W}_i$ gleich gross sein.
\begin{figure}[h!]
  \caption{aufeinander gelegte gefaltene Bilder}

\end{figure}
Die Tiefe des neuen Bildes $\tilde{\ten{B}}$ ist jetzt gerade die Anzahl Filter $c$.
Wir haben bereits früher immer die Tiefe des Bildes (bzw. die Anzahl
Farbkomponenten) mit $c$ bezeichnet. Somit ist $c$ sowohl die Anzahl Filter
einer Schicht, wie auch die Tiefe des verarbeiteten Bildes.

\para{}
\cite{Goodfellow-et-al-2016}
\cite{deeplearning.ai:cnn}

\subsection{Padding}
Wie bereits erwähnt schrumpfen die Bilder (an den Rändern), wenn man einen Filter auf sie anwendet.
Zur Erinnerung: Dies liegt daran, dass pro Lage des Zentralelements nur ein Pixel
des neuen Bildes entsteht und das Zentralelement nunmal nicht auf allen
Ursprungspixeln zu liegen kommt. Die Ränder fallen so weg. Diese Grösse des
Wegfalls hängt von der Filtergrösse $f$ ab. Hierbei gilt folgende Formel für die
neue Grösse $n_1$ des Bildes, anhand der alten Grösse $n_0$. $n$ bezeichnet
eine der zwei Seitenlängen.
\begin{equation}
  n_1 = n_0 - f + 1
\end{equation}

Das Problem hierbei ist, dass nach einigen Faltungen das Bild extrem geschrumpft
ist und im Grenzfall kleiner als der Filter wird. Das darf natürlich nicht
passieren. Ausserdem kommt es dabei zu einem relativen Informationverlust an den Ränder, da es im
Innern des Bildes zu viel Überlappungen der Filterlagen kommt und es bei den
Rändern nur seltener zur Überlappung kommt.
\para{}
Diese Probleme werden mit sogenanntem \keyword{Padding} behoben. Padding ist ein
Schritt, welcher vor der eigentlichen Faltung stattfindet. Dabei werden einfach
zusätzliche Ränder (Zeilen und Spalten) an das Ursprungsbild angebracht. Der
Tensor wird der Länge und der Breite nach (nicht der Tiefe entlang!) an den Enden erweitert. Die neuen Elemente werden dabei auf den Wert
$0$ gesetzt.

\begin{figure}[h!]

  \caption{Padding}
\end{figure}

Das Padding $p$ ist eine Zahl, welche angibt wieviele Elemente an allen Rändern
hinzugefügt werden. Padding $p = 1$ bedeutet, dass an allen Kanten jeweils eine
Reihe bzw. Spalte hinzugefügt wird.
Begrifflich unterscheidet man zwischen zwei Arten von Padding:
\begin{itemize}
\item{\keyword{Valid-Padding}: Es werden keine züsäzlichen Elemente angebracht. $p$ ist also 0.}
\item{\keyword{Same-Padding}: Es werden so viele Reihen und Spalten angebracht, dass
    die Grösse des Bildes nach der Faltung unverändert bleibt.}
\end{itemize}
\para{}
Um Same-Padding durchzuführen, muss man $p$ so wählen, dass es den Wegfall durch
den Filter gerade kompensiert. Da durch das Padding das Bild der Länge und
Breite nach jeweils um zwei $p$ länger wird, erhält man den Ausdruck $n_1 =
n_0 - f + 1 + 2p$. Wenn man diese Formel jetzt nach $p$ auflöst,
erhält man folgende Formel für das Wählen des Same-Paddings $p$.
\\
\begin{equation}
  p = \frac{f-1}{2}
\end{equation}

\cite{deeplearning.ai:cnn}

\subsection{Stride}
Bis jetzt haben wir bei den Filterfaltungen, den Filter pro Anwendung immer nur
um einen Pixel verschoben. Das muss aber nicht so sein. Man kann den Filter
auch mit einer anderen Schrittgrösse verschieben. Diese Eigenschaft bezeichnet
man als \keyword{Stride} $s$. Falls $s = 2$ gewählt wird, bedeutet das, dass der
Filter sich pro Matrixmultiplikation um zwei Elemente bewegt. Somit wurde eine
Position übersprungen. Zur Folge hat dies, dass das neue Bild
deutlich kleiner geworden ist, denn das Zentralelement überspringt somit auch
diese Felder und bildet so deutlich weniger Pixel.
\para{}
Folgende Formel beschreibt die Dimensionen des neuen Bildes unter
Berücksichtigung der Filtergrösse $f$, dem Padding $p$ und dem Stride $s$.
\\
\begin{equation}
  n_1 = \frac{n_0 + 2p - f}{s} + 1
\end{equation}

\cite{deeplearning.ai:cnn}

\subsection{Vorzüge von Filtern}
Es stellt sich nun die Frage, weshalb sich Filter für Maschinelles
Lernen mit Bildern eignen.
Wie bereits erwähnt besteht die Aufgabe von Filtern darin, bestimmte Features
eines Bildes hervorzuheben und die Restlichen auszublenden. Genau die gleiche
Aufgabe erfüllen die Neuronen in einem KNN. Auch sie sollen Features der
Inputdaten erlernen, wobei gewisse Neuronen auf gewisse Features reagieren.
Weshalb verwendet man also nicht einfach KNNs? (abgesehen
von dem Problem mit der Überzahl an Modellparametern, siehe Sektion \ref{sec:CNN_parameter_problem})
\para{}
Man muss erkennen, dass Bilddaten sich deutlich von sonstigen Daten unterscheiden.
Bildfeatures bzw. Pixel sind nur im Kontext von ihren Nachbarn relevant. Denn
ein Pixel ist erst dann eine Kante oder eine Ecke, wenn er zwei verschiedene
Farbregionen voneinander trennt. Oder ein Gegenstand wird erst durch eine ganzen Haufen
von Pixeln und deren relative Position züinander ausgezeichnet.
Man bezeichnet diesen Umstand als \keyword{lokalisierte Features}.
\para{}
Desweiteren sind Bildausschnitte nicht immer gleich
orientiert. Wenn man ein Gesicht auf einem Bild erkennen möchte, sollte es aber
keine Rolle spielen, wo auf dem Bild es sich befindet, welche Grösse es
hat und was für eine Lage es hat. Um diese Eigenschaften irrelevant für das
Modell zu machen, muss dieses gewisse \keyword{Invarianzen} erfüllen.
\para{}
Die Eigenschaft, welche CNNs geeignet macht für lokalisierte Features und
solche Invarianzen, bezeichnet man als \keyword{Parameter-Sharing}.
Es bezeichnet den Umstand, dass auf
mehrere oder alle Features die gleichen Modellparameter wirken. Bei CNNs wird
dies durch die Bewegung des Filters bzw. seiner Einträge über (fast) alle
Pixel bewerkstelligt. Somit ist
es egal, wo und wie sich die lokalisierten Features befinden. Dies führt dann eben zu
den gewünschten Invarianzen: Translations-, Rotations- und
Helligkeitsinvarianz. \\
Eine weiterer Vorteil durch dieses Parameter-Sharing ist, dass die Inputbilder
beliebige Dimensionen besitzen können, da die Filter ihre Bewegung lediglich an
die Grösse des Bildes anpassen müssen.

\cite{deeplearning.ai:cnn}

\subsection{Die Convolutional-Schicht}
Nun möchten wir alles zusammentragen, was wir über Filter gelernt
haben, um nun die Convolutional-Schicht zu definieren.
\para{}
Beginnen tut eine Convolutional-Schicht $l$ mit seinem Input, also den
Aktivierungen $\ten{A}^{l-1} \in \set{R}^{h^l \times w^l \times c^l}$, welche die vorherige Schicht ($l-1$) produziert hat,
oder falls es die erste Schicht ist, erhält sie den Input $\ten{X}$ des Netzes,
welchen man auch mit $\ten{A}^0$ bezeichnet. \\
Die Schicht besitzt $c^l$ Varianten an Filtern $\ten{W}^l_i \in
\set{R}^{f^l \times f^l \times c^{l-1}}$. Diese Filter haben alle die gleichen
Eigenschaften, bezüglich: der Grösse $f^l$, der Tiefe $c^{l-1}$, dem Padding
$p^l$ und dem Stride $s^l$. Sie unterscheiden sich nur in den Modellparametern
$w^l_{i\,|\,\alpha,\beta,\gamma}$.
\\
\begin{equation*}
  \ten{W}^l_i =
  \begin{bmatrix}
    \begin{pmatrix}
      w_{i\,|\,1,1,1}^l & \cdots & w_{i\,|\,1,f,1}^l \\
      \vdots & \ddots & \vdots \\
      w_{i\,|\,f,1,1}^l & \cdots & w_{i\,|\,f,f,1}^l
    \end{pmatrix}
    & \stackrel{\mathclap{\normalfont\mbox{$\vv{\gamma}$}}}{\cdots} &
    \begin{pmatrix}
      w_{i\,|\,1,1,c^l}^l & \cdots & w_{i\,|\,1,f,c^l}^l \\
      \vdots & \ddots & \vdots \\
      w_{i\,|\,f,1,c^l}^l & \cdots & w_{i\,|\,f,f,c^l}^l
    \end{pmatrix}
  \end{bmatrix}
\end{equation*}
\\
Nun faltet man jeden Filter $\ten{W}_i^l$ einzeln über das Bilder
$\ten{A}^l$ und erhält so mehrere neue 2D-Bilder $\tilde{\mat{A}}_i^l$.
\\
\begin{equation}
  \tilde{\mat{A}}_i^l = \ten{W}_i^l * \ten{A}^l
\end{equation}
\\
Die Faltung berechnet man nach folgender Gleichung:
\\
\begin{equation}\tag{FO}
  (\tilde{\mat{A}}_i)_{y,x}^l = (\ten{W}_i^l * \ten{A}^l)_{y,x} = \sum_{v=1}^{f} \sum_{u=1}^{f} \sum_{w=1}^c (\ten{W}^l_i)_{v,u,w} (\ten{A}^l)_{x+v-1,y+u-1,w}
\end{equation}
\\
Jede dieser Matrizen ist ein Qürschnitt $\tilde{\mat{A}}^l_{:,:,i}$ entlang der
Tiefe des neuen 3D-Tensors $\tilde{\ten{A}}^l \in \set{R}^{h^{l+1} \times w^{l+1} \times c^l}$.
Hierbei ist die Tiefe $c^l$ gerade die Anzahl Filter $c^l$ und die Höhe
$h^{l+1}$ und die Breite $w^{l+1}$ werden jeweils nach der folgenden Formel
berechnet anhand der Höhe $h^l$ und der Breite $w^l$ des Ausgangsbildes.
\\
\begin{equation}
  n^{l+1} = \frac{n^l + 2p^l - f^l}{s^l} + 1
\end{equation}
\\
Da die Faltungsoperation eine lineare Operation ist, braucht man wieder eine
nicht-lineare Aktivierungsfunktion um das Modell zu befähigen nicht-lineare Probleme zu
lösen.
Deshalb wendet man in einem letzten Schritt die vektorisierte Aktivierungsfunktion
$\vecf{\varphi}$ auf den Tensor $\tilde{\mat{A}}^l$ an und erhält so die
neue Aktiverung der nächsten Schicht $\ten{A}^{l+1}$. Empirisch hat man
festgestellt, dass es für CNNs eine bessere Aktivierungsfunktion als die
Sigmoidfunktion gibt. Die ReLU-Aktivierungsfunktion $\vecf{\varphi}^{ReLU}$ aus Sektion (\ref{sec:ReLU})
erziehlt bessere Resultate als die Sigmoidfunktion. Deshalb verwendet man
eigentlich nur diese in CNNs.
\begin{equation}
  \ten{A}^{l+1} = \vecf{\varphi}[\tilde{\ten{A}}^l]
\end{equation}


\cite{wiki:cnn}
\cite{deeplearning.ai:cnn}
\cite{Goodfellow-et-al-2016}
\cite{Nielsen}

\section{Dimensionalitätskontrolle}
Beim Anwenden einer Convolutional Schicht gehen Informationen verloren, da nur
die relevanten Features hervorgehoben werden und der Rest verworfen wird. Jedoch
schrumpft das Bild entweder gar nicht (bei Same-Padding) oder es schrumpft nur
sehr leicht (bei Valid-Padding). Dies ist ein Problem, da die Information in
deutlich weniger Pixeln bzw. Tensorelementen codiert werden könnte. Deshalb
haben wir einen unötigen Ressourcenverbrauch und eine erhöhte Gefahr für
Overfitting. Um diesem Problem entgegenzuwirken, gibt es einerseits sogenannte
\keyword{Pooling-Schichten} und als Gegenstück dazu
\keyword{Upsampling-Schichten}. Ersteres wird verwendet, um die Dimensionalität
der Bilder zu vermindern, und letzteres, um die Dimensionalität der Bilder zu
erweitern. Somit ermöglichen diese Schichten eine kontrollierte Art (im Gegensatz zum
Padding) die Dimensionalität einzustellen.

\subsection{Die Pooling-Schicht}
Die Pooling-Schicht verringert die Dimensionalität durch das Zusammenfassen eines Feldes
von Tensorelementen zu einem einzigen Tensorelement der nächsten Schicht.
Dabei geht das Feld nur entlang der Länge und Breite des Bildes und
nicht entlang der Tiefe. Dies bedeutet, das Pooling wird auf jeden Channel
einzeln angewandt, dadurch ändert sich die Tiefe des Bildes nicht.
\para{}
Für die Beschreibung einer Pooling-Schicht wird viel Begrifflichkeit der
Convolutional-Schicht und der Filter übernommen.
Die Grösse des Elementfeldes, welches zusammengefasst wird, bezeichent man analog zur
Filtergrösse mit $f^l$. Das Stride $s^l$ bezeichnet auch beim Pooling wieder, wie
gross die Schrittgrösse beim Verschieben des Feldes ist. Meistens wählt man den
Stride $s^l$ gerade gleich der Feldgrösse $f^l$, damit alle Pixel
zusammengefasst werden. Kleiner als $f^l$ kann $s^l$ nicht gewählt werden.
\para{}
Man unterscheidet zwischen zwei Arten von Pooling:
\begin{itemize}
\item{\keyword{Average-Pooling}: Das Elementenfeld wird zusammengefasst, indem
    man das arithmetische Mittel der Elemente bildet.}
\item{\keyword{Max-Pooling}: Das Elementenfeld wird zusammengefasst, indem das
    Element mit dem höchsten Wert beibehalten wird und die anderen verworfen werden.}
\end{itemize}
In Praxis verwendet man eigentlich nur Max-Pooling, da es deutlich bessere
Resultate erzielt.
\begin{examplebox}{MaxPooling Beispiel}
  Wir wählen eine MaxPooling-Schicht mit $f^l = 2$ und $s^l = 2$.
  Diese fasst also jedes $(2 \times 2)$-Feld zu einem Element zusammen. Da aus
  vier Elementen jeweils eines wird, entspricht dies einer Informationsreduktion
  von 75\%.
  \para{}
  \begin{equation*}
    \begin{pmatrix}
      5 & 2 & 4 & 3 \\
      8 & 9 & 5 & 1 \\
      3 & 8 & 6 & 7 \\
      8 & 1 & 4 & 2 \\
    \end{pmatrix}
    \to_{\text{MaxPool}}
    \begin{pmatrix}
      9 & 5 \\
      8 & 7 \\
    \end{pmatrix}
  \end{equation*}
\end{examplebox}

\para{}
Es ist festzuhalten, dass eine Pooling-Schicht keinerlei Modellparameter
besitzt. Somit gibt es nichts zu trainieren. Sie besitzt lediglich einige
Hyperparameter, wie die Feldgrösse $f^l$ und den Stride $s^l$.
\para{}
Um zu berechnen, was die neuen Dimensionen nach dem Pooling sind, gelten die
gleichen Formeln wie für die Filteroperationen:
\\
\begin{equation}
  n_1 = \frac{n_0 - f}{s} + 1
\end{equation}
\\


\cite{deeplearning.ai:cnn}
\cite{Goodfellow-et-al-2016}

\subsection{Die Upsampling-Schicht}
Die Upsampling-Schicht bildet das Gegenstück zur Pooling-Schicht, da sie die
Dimensionalität der Bilder erhöht. Auf den ersten Blick erscheint unklar,
weshalb man die Dimensionalität erhöhen wollen könnte, da die eigentliche
Motivation dieser Schichten in der Verhinderung von Overfitting besteht. Jedoch
sollte in Kapitel (\ref{sec:autoencoder}) klar werden, weshalb solche Schichten
notwendig sind.
\para{}
Man verwendet auch hier wieder den Begriff der Feldgrösse $f^l$. Diesemal
beschreibt dieser Wert, wie stark das Bild hochskaliert wird. Beim Upsampling
wird ein Element zu einem $(f^l \times f^l)$-Feld hochgerechnet und erweitert so die
Dimensionalität.
\para{}
Es stellt sich natürlich die Frage, was für Werte das neue Feld erhält.
Dafür muss man eine Interpolationsart wählen.
Zwei Arten sind besonders verbreitet:
\begin{itemize}
\item{\keyword{Bilineare Interpolation}}
\item{\keyword{Nächste-Nachbar-Interpolation} (engl.: nearest-neighbor-interpolation)}
\end{itemize}
Wir werden hier nur die Nächste-Nachbar-Interpolation betrachten. Sie ist
ziemlich trivial, denn alle neuen Feldelemente nehmen einfach den Wert des alten
Elements an. Der Wert wird also einfach vermehrt.


\cite{deeplearning.ai:cnn}
\cite{Goodfellow-et-al-2016}

\pagebreak
\chapter{Autoencoder}\label{sec:autoencoder}
Nun möchten wir uns vorerst von der Architektur des Convolutional Neural
Networks abwenden, um den Autoencoder zu betrachten. Wir werden uns erneut
mit der Funktionsweise und der Nützlichkeit dieser Architekur
auseinandersetzen. Danach werden wir den Autoencoder mit dem Convolutional
Neural Network fusionieren und so einen Convolutional-Autoencoder erhalten. Zu
guter letzt betrachten wir dann noch eine Anwendung eines solchen
Convolutional-Autoencoder, nämlich einen Denoiser.
\para{}
\bigskip
Ein \keyword{Autoencoder} ist eine weitere Architektur eines KNNs. Diese Architektur
beschreibt das Netzwerk jedoch nicht bezüglich der Schichtenarten, sondern beschreibt es
auf der Ebene der Netzform. \\
Die Aufgabe eines Autoencoders besteht darin eine neue Repräsentation einer Datenmenge
zu erlernen. Diese neuartige Repräsentation soll mit weniger Daten möglichst die gleiche
Information codieren. Somit entwickelt das Modell eine \keyword{effiziente
  Daten-Codierung} für einen Datensatz. Dadurch kann er zur
\keyword{Dimensionalitätsreduktion} verwendet werden. \\
Neben dieser neuen Repräsentation erlernt das Netzwerk noch, wie es
aus dieser Codierung wieder die Ursprungsdaten reproduzieren kann.
\para{}
Autoencoder gehören nicht dem klassischen überwachtem Lernen an. Strenggenommen gehören
sie dem unüberwachtem Lernen an, da in den Trainingsdaten keine Labels
enthalten sein müssen. Jedoch ist diese Begrifflichkeit etwas irreführend,
weil nur deshalb keine Labels benötigt werden, da die Labels gleich den Features sind!
Somit wird das Netzwerk trotzdem darauf trainiert gewisse gewünschte Outputs
zu liefern.

\cite{book:autoencoder}

\section{Topologie}
Ein Autoencoder besteht wie das klassische KNN auch aus Neuronen.
Wie bereits erwähnt sind die ``Labels'' eines Autoencoders gleich
den Features. Somit versucht ein Autoencoder einfach die Werte der Inputneuronen
möglichst exakt in die Outputneuronen zu kopieren.
Diese Operation wäre an sich ziemlich bedeutungslos und nutzlos, da mit den
Daten nichts passiert. Das Netzwerk würde einfach erlernen eine
Identitätsfunktion zu imitieren.
Aus diesem Grund muss man gewisse Einschränkungen einführen. Diese bringen das Netzwerk dazu,
interessante Methoden zu entwickeln um die Features \textit{approximativ} zu rekonstruieren.
\para{}
Die angesprochene Einschränkung besteht darin, dass dem Autoencoder nicht
beliebig viel Kapatzität für die Codierung der Features gegeben wird.
Es stehen dem Netzwerk nur wenige Zahlenwerte zur Verfügung um die Features
in der neuen Repräsentation zwischenzuspeichern, bevor sie wieder in die ursprünliche Form gebracht werden.
Diese Kapazitätseinschräkung wird durch die Topologie des Autoencoders bezweckt.
\para{}
\bigskip
Der einfachste Autoencoder, besteht aus drei Schichten, welche sich wie folgt
eingliedern lassen:
\begin{itemize}
\item{eine Inputschicht mit $d$ Neuronen}
\item{eine Zwischenschicht mit $p$ Neuonen, bezeichnet als \keyword{Flaschenhals}}
\item{eine Outputschicht mit $d$ Neuronen}
\end{itemize}
In Abbildung (\ref{fig:basic_autoencoder}) ist
ein beispielhafter Autoencoder abbgebildet, mit $d = 5$ Features und einem
Flaschenhals der Grösse $p = 2$.
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{ae1.pdf}
  \label{fig:basic_autoencoder}
  \caption{Schichten eines kurzen Autoencoders}
\end{figure}
\para{}
Die Inputschicht enthält die Features $\vec{x} \in \set{R}^d$. Die Outputschicht dagegen
enthält die \keyword{Rekonstruktionen} der Features, welche wir mit $\vec{\hat{x}} \in \set{R}^d$
bezeichnen. Als logische Konseqünz müssen die beiden Schichten die gleiche
Anzahl $d$ an Neuronen besitzen.
Die Flaschenhalsschicht hingegen besitzt nur $p$ Neuronen, wobei $p \ll d$.
Somit ist die sie deutlich kleiner als die anderen beiden Schichten und bildet
so die Kapazitätseinschränkung.
\para{}
Wir haben nun einen Autoencoder nur mit einer Zwischenschicht betrachtet.
Gängier ist es jedoch das Netz aus mehreren Zwischenschichten zu baün. Dabei
werden die Schichten zum Flaschenhals hin immer schmaler und die Schichten nach
dem Flaschenhals wieder immer dicker. In Abbildung (\ref{fig:big_autoencoder})
ist dies gut ersichtlich. Wir werden nun aber weiterhin mit dem Autoencoder,
welcher nur eine Zwischtenschicht besitzt, die Funktionsweise herleiten.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{ae2.pdf}
  \label{fig:big_autoencoder}
  \caption{Schichten eines tieferen Autoencoders}
\end{figure}

\section{Funktionsweise}
Ein Autoencoder möchte die Features $\vec{x}$ von der Inputschicht in die
Outputschicht kopieren. Jedoch zwingt der Flaschenhals den Autoencoder dazu eine
neue Repräsenation der Features zu erlernen. Dies hat den Grund, dass die
Features in ihrer jetzigen Form keinen Platz im Flaschenhals finden, da $p \ll d$.
Somit muss das Modell die Informationen, welche in den Features stecken
nur auf einige wenige Eigenschaften komprimieren. Es muss gewissermassen einen
Code entwickeln. Diese neue Repräsentation nennt man das \keyword{Encoding}. \\
In einem zweiten Schritt muss der Autoencoder aus diesem Encoding wieder
versuchen die ursprünlichen Features zu rekonstruieren. Diese Rekonstuktion
bezeichnet man als das \keyword{Decoding}.

\paragraph{Encoder und Decoder}
Man kann einen Autoencoder gewissermassen als zwei Teilmodelle betrachten. Das
einte Modell erzeugt das Encoding und das andere das Decoding. Wir spalten
dafür die Hypothesenfunktion $h$ in ein Funktionenpaar $(\phi,\psi)$ auf.
\begin{itemize}
\item{$\phi: \fspace{X} \to \fspace{E}$ Encoder-Funktion}
\item{$\psi: \fspace{E} \to \fspace{X}$ Decoder-Funktion}
\end{itemize}
Die erste Teilfunktion $\phi$ ist dafür zuständig das Encoding zu erzeugen. Sie
bildet den Inputraum $\fspace{X} \subseteq \set{R}^d$, welcher die Features $\vec{x}$ enthält, auf
den Encodingraum $\fspace{E} \subseteq \set{R}^p$, welcher die neuen
Repräsenation $\vec{x^*}$ enthält, ab. \\
Das Gegenstück ist die Teilfunktion $\psi$. Sie bildet den Encodingraum
$\fspace{E}$ zurück auf den Inputraum $\fspace{X}$ ab.
\para{}
Der Output des Autoencoders $\vec{\hat{x}}$ wird somit durch die aufeinander
folgende Anwendung der Teilfunktion auf die Features $\vec{x}$ gebildet:
\begin{equation}
  \vec{\hat{x}} = \psi(\phi(\vec{x})) = (\psi \circ \phi)(\vec{x})
\end{equation}

Wenn wir dieses Modell trainieren, minimieren wir die Kostenfunktion. Als
Beispiel können wir den Mittleren-quadratischen-Fehler $C_{MSE}$ nehmen.
Verwenden wir ihn als Kostenfunktion, minimieren wir folgenden Ausdruck:
\begin{equation}
  \min_{\phi,\psi} {\|\vec{x} - \vec{\hat{x}}\|}^2 = \min_{\phi,\psi} {\|\vec{x} - (\psi \circ \phi)(\vec{x})\|}^2
\end{equation}
Anderst ausgedrückt versuchen wir Funktion $\phi$ und $\psi$ zu finden, welche
die Differenz der ursprünglichen Features $\vec{x}$ und den Rekonstruktionen
$\vec{\hat{x}}$ minimieren. Dabei muss der Autoencoder
Dimensionalitätsreduktion betreiben. Er entwickelt einen Code um die Features
in der Flaschenhalsschicht komprimiert zu repräsentieren. Durch die
Kapazitätseinschräkung $p \ll d$ muss $\fspace{E}$ eine niedrig-dimensionale
Codierung der Features darstellen. Dem Modell ist es nicht möglich die Inputs
identisch wiederherzustellen, da bei der Abbildung $\phi$ Informationen verloren
gegangen sind. Dadurch sollte der Code im Optimalen Fall nur noch die Merkmale
der Features umfassen, welche für die grösste Information codieren.
Dieser Code kann extrahiert werden, um eine Repräsentation der Features zu
erhalten, welche nicht mehr mehrere Hundert Merkmale umfasst sonder nur noch ein
paar wenige. Jedliche natürliche Schwankungen in den Werten der Features gehen
verloren, da sie nicht im COde enthalten sind. Sie können nicht verallgemeinert werden.

\paragraph{Der Code}
Bei der Suche dieser Funktionen muss der Autoencoder einen gewissen ``Code''
entwickeln, um die Features neu zu repräsentieren. Dies da der Falschenhals
nicht den nötigen Kapazität bietet, um die Features in der ursprünlichen Form
zu halten. Somit muss der Autoencoder Dimensionalitätsreduktion betreiben, um
die Features komprimiert darzustellen.
\para{}
Durch diese Komprimierung ist es dem Modell unmöglich die Rekonstruktionen
perfekt zu machen. Sie sind immer approximativ, da bei der Komprimierung
Informationen verloren gegangen sind. Am effektivsten wird die Kostenfunktion
minimiert, wenn das Modell einen Code entwickelt, welcher die Features mit dem
höchsten Informationsgehalt abstrahiert. Die Features, welche unwichtig sind
oder nur durch züfalliges Rauschen der Daten entsteht können verworfen werden,
da sie nicht verallgemeinert werden können.
\para{}
Es ist wichtig zu erkennen, das der Code welcher der Autoencoder entwickelt
\keyword{datenspezifisch} ist. Das heisst er kann nicht wie z.B ein
Bildkompressionalgorithmus auf beliebige Daten angewendet werden. Stattdessen
ist er nur sinnvoll für die Kompression von Daten, mit welchen er trainiert
wurde. Der Autoencoder erlernt die Eigenschaften eines Datensatz zu abstrahieren
und neu darzustellen.
BEISPIEL EINER ABSTRAKTION (a la Menschliches Gesicht beschreiben).

\section{Anwendungen}
Die Eigenschaft, welche einen Autoencoder sich für die verschiedensten
Anwendungen geeignet macht, ist die Dimensionalitätsreduktion. \\
Wir werden nun einige Anwendungen grob betrachten.

\subsection{Kompression}
Es handlet sich um Verlustbehaftete Kompression.
Dimensionalitätsreduktion. Falls lineare Neuronen, ähnlich zur Hauptkomponentenanalyse
$\frac{p}{d}$ des ursprünglichen Speicherbedarfs. Diese Komprimierung ist dabei Datenspezifisch.

Datenverkehr beschleunigen, weil weniger Daten.

\subsection{Neue Datenrepräsentation}
neues Datenformat

\subsection{Visualisierung von Daten}
Visualisierung von hochdimensionaler Daten in $\set{R}^2$ und $\set{R}^3$


Im übernachsten Kapitel werden wir eine Anwendung noch genaür Betrachten die
des Denoisers.

\section{Convolutional-Autoencoder}
Ein Convolutional-Autoencoder ist wie der Name es schon sagt, ein Autoencoder,
welcher anstatt die normalen Fully-connected-Schichten eines KNNs die Schichten eines CNN verwendet. \\
Er ist interessant, weil er zur Kompession von Bildern verwendet werden kann.
Wie bereits erwähnt ist auch hier die Kompression datenspezifisch.
Beispielsweise könnte man einen Convolutional-Autoencoder mit einem Datensatz
von menschlichen Gesichtern trainieren. Dieser würde dann einen
datenspezifischen Code für diese Gesichter entwickeln. Mann kann sich
vorstellen, dass das Modell nicht mehr die Farbwerte der Features im Encoding
speichert, sondern stattdessen diese Features abstrahiert. Somit könnten
Eigenschaften, wie die Augenfarbe, Breite der Nase, Mundposition und vieles mehr
im Flaschenhals gespeichert werden. Diese Informationen brauchen offenkundig
weniger Kapazität als alle Pixelwerte.
\para{}
Des weiteren sind Convolutional-Autoencoder interessant, da sie uns auf eine
weitere Anwendung eines Autoencoders bringen: den Denoising-Autoencoder.
Diesen betrachten wir im nächsten Abschnitt.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{conv-autoencoder.png}
  \caption{Darstellung eines Convolutional Autoencoders}
\end{figure}


\cite{yt:autoencoder_faces}

\section{Denoising-Autoencoder}
Ein Denoising-Autoencoder oder einfach Denoiser ist ein leicht abgeänderter
Autoencoder. Er wird verwendet um ein Rauschen aus Bildern zu entfernen.
\para{}
Er unterscheidet sich insofern von dem klassischen Autoencodern, dass er
richtige Labels hat und nicht einfach die Features kopiert. Sein Input stellen
die verrauschten Bilder dar und die Labels sind die unverrauschten Bilder. Somit
wird er darauf trainiert das Rauschen zu entfernen. \\
Man mag sich nun die Frage stellen, weshalb man hierfür einene Autoencoder und
kein normales CNN verwendet. Empirisch hat sich einfach gezeigt, dass sich
Denoiser dafür besser eignen. Das hat nachvollziehbare Gründe. Bei der
Entwicklung des Code für Encoding-Repräsentation, kann das Rauschen nicht
codiert werden, da es völlig zufällig ist. Es existieren keine Muster oder
Gesetzmässigkeiten, welche anderst codiert werden könnten. Somit muss der
Autoencoder diese Rausch-Features verwerfen. Somit hat er leichteres Spiel
gegenüber sonstigen CNNs.

Corrupted Input
\subsection{Generierung der verrauschten Bilder}
Um den Autoencoder zu trainieren, muss man die verrauschten Daten generieren.
Dies macht man indem man auf die Features ein additives Gauss'sches Rauschen anwendet.
Man verwendet dafür einfach eine Gauss'sche Normalverteilung
$\mathcal{N}(\mu = 0, \sigma^2 = 1)$ mit Erwartungswert $\mu = 0$ und Varianz
$\sigma^2 = 1$. Man entnimmt dieser dann einfach jeweils einen Zufallswert und
addiert ihn zu jedem Feature. Somit erhält man einen Datensatz, welcher aus
verrauschten Daten besteht.
\begin{gather*}
  r \sim \mathcal{N}(\mu = 0, \sigma^2 = 1) \\
  \tilde{x} = x + r
\end{gather*}


% ------------------------------------

\chapter{Implementation für Maschinelles Lernen}
In diesem Kapitel werden wir zwei Frameworks betrachten, welche eine
Implementation der ML-Algorithemen realisiert haben. Wir werden sie in ihrer
groben Funktionsweise untersuchen. Diese Tools werden wir im
nächsten Kapitel benutzten um dann ein konkretes Modell zu programmieren.

\section{TensorFlow}
\begin{wrapfigure}{l}{3cm}
  \includegraphics[width=3cm]{tf_logo.png}
  \caption{TF-Logo}
\end{wrapfigure}
\keyword{TensorFlow}, kurz TF, ist ein von Google entwickeltes \keyword{Framework} für
\keyword{datenstromorientierte Programmierung}. Es ist im Grunde genommen eine
Mathematik-Programmbibliothek für numerische Berechnungen, dessen Hauptanwendungsbereich im Maschinellen Lernen
liegt. Die Vorteile von TF bestehen vorallem darin, dass die praktische
Anwendung des Frameworks sehr
einfach ist und fast keine mathematische Vorkenntnisse voraussetzt. Somit kann
man sehr einfach ein ML-Modell definieren, trainieren und verwenden. Ausserdem
besitzt TF sehr performante Implementierungen, welche Algorithmen, wie die
Vorwärtspropagierung und die Rückwärtspropagierung, sehr schnell ausführen
lässt. Somit geht das Training von ML-Modellen deutlich schneller als bei
selbstgeschriebenen Implementationen.
TF ist gratis und ein Open-Source-Projekt. Es kann auf Github gefunden werden
(siehe QR-Code). \\
\qrcode[height=2cm]{https://github.com/tensorflow/tensorflow}
\para{}
In der Industrie ist TensorFlow sehr weit verbreitet. Es wird von den
verschiedensten Unternehmen für ihre ML-Anwendungen gebraucht; darunter waren:
Google, Twitter, AirBnB, Intel, CocaCola, Snapchat und noch viele mehr.
\para{}
Grundsätzlich wurde TF für Maschinelles Lernen entwickelt. Speziell eignet es
sich für Deep Learning mit künstlichen neuronalen Netzen. Es ist ein
Interface, welches das definieren von Modellen erlaubt, welche dann mit den
eingebauten ML-Algorithmen trainiert werden können. \\
Trotz dieser eigentlichen Grundidee von TF, sollten die Abstraktionen eigentlich
für beliebige Anwendugen, welche mit numerischen Berechnungen zu tun haben,
sich eignen.
\para{}
Im folgenden werden die grobe Funktionsweise von Tensorflow untersuchen. Um
genau zu sein betrachten wir Tensorflow-Core Version r1.14.
\footnote{Momentan befindet sich TF 2.0 in der Beta. Es ist eine grosse
  Überarbeitung des Frameworks. Es ist zu empfehlen, sobald TF 2.0 offiziel
  veröffentlich wurde, es zu verwenden.}
Wir werden TF nur sehr grob betrachten, da wir später nicht direkt damit
arbeiten werden, sondern mit der High-Level-API Keras. Trotzdem ist er hilfreich
ein grobes Verstäntniss über TF zu besitzen.
\para{}
Die wichtige Resource um TensorFlow zu lernen ist wohl die offizielle TensorFlow
Website. Dort findet man auch die Dokumentation.
\para{}
\qrcode[height=2cm]{https://www.tensorflow.org/api/stable}

\para{}
\cite{book:tensorflow}

\subsection{Client und Master}
Wie die meisten grossen Programmbibliotheken ist Tensorflow in Backend und
Fontend aufgeteilt. Das Frontend von Tensorflow bezeichnet man als
\keyword{Client} und das Backend als \keyword{Master}.
\para{}
\begin{infobox}{Fontend und Backend}
  Das \keyword{Frontend} ist der Teil eines Programmes, mit welchem der Benutzer
  interagiert. Es stellt eine Schnittstelle zu dem sogennanten Backend eines
  Programmes dar. Es exponiert so gewisse Funktionalitäten des Programmes dem
  Benuzter gegenüber, ohne dass er dabei etwas von der zugrundeliegenden Logik mitbekommt.
  \para{}
  Das \keyword{Backend} ist das Gegenstück zum Frontend. Es implementiert
  jedigliche Programmlogik, welche das gewünschte Verhalten der Applikation
  hervorruft. Der Benutzer hat keinen direkten Zugriff auf diesen Teil des
  Programmes und bekommt somit nichts von dessen Existenz mit.
\end{infobox}
\para{}
Der Client - also das Frontend - von TF ist in \keyword{Python} geschrieben.
\footnote{
  Tensorflow besitzte mehrere Frontends, welche in den verschiedensten
  Programmiersprachen geschrieben sind. Wir werden jedoch nur das
  Python-Frontend betrachten, da es am besten von TF unterstützt ist.
}
Python ist allgemein als eine sehr einsteigerfreundliche Programmiersprache bekannt. Sie
ermöglicht es dem Benutzer mit wenigen Zeilen Code und ziemlich intuitiv
TensorFlow zu verwenden. Wenn wir also mit TensorFlow programmieren, werden wir
mit Python-Code arbeiten.
\para{}
Der Master - also das Backend - von TF ist dagegen in \keyword{C++} (und CUDA)
geschrieben. Dies hat den Grund, dass C++ im Gegensatz zu Python sehr performant
ist. Also ist die Zeit zum Auführen eines Programmes sehr kurz. Dafür ist der
Programmcode deutlich anspruchsvoller und verboser. Da der Benutzer jedoch keine
direkte Interaktion mit dem Master hat, bekommt er davon nichts mit.

\subsection{Verwendung - der Client}
Nun möchten wir betrachten wie man TensorFlow zu verwenden hat, damit wir
später ein ML-Modell damit basteln können. Dafür werden gewisse
Python-Kenntnisse vorausgesetzt. Wir betrachten nun also
den Client von TF. Man verwendet verschiedene Konzepte um
programmatisch mit TF zu arbeiten. Im folgenden Abschnitt werden wir diese
Konzepte behandeln.

\subsubsection{Graph}
TensorFlow ist datenstromorientiert. Das bedeutet, dass man eine Berechnung
(engl.: computation) durch einen gerichteten Graphen beschreibt. Zürst stellt
man diesen Graph auf und fügt ihn zusammen und erst nach der Definition führt man
ihn aus.
Es handelt es sich dabei um einen \keyword{Computation Graph}, wie wir ihn schon in Sektion (\ref{sec:backpropagation}) kennengelernt
haben. In TensorFlow bezeichnet man diesen Graphen als einen \keycode{tf.Graph}.
\para{}
Er besteht aus einer Menge an Knoten, welche miteinander über Pfade
verbunden sind. Die Knoten halten Werte in sich, welche über die Pfade,
verrechnet werden und in den nächsten Knoten eingespeist werden. Die Pfade sind also die Operationen, welche auf die Werte
angewandt werden. So führt der Graph eine Datenstrom-Berechnung aus.
Jeder Knoten hat null oder mehr Inputs und null oder mehr Outputs.
\para{}
Man muss einen Graph nicht explizit definieren (kann es aber, falls man mehrere
Graphen braucht), da beim defnieren der Operationen sie implizit dem
sogennanten \keyword{Default Graph} hinzugefügt werden.

\subsubsection{Tensoren}
Die zentrale Dateneinheit in TensorFlow sind Tensoren, wie wir sie bereits in
Sektion (\ref{sec:tensor}) kennengelernt haben.
Auch die Knoten des Graphs reprasentieren Tensoren, welche aber keine feste Wert
enthalten, da sie erst beim Ausführen des Graphens, mit Werten befüllt werden.
Nach der Ausführung der Graphs, kann man die Zahlenwerte auslesen.
\para{}
Man verwenet in TF deshalb Tensoren, weil so die Daten beliebige Dimensionalitäten
und Formen annehmen können. Es können sowohl Listen von Zahlen (Vektoren),
wie auch Bilder (Tensoren dritter Ordnung) verrechnet werden. \\
Nun sollte auch klar sein, weshalb wir bei der Herleitung der Gleichungen, auch
immer eine Matrixschreibweise erarbeitet haben.
\para{}
In Tensorflow bezeichnet man einen Tensor mit \keycode{tf.Tensor}.
Er ist definiert durch seine Form (Stufe inbegriefen) und den Datentyp seiner
Elemente (z.B. tf.float32).
\para{}
Jeder tf.Tensor ist mit einer einzigen Graph-Ausführung assoziert. Sie
besitzen ihren Wert nur für den jeweiligen Durchlauf, danach wird er verworfen.
Man sagt die Tensoren sind ``immutable'', da die Werte nicht erhalten bleiben
und so nicht modifiziert werden können.
\para{}
Die söben beschriebene Tensoren agieren einfach als Durchflussstellen innerhalb
des Graphen. Sie müssen nicht explizit definiert werden, da sie automatisch
eingefügt werden, wenn der Graph durch seine Operationen definiert wird.
\para{}

Neben dieser Art von Knoten, gibt es noch verschiedene Inputknoten.
stellen den Input in den Graphen dar und müssen explizit definiert werden. Von ihnen gibt es verschiedene Arten,
welche wir nun betrachten werden.

\paragraph{tf.constant}
Der \keycode{tf.Constant} Tensor ist ein Input-Knoten in den Graphen.
Er hat immer einen konstanten Wert als Output. Dieser wird beim Erstellen
angegeben.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  <Varname> = tf.constant(<Wert>)
\end{minted}
\para{}
Folgender Python-Code ist ein Beispiel, welcher drei konstante Tensoren mit unterschiedenlichen
Formen erstellt. Als Argument, gibt man die Elemente des Tensors, worüber auch
die Form ersichtlich an.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  skalar = tf.constant(1.0)
  vector2 = tf.constant([1.0, 2.0])
  matrix2x2 = tf.constant([[1.0, 2.0], [3.0, 4.0]])
\end{minted}
\[\text{skalar}=1 \quad \text{vector2}=\begin{pmatrix}1&2\end{pmatrix} \quad
  \text{matrix2x2}=\begin{pmatrix}1&2\\3&4\end{pmatrix}\]

\paragraph{tf.placeholder}
Um den Graphen für Maschinelles Lernen zu verwenden, braucht man eine
Möglichkeit ihn mit Inputs für die Modelle zu befüttern. Um dies möglichst
komfortabel zu machen, gibt es den sogennanten
\keycode{tf.Placeholder} Tensor. Wie der Name es schon sagt, ist er ein
Platzhalter für die Inputs, welcher beim Start der Graphausführnug eingespeist
werden. Man muss einzig seine Dimensionen und seinen Datentyp (z.B. tf.float32) bei der Erstellung angeben.
\para{}
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  <Varname> = tf.placeholder(dtype=<Datentyp>, shape=<Form>)
\end{minted}
Für den Datentyp wählt man eigentlich immer
\mintinline{python}{dtype=tf.float32}. Somit kann man mit Gleitkommazahlen rechnen.
Die Form gibt man einfach als $n$-Tupel von Integern an, welche die Anzahl
Komponenten in der jeweiligen Dimension angeben. Somit bedeutet die Form
\mintinline{python}{shape=(4,2,3)} einen Tensor im Tensorraum $\set{R}^{4 \times
  2 \times 3}$.

\paragraph{tf.variable}
Ein weiteres Kriterium um Maschinelles Lernen mit dem tf.Graph durchzuführen,
ist dass man Modellparameter einführen kann. Dafür ist der
\keycode{tf.Variable} Tensor geeignet. Er ist im Gegensatz zu den anderen
Tensoren ``mutable''. Das bedeutet, dass seine Werte unabhängig von einer
einzigen Graphenausführung existieren und erhalten bleiben. Seine Elementen
können modifiziert werden. Somit sind sie geeignet, für Modellparameter.
Die Werte bleiben grundsätzlich jede Ausführung erhalten und können beim
Trainingsdurchlauf angepasst werden.
\para{}
Folgender Python-Code kreirt eine tf.Varaible mit einem Anfangswert und ändert ihren Wert danach.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  <Varname> = tf.Variable(<Initialwert>, name=<optionaler Name>)
  <Varname>.assign(<neuer Wert>)
\end{minted}
Bei der neuen Wertebesetzung kann auch der alte Wert des Tensors referenziert
werden, um den Wert zum Beispiel zu erhöhen:
\mintinline{python}{var.assign(var + 2.0)}.

\subsubsection{Operationen}
Die Knoten des tf.Graph - also die Tensoren - werden über die Pfade aufeinander
abbgebildet. Die Operationen, welche sie aufeinander abbildet, bezeichnet man
als \keycode{tf.Operation}. Eine tf.Operation besitzt jeweils einen Namen und repräsentiert eine
abstrakte Berechnung. Sie hat jeweils null oder mehr Tensor-Objekte als
Input, wie auch als Output.
\para{}
Man definiert meistens keine neuen Operationen, sondern verwendet einfach die vordefinierten.
Ein Beispiel dafür wäre die \keycode{tf.matmul(a,b)}, welche eine
Matrixmultiplikation zwischen \mintinline{python}{a} und \mintinline{python}{b}
ausführt. Im folgenden Code wird das Resulat im neuen Tensor
\mintinline{python}{c} gespeichert.
``MatMul'' bezeichnet).
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  c = tf.matmul(a, b)
\end{minted}
\para{}
Mit diesem Konzept können wir nun endlich den eigentlichen Graph definieren.
Dafür muss man lediglich ein paar Inputtensoren erstellen, und diese mithilfe
von Operationen verrechnen. Natürlich kann man die Resulate der Operationen
dann weiter verrechnen, bis man den gewünschten Grapen hat.
\para{}
Hier ein Beispiel eines solchen Graphs.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  # Inputknoten
  a = tf.constant([[1.0, 2.0], [3.0, 4.0]])
  b = tf.constant([[1.0, 1.0], [0.0, 1.0]])
  # Operationen
  c = tf.matmul(a, b)
  d = tf.placeholder(dtype=tf.float32, shape(2,2))
  e = tf.add(c, e)
\end{minted}

\begin{figure}
  \caption{Visualisierung des beschriebenen \keycode{tf.graph}}
\end{figure}
\para{}


\subsubsection{Session}
Der Client kommuniziert mit dem Master über eine sogennante
\keycode{tf.Session}.
Sie ist die Schnittstelle zwischen Frontend und Backend und ermöglicht es einen tf.Graph
zu erstellen und ihn auszuführen.
Um die Session zu erstellen verwendet man die Funktion \keycode{tf.Session()}.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  sess = tf.Session()
\end{minted}
Es ist Best-Practise eine Session, wie eine Datei über das
\keycode{with}-Statment zu öffen und zu schliessen.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  with tf.Session() as sess:
  # Code
\end{minted}
\para{}
Um den Graphen nun zu berechnen, muss man lediglich die Session ausführen, das
geschieht über die Methode \keycode{tf.Session.run()}.
Diese Funktion erwartet als Argument einerseits den Knotenpunkt, welchen man
berchnen möchte. Dessen Wert ist der Rückgabewert der Funktion.
Andererseits erwartet die Funktion noch einen Python-Dictionary, den sogennanten
\keycode{feed\_dict}, welcher die \code{tf.placeholder} mit ihren Werten befüllt.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  result = sess.run(<Knotenpunkt>, feed_dict=<Placeholder-Dictionary>)
\end{minted}
Es folgt nun ein Codebeispiel, welches die Forwärtspropagierung eines
Sigmoide Neurons mit drei Inputs implementiert. Es wird es mit den Inputs $x = \trans{\begin{pmatrix}1&2&3\end{pmatrix}}$ ausgeführt.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  import tensorflow as tf

  # Inputs
  x = tf.placeholder(dtype=tf.float32, shape=(3))

  # Modellparameter
  w = tf.Variable(tf.random_uniform([3], -1, 1), name='weight')
  b = tf.Variable(tf.random_uniform(1, -1, 1), name='bias')

  # Forwärtspropagierung
  z = tf.add(tf.tensordot(x, w, 1), b)
  a = tf.sigmoid(z)
  y = a

  with tf.Session() as sess:
  res = sess.run(y, feed_dict = {x: [1, 2, 3]})
  print(res)
\end{minted}

\subsubsection{Optimizer}
Um ein ML-Modell in TF zu implementieren, muss man das ganze Modell mit
allen Operationen als Graphen definieren. Dies hört sich im Moment noch sehr
aufwendig an, ist es jedoch nicht. Wir werden nämlich die High-Level-API Keras
zur Hilfe ziehen, bei der Graphen erstellt.
Keras stellt Implementationen von verschiedenen KNN-Schichten zu verfügen, damit man sie nicht mehr selber
erstellen muss. Diese kann man dann einfach zu einem Graphen zusammenfügen.
Aber dazu später mehr.
\para{}
Die Ausführung des erstellen Graphen stellt dann einfach eine
Vorwärtspropagierung dar. Bevor jedoch das Modell brachbare Resualte liert,
muss man es trainieren. Dafür verwendet man einen \keycode{tf.train.Optimizer}.
\para{}
Der erste Schritt ist dafür eine Kostenfunktion zu definieren. Es sthene dabei
Verschiedne aus \keycode{tf.losses} zur Verfügung. Als Argument nimmt die
Kostenfunktion die Label-Tensoren \code{labels} und die Vorhersagen-Tensoren \code{predictions}.
Beispielsweise könnte man den Mittleren Quadratischen Fehler wählen:
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  cost = tf.losses.mean_squared_error(labels=y_hat, predictions=y)
\end{minted}
Nun erstellt man den Optimizer. Dabei stehen wieder mehrere zur Auswahl. Der
\keycode{tf.train.GraidentDescentOptimizer} ist eine Implementation des
Gradientenverfahren. Er nimmt als Arugment die Lernrate $\eta$.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  optimizer = tf.train.GradientDescentOptimizer(learning_rate=<>)
\end{minted}
Nun kann man den Optimizer damit beauftragen die Kostenfunktion \code{loss} zu
minimieren. Ausserdem muss man dem Optimizer eine List von \code{tf.Variable}s
geben, welche die zu optimierenden Modellparameter sind.
Man erhält so man eine Optimierungs-Operation, welche man dann
ausführen kann.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  opt_op = opt.minimize(cost, var_list=<Hyperparameter-Liste>)
  opt_op.run()
\end{minted}
\para{}
Nun folgt ein kleines Beispielmodell, welches trainiert wird.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  import tensorflow as tf

  # Inputs
  x = tf.placeholder(dtype=tf.float32, shape=(3))

  # Modellparameter
  w = tf.Variable(tf.random_uniform([3], -1, 1), name='weight')
  b = tf.Variable(tf.random_uniform(1, -1, 1), name='bias')

  # Forwärtspropagierung
  z = tf.add(tf.tensordot(x, w, 1), b)
  a = tf.sigmoid(z)
  y = a

  cost = tf.losses.mean_squared_error(labels=y_trü, predictions=y_pred)


  with tf.Session() as sess:
  print(sess.run(cost)) # cost is just another node
  optimizer = tf.train.GradientDescentOptimizer(0.01)
  train = optimizer.minimize()
  res = sess.run(y, feed_dict = {x: [1, 2, 3]})
  print(res)

  # Daten
  x = tf.constant([[1], [2], [3], [4]], dtype=tf.float32)
  y_trü = tf.constant([[0], [-1], [-2], [-3]], dtype=tf.float32)

  # Modell
  linear_model = tf.layers.Dense(units=1)
  y_pred = linear_model(x)

  # Evalutaion
  sess = tf.Session()
  init = tf.global_variables_initializer()
  sess.run(init)

  print(sess.run(y_pred))

  # Loss
  loss = tf.losses.mean_squared_error(labels=y_trü, predictions=y_pred)

  print(sess.run(loss))


  # Training
  optimizer = tf.train.GradientDescentOptimizer(0.01)
  train = optimizer.minimize(loss)


  for i in range(100):
  _, loss_valü = sess.run((train, loss))
  print(loss_valü)

\end{minted}


\subsection{Implementation - der Master}
TF ist gerade deshalb sehr attraktiv für ML, da es sehr performant (schnell
ausführbar) ist und dadurch die Trainingszeit für die Modelle relativ
kurz haltet. Dies wurde durch verschieden Designentscheidungen von TF erreicht.
\para{}
In diesem Abschnitt werden wir grob die zugrundeliegende Implementation von
TensorFlow betrachten - also den Master - und dabei herausfinden weshalb TF so performant ist.
Ausserdem werden wir herausfinden, wie TF ableiten kann.


\subsubsection{Devices}
Die Kernstücke des Masters sind die verschiedenen \keyword{Devices}, auf
welchen die Berechnungen ausgeführt werden. Ein Device ist jedliche Art von
Computerprozessor, auf welchem die Graphen ausführt werden können.
Zu diesen Prozessoren gehören die CPU (Hauptprozessor) und die GPU (Grafikprozessor).
\footnote{
  Die normale TensorFlow Version ist nicht in der Lage die GPU zu verwenden. Um
  sie verfügbar zu machen, muss man die GPU Version installieren. Wir werden im folgenden
  von der GPU-Version ausgehen, da sie erhebliche Performance Vorteile gewährt.
}
\para{}

Der Master analysiert die verfügbaren Devices und bewertete sie, bezüglich
ihren Fähigkeiten. Anhand dieser Bewertung entscheidet der Master dann, auf
welchem Device die Berechnugen ausgefürt werden.
\footnote{
  Es ist möglich die Graphen auf mehreren Devices gleichzeitig auszuführen und
  so noch schneller die Modelle zu trainieren. Dies ist jedoch erst für sehr
  aufwendige Projekte nötig und erfordert ein fortgeschritteneres Versteantniss.
}
Insofern GPUs zur Verfügung stehen, wählt TF grundsätzlich immer diese, da
sie Tensoroperationen deutlich schneller als CPUs verarbeiten können.

\subsubsection{Performance und Hardwarebeschleunigung}
Nun möchten wir die Gründe für die beachtliche Performance von TF erschliessen.
Der erste wichtiger Faktor dabei ist, dass der Master hauptsächlich in C++
geschrieben ist. C++ ist eine Programmiersprache die sehr nahe am Maschinen-Code
(engl.: low-level) ist. Somit ist der Code sehr hardwarenah und kann so
schneller ausgeführt werden. Der Nachteil besteht dafür darin, dass die Programmiersprache
ziemlich verbos ist. Das bedeutet man braucht viel Code um eine relativ einfach Idee
auszudrücken. Deshalb wurde der Client von TF in Python geschrieben, was
Abhilfe verschafft und so ein relativ einfaches und unverboses Entwickeln ermöglicht.
\para{}
Der andere wichtige Aspekt für die Performance ist wohl die sogennante
\keyword{Hardwarebeschleuniging} (engl. Hardware Acceleration), von welcher TF
gebrauch macht. \\
Hardwarebeschleunigung bezeichnet eine Sammlung an Methoden,
bei welchen man spezialisierte Hardware verwendet um rechenintensive Aufgaben
schneller auszuführen. Die Architektur einer CPU ist zwar so ausgelegt, dass
sie beliebige Aufgaben ausführen kann, jedoch meistens nicht besonders
effizient. Deshalb wurden einerseits neue externe Hardwarebausteine, wie die
GPU, welche sich für Grafikberechnungen besonders eignet, entwickelt.
Andererseits wurde auch die interne Architektur der CPU so angepasst, dass sie
auch spezielisierte Aufgaben besser meistert.
\para{}
Der Grossteil der Hardwarebeschleunigung wird durch die
\keyword{Parallelisierung} von Operationen erreicht. Dabei spaltet man eine
rechenintensive Aufgabe in viele Teilaufgaben auf, welche gleichzeitig in
gleicher Weise ausgeführt werden. Voraussetzung dafür ist, dass die
Teilaufgaben unabhängig voneinander bearbeitet werden können. \\
Ein typisches Beispiel für die Paralleliserbarkeit von Operationen ist die
Matrixmultiplikation. Möchte man eine Matrix $\mat{A} \in \set{R}^{n \times m}$ mit
einer zweiten Matrix $\mat{B} \in \set{R}^{m \times p}$ multiplizieren, muss man
praktisch $m$-mal die gleichen Schritte ausführen: Man berechnet jeweils das
Skalarprodukt zwischen der $i$-ten Zeile von $\mat{A}$ und der $i$-ten Spalte
von $\mat{B}$. Alle diese Skalarprodukte haben keinen Einfluss aufeindander und
können so unabhänigig mit der gleichen Prozedur berchnet werden.
\para{}
Allgemein eignen sich Tensoroperatiön für Parallelisierung und damit zur Hardwarebeschleunigung.
Nun ist auch ersichtlich, weshalb TensorFlow als Hauptdatentyp Tensoren
verwendet; damit es gebrauch von der Hardwarebeschleunigung machen kann. wir fast alle Gleichungen des Maschinellen

\paragraph{CPU-Beschleunigung}
Die Hardwarebeschleunigung der CPU ist für TensorFlow nur von geringfügiger
Relevanz, da TF sich vorallem für GPU-Hardwarebeschleunigung eignet
und dafür optimiert ist. Ausserdem ist es praktisch unmöglich die gleiche
Performance von TF mit einer CPU zu erreichen, welche mit einer GPU möglich ist.
\para{}
Hauptsächlich parallelisiert TensorFlow mithilfe von der \keyword{SIMD}. SIMD
ist ein Prinzip zur Paralleliserung einer Insturktion für mehrere
Dateneinheiten und steht deshalb für ``Single Instruction, Multiple Data''.
SIMD umfasst einerseits die spezifische CPU-Architektur und den dazugehörigen
Befehlssatz, um die Operatiön zu parallelisieren.
Es gibt verschiedene

SSE4.1, SSE4.2, AVX, AVX2, FMA

\paragraph{GPU-Beschleunigung durch cuDNN und CUDA}
cuDNN ist die NVIDIA CUDA Deep Neural Network library. Sie ist eine
GPU-beschleunigte Bibliothek für deep neural networks. Sie stellt
implementation für standard routines wie Vorwärts- und Rückwärtspropagierung bereit.

\subparagraph{CUDA}
\subparagraph{cuDNN}


\subsubsection{Automatische Gradientenberechnung}
Als wir die Rückwärtsprogagierung hergeleitet haben, haben wir mithilfe von
Computation Graphs die Kettenregel angewandt um die partiellen Ableitungen zu
bilden. Genau das gleiche macht TF, jedoch automatisiert. Aus diesem Grund ist
TF auch datenstromorient.


% Many optimization algorithms including common ml training algorithms like SGD
% compute the gradient of a cost function with respect to a set of inputs.
% Therefor TF has a built-in support for automatic gradient computation.
% It extends the TF Graph using following procedure:
% If TF needs to compute the gradient of a tensor $C$ with respect to some tensor
% $I$ it first finds the path in the computation graph from $I$ to $C$. It then
% backtracks from $C$ to $I$ and for each operation on the backward path it adds a
% node to the graph composing the partial gradients using the chain rule. The
% newely added node computes the gradient function for the corresponding operation
% in the forward path.
% Code: [db,dW,dx] = tf.gradients(C, [b,W,x])


\subsection{Tensorboard}
TensorBoard ist eine externes Zusatzprogramm zu TensorFlow. Mit dessen Hilfe
kann man Daten zum Modell-Graph und zu dem Trainingsprozess sammeln
und diese dann Visualisieren.
Die wohl praktischste Anwendung von TensorBoard liegt darin, dass man es zum
Einstellen der Hyperparameter benutzen kann.
\para{}
TensorBoard ist als WebServer geschrieben und kann über einen WebBrowser
benutzt werden.
Man muss TensorBoard als Backend/Callback in TensorFlow aktivieren, damit die
nötigen Daten während der Graphausführung gesammelt werden.

\begin{figure}[h!]
  \centering

  \caption{das TensorBoard-Webinterface}
\end{figure}

\subsubsection{Visualisierung des Computational Graphs}
TensorBoard kann dazu benutzt werden den Computational Graph eines Modell zu
visualisieren und zu untersuchen.
Da ein Modell aus tausenden von Knotenpunkten bestehen kann, gruppiert
TensorBoard die Knoten sinnvoll und sorgt so für Übersichtlichkeit.


\begin{figure}[h!]
  \centering

  \caption{ein Graph in TensorBoard}
\end{figure}

\subsubsection{Visualisierung des Trainings}
Mithilfe von TensorBoard kann man das Trainings eines Modells beurteilen. Man
bekommt Funktionsgraphen zur Entwicklung der Kostenfunktion.
TensorBoard eignet sich sehr gut um die Kostenfunktionentwicklung verschiedener
Modelle zu vergleichen. Somit kann man verschiedene Modell güt gegeinander
überstellen. Desweitern ist es so Modelle mit verschiedene Hyperparameter zu
erstellen, zu visualisieren und dann das beste auszuwahlen.

\begin{figure}[h!]
  \centering

  \caption{Trainingsvisualisierungen in TensorBoard}
\end{figure}


\pagebreak
\section{Keras}
\begin{wrapfigure}{l}{3cm}
  \includegraphics[width=3cm]{keras_logo.jpg}
  \caption{Keras-Logo}
\end{wrapfigure}
\keyword{Keras} ist ein Open-Source Deep-Learning Framework geschrieben in
Python. Keras wurde entwickelt, um eine einheitliche Schnittstelle für
verschieden Backends, wie TensorFlow, Microsoft Cognitive Toolkit und Theano zu
bieten. Seit TF Version 1.4 ist Keras ein fester Bestandteil der TensorFlow-Core-API.

Keras ermöglicht ein sehr simples und benutzerfreundliches defnieren von
Deep-Learning-Modeln welche dann sehr einfach trainiert werden können.
Wir werden also anstatt TF direkt zu verwenden, über Keras das Modell definieren.
Deshalb ist ein tiefgehendes Verstätniss von TF überflüssig.

\subsection{Grundsätliche Funktionsweise}
In Keras kann man sehr einfach ein sogennantes \keyword{Seqüntial-Model} baün.
Dies ist ein Modell, welches einfach ein linearer Stapel von verschiedenen
Schichten ist. Keras stellt verschiedene Arten von KNN-Schichten bereits zur
verfügung, welche einfach aneinander gereiht werden können und so einen Graph bilden.
\para{}
Es gibt verschiedene Arten ein Modell zu definieren. Entweder definiert man eine
\code{tf.keras.Seqüntial} Variabel, welcher dann mit der Methode \code{add()}
neue Schichten hinzugefügt werden.
Oder man speichert die verschiedenene Schichten in Variabeln und gibt dann die
Anfangsschicht und die Schlussschicht bei der Erstellung des Modellvariabel an.

\subsection{Schichten}
Wir werden nun kurz die verschiedenen Schichttypen von Keras durchgehen, welche wir in
der Theorie schon vorgestellt haben.

\paragraph{Input-Schicht}
Die Input-Schicht ist ziemlich selbsterklärend. Sie ist einfach die
Platzhalter-Schicht, welche alle Features enthält.
Man definiert sie folgendermassen:
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  tf.keras.Input(shape=(<Form>)
\end{minted}

\paragraph{Dense-Schicht}
Wir beginnen mit dem Fully-Connected-Schicht. Diese wird in Keras als
\code{Dense} bezeichnet. Der Name kommt davon, dass sie dicht mit jedem Neuron
der vorherigen Schicht verbunden ist.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  tf.keras.layers.Dense(<Anzahl Neuronen>,
  activation=<Aktivierungsfunktion>)(<vorherige Schicht>)
\end{minted}

\paragraph{Conv2D-Schicht}

\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  tf.keras.layers.Conv2D(filters=<#Filters>, kernel_size=<Filtergrösse>,
  strides=<>, padding=<>, activation=<>, )
\end{minted}
Für alle Schichten eines CNNs gilt folgendes Datenformat:
Input muss folgende Form haben $(samples, channels, rows, cols)$. Output hat
analoge Form $(samples, filters, new\_rows, new\_cols)$

\paragraph{MaxPool2D-Schicht}

\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  tf.keras.layers.MaxPool2D(pool_size=<Feldgrösse>, strides=<>, padding=<>)
\end{minted}


\paragraph{UpSampling2D-Schicht}

\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  tf.keras.layers.UpSampling2D(size=<>, interpolation=<>)
\end{minted}


\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  model = tf.keras.Seqüntial()
  # Adds a densely-connected layer with 64 units to the model:
  model.add(layers.Dense(64, activation='relu'))
  # Add another:
  model.add(layers.Dense(64, activation='relu'))
  # Add a softmax layer with 10 output units:
  model.add(layers.Dense(10, activation='softmax'))
\end{minted}


\subsection{Training und Evaluierung}

Nach dem das Modell definiert wurde, muss man es Kompilieren.
Dies macht man mit der Methode \code{tf.keras.Model.compile} diese erwartet als
Arugmente, den Optimzier und eine Kostenfunktion. Man kann optional noch gewisse
Metriken angeben, welche wahrend des Trainings erfasst werden sollen.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  model.compile(optimizer=tf.train.AdamOptimizer(0.001),
  loss='categorical_crossentropy',
  metrics=['accuracy'])
\end{minted}
Die Optimizer sind die gleichen wie die in TensorFlow-Core. Sie befinden sich
alle in tf.train. Die Kostenfunktionen sind auch die gleichen wie in TF-Core.
Zum beispeil tf.loss.mean\_squared\_error.
\para{}
Hier ein Beispiel:
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  # Configure a model for mean-squared error regression.
  model.compile(optimizer=tf.train.AdamOptimizer(0.01),
  loss='mse',       # mean squared error
  metrics=['mä'])  # mean absolute error

  # Configure a model for categorical classification.
  model.compile(optimizer=tf.train.RMSPropOptimizer(0.01),
  loss=tf.keras.losses.categorical_crossentropy,
  metrics=[tf.keras.metrics.categorical_accuracy])

\end{minted}

\subsubsection{Fit}
Input NumPy data
For small datasets, use in-memory NumPy arrays to train and evaluate a model. The model is "fit" to the training data using the fit method:

\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  import numpy as np

  def random_one_hot_labels(shape):
  n, n_class = shape
  classes = np.random.randint(0, n_class, n)
  labels = np.zeros((n, n_class))
  labels[np.arange(n), classes] = 1
  return labels

  data = np.random.random((1000, 32))
  labels = random_one_hot_labels((1000, 10))

  model.fit(data, labels, epochs=10, batch_size=32)

\end{minted}

tf.keras.Model.fit takes three important arguments:

epochs: Training is structured into epochs. An epoch is one iteration over the entire input data (this is done in smaller batches).
batch size: When passed NumPy data, the model slices the data into smaller batches and iterates over these batches during training. This integer specifies the size of each batch. Be aware that the last batch may be smaller if the total number of samples is not divisible by the batch size.
validation data: When prototyping a model, you want to easily monitor its performance on some validation data. Passing this argument—a tuple of inputs and labels—allows the model to display the loss and metrics in inference mode for the passed data, at the end of each epoch.

\subsubsection{Predict}
The tf.keras.Model.evaluate and tf.keras.Model.predict methods can use NumPy data and a tf.data.Dataset.

To evaluate the inference-mode loss and metrics for the data provided:
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}

  data = np.random.random((1000, 32))
  labels = random one hot labels((1000, 10))

  model.evaluate(data, labels, batch size=32)

  model.evaluate(dataset, steps=30)


  And to predict the output of the last layer in inference for the data provided, as a NumPy array:

  result = model.predict(data, batch size=32)
  print(result.shape)
\end{minted}

\subsection{Weiss nit}

\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  inputs = tf.keras.Input(shape=(32,))  # Returns a placeholder tensor

  # A layer instance is callable on a tensor, and returns a tensor.
  x = layers.Dense(64, activation='relu')(inputs)
  x = layers.Dense(64, activation='relu')(x)

  predictions = layers.Dense(10, activation='softmax')(x)

  Instantiate the model given inputs and outputs.

  model = tf.keras.Model(inputs=inputs, outputs=predictions)

  # The compile step specifies the training configuration.
  model.compile(optimizer=tf.train.RMSPropOptimizer(0.001),
  loss='categorical_crossentropy',
  metrics=['accuracy'])

  # Trains for 5 epochs
  model.fit(data, labels, batch_size=32, epochs=5)
\end{minted}


% ------------------------------------

\chapter{Entwicklung eines Denoising-Autoencoders}
Nun sind wir in dem Kapitel angelangt, in welchem das ganze erarbeitete Wissen
zusammenfliesst. Wir werden ein konkretes Machine-Learning-Modell in TensorFlow
zusammen mit Keras programmieren. Dabei wird darauf eingegangen, welche Schritte die
Entwicklung eines solchen Modells umfasst und wie dabei am besten vorgegangen
wird.

\section{Das konkrete Modell}
Als konkretes Modell werden wir einen Convolutional-Denoising-Autoencoder,
dessen Theorie wir bereits behandelt haben, programmieren.
Wir werden ihn verwenden um das Bildrauschen von Bildern wegzurechnen und sie so
wieder erkenntlich zu machen. Als
Bilder werden wir handgeschriebene Ziffern vom MNIST-Datenset verwenden.

\begin{figure}
  \caption{grobes Schema des Modells}
\end{figure}

\para{}
\subsection{Daten}
Als Trainingsdaten verwenden wir den \keyword{MNIST}-Datensatz. Er ist der wohl
bekannteste Datensatz für beispielhaftes Maschinelles Lernen.
Er besteht aus schwarz-weiss Bildern von handgeschrieben Ziffern.
Züsätzlich gäbe es auch noch Labels zu den Ziffern, da das Modell meistens
für Ziffernerkennung trainiert wird. Jedoch werden
wir diese nicht brauchen, da wir einen Autoencoder trainieren.
\para{}
Der Datensatz besteht aus einem Trainingsdatensatz von 60'000 Sampels und einem Testdatensatz
von 10'000 Sampels. Alle Ziffern wurden in söfern schon korrekt formatiert,
dass ihre Grösse auf $28 \times 28$ Pixel normalisiert wurde und sie im Bild
zentriert sind.

\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{mnist.png}
  \caption{ein Auszug an MNIST-Bildern \cite{res:mnist_images}}
\end{figure}

\para{}
\cite{net:mnist}

\section{Setup}
Zuallererst müssen wir unser Entwicklungsumfeld richtig konfigurieren. Das
bedeutet, dass wir Python zusammen mit den verschiedenen Programmbibliotheken
installieren müssen.
\para{}
Die Installationsschritte werden in dieser Arbeit für eine arch-basierte
Linuxdistribution, welche den \keyword{Pacman}-Package-Manager verwendet, erklärt.
\para{}
Falls der Leser ein anderes Betriebssystem verwendet, ist auf die offizielle
TensorFlow Website für die Installationsschritte zu verweisen:
\para{}
\qrcode[height=2cm]{https://www.tensorflow.org/install}

\paragraph{Python3}
Um TensorFlow und Keras verwenden zu können, muss Python3 installiert sein.

Um Python3 mithilfe von Pacman zu installieren, muss man folgenden Befehl in
der Kommandozeile ausführen.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize]{bash}
  sudo pacman -S python
\end{minted}

\paragraph{TensorFlow}
Um TensorFlow zu installieren muss man unter Arch Linux nur das entsprechende
Package installieren.
Falls man einen Computer mit einer Nvidia-Grafikkarte mit einer ``Compute
Capability'' von mehr als 3.5 besitzten, kann man von der
GPU-Hardwarebeschleunigung gebraucht machen. Somit installiert man die
CUDA-Version von TF mithilfe folgenden Kommandos:
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize]{bash}
  sudo pacman -S python-tensorflow-cuda
\end{minted}
Pacman installiert nun automatisch zürst alle Programmabhängigkeiten, wie
CUDA, cuDNN und das TensorFlow-Backend, bevor es das eigentliche
Python-TensorFlow installiert.
\para{}
Falls man nicht eine geeignte Grafikkarte besitzt, installiert man am besten
einfach die normale TF-Version ohne Unterstützung für die GPU. Dies macht man
mit diesem Kommando:
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize]{bash}
  sudo pacman -S python-tensorflow
\end{minted}
\para{}
Nun ist TF hoffentlich erfolgreich installiert. Keras muss nicht explizit
installiert werden, da es in TensorFlow implementiert ist.

\paragraph{Python-Module}
Wir werden für unser Python-Programm von zwei Packages Gebrauch machen, welche
nicht in der Standardbibliothek von Python enthalten sind:
\begin{itemize}
\item{NumPy: Ein Package welches verschiedene Mathematische Konzepte
    implementiert; vorallem Vektor- und Matrix-Arithmetik}
\item{Matplotlib: Ein Package zum erstellen von Plots und Grafiken}
\end{itemize}

Man installiert beide mit folgenden Kommandos.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize]{bash}
  sudo pacman -S python-numpy
  sudo pacman -S python-matplotlib
\end{minted}

\section{Entwicklung}
Nun beginnen wir mit der eigentlichen Programmierung.

\subsection{Testprogramm}
Zuallerst werden wir ein kleines Testprogramm schreiben, welches überprüft, ob
alle Programmabhängigkeiten korrekt installiert wurden und verwendet werden können.

In den ersten Zeilen des Pythonprogamms werden wir die Importstatments
schreiben, um NumPy, Matplotlib und TensorFlow verfügbar zu machen.
Keras muss nicht explizit geladen werden, da es in TensorFlow drin steckt.
Dann schreiben wir die Versionnummern der verschiedenen Programmbibliotheken
in die Konsole und können so überprüfen, ob alles richtig konfiguriert ist.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  import numpy as np # NumPy wird impotiert
  import matplotlib as mpl # Matplotlib wird impotiert

  import tensorflow as tf # TensorFlow wird impotiert

  print(np.__version__) # Schreibt die NumPy-Version nach stdout.
  print(mpl.__version__) # Schreibt die Matplotlib-Version nach stdout.
  print(tf.__version__) # Schreibt die TensorFlow-Version nach stdout.
\end{minted}
Falls der Output folgenden Charakter hat (die Versionsnummern müssen nicht
die gleichen sein) und keine Fehlermeldungen angezeigt
werden, sollte alles funktionieren.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{text}
  1.17.
  3.1.1
  1.14.0
\end{minted}
\para{}

\subsection{Trainingsdaten}
Da wir nun verfiziert haben, dass alle Programmabhängigkeiten funktionieren,
können wir mit dem eigentlichen Programm beginnen.
\para{}
Im den folgenden Auschnitten werden wir den Code immer weiter ausbaün und dabei
erfahren, was er bewirkt. Wir beginnen unser eigentlichens Programm mit folgenden Importstatments.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  import numpy as np
  import matplotlib.pyplot as plt
  import tensorflow as tf
\end{minted}

\subsubsection{Laden des MNIST-Datensatzes}
Der nun erste Schritt besteht darin, die Datensätze zu laden. Wie bereits
erwähnt, werden wir den MNIST-Datensatz verwendet. Da dieser dermassen bekannt
ist, gibt es eine Funktion in Keras, welche automatisch die Daten herunterlädt
und sie als NumPy Arrays zurückgibt. Die Funktion gibt die verschiedenen
Komponenten der Daten in folgendem Format zurück \code{(x\_train, y\_train),
  (x\_test, y\_test)}. Da wir nur an den Features \code{x} interessiert sind,
verwerfen wir die Labels \code{y} einfach mithilfe der Wegwerf-Variabel \code{\_}.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  (x_train, _), (x_test, _) = tf.keras.datasets.mnists.load_data()
\end{minted}
Nun sind die Features fürs Training in der Variabel \code{x\_train} und die
Features des Trainingsdatensatzes in der Variabel \code{x\_test} gespeichert.

\subsubsection{Formatieren der Daten}
Nun müssen wir die Daten transformieren, damit sie die richtige Form für
unser Modell haben. Wie bereits erklärt, ist der MNIST-Datensatz ziemlich
handsam, da die Bilder alle die gleichen Masse haben und die Ziffern zentriert sind.
\para{}
Wir müssen die Grauwerte normalisieren, denn im Moment liegen sie noch
im Intervall $[0, 255]$ und sind vom Typ Integer. Unser Modell kann am besten
mit Kommazahlen im Intervall $[0,1]$ umgehen. Um diese Anpassung vorzunehmen
brauchen wir folgenden Code.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  x_train = x_train.astype('float32') / 255.0 # Normalisierung
  x_test = x_test.astype('float32') / 255.0 # Normalisierung
\end{minted}
Desweiteren werden wir in unserem Modell ConvLayers verwenden. Diese in Keras
implementieren ConvLayers erwarten die Inputs in der Form $(m, w, h, c)$, wobei
$m$ die Anzahl Bilder ist, $w$ die Bildbreite, $h$ die Bildhöhe und $c$ die
Anzahl Farbkomponenten. Wir formen nun mithilfe von NumPy das Array um.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  x_train = np.reshape(x_train, (len(x_train), 28, 28, 1)) # neue Form: (60'000, 28, 28, 1)
  x_test = np.reshape(x_test, (len(x_test), 28, 28, 1)) # neue Form: (10'000, 28, 28, 1)
\end{minted}
Nun besitzen die Daten die richtige Form für unser Modell.

\subsubsection{Generieren der verrauschten Bilder}
Den Input in unser Modell stellen nicht die normalen MNIST-Bilder dar, sondern
eine verrauschte Variante von ihnen. Diese werden wir nun generieren in dem wir
ein additives Gauss'sches Rauschen anwenden.
\para{}
Zürst erstellen wir eine Matrix $\mat{R} \in \set{R}^{28 \times 28 \times 1}$, welche die gleiche Form, wie die
Bilder hat. Diese Matrix befüllen wir mit Zufallswerten. Für die
Zufallswerte verwenden wir eine Gauss'sche Normalverteilung
$\mathcal{N}(\mu = 0, \sigma^2 = 1)$ mit Erwartungswert $\mu = 0$ und Varianz
$\sigma^2 = 1$. Da wir für jedes Bild eine eigene Rauschmatrix brauchen,
generieren wir eine Liste $(\mat{R}_1,\mat{R}_2,\ldots,\mat{R}_m$) der Länge $m$ an
Rauschmatrizen. Dafür verwenden wir die NumPy Funktion
\code{np.random.normal(loc=<$\mu$>, scale=<$\sigma^2$>,size=<Form>)}.
Die somit erhaltenen Rauschmatrizen multiplizieren wir dann mit einer
Rauschkonstane \code{noise\_factor} und addieren das Resulat auf die MNIST-Bilder.
Nach dem Hinzufügen der Rauschwerte, müssen wir die Grauwerte noch auf das Intervall
$[0,1]$ kappen. Dafür verwenden wir die NumPy-Funktion \code{np.clip(var, min, max)}. Diese Schritte wenden
wir sowohl auf den Trainingsdatensatz, wie auch auf den Testdatensatz an. \\
Somit lautet unser Code:
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  noise_factor = 0.5

  # für x_train
  noise_matrices = np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)
  noise_matrices *= noise_factor
  x_train_noisy = x_train + noise_matrices
  x_train_noisy = np.clip(x_train_noisy, 0.0, 1.0)

  # für x_test in Kurzfassung
  x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)
  x_test_noisy = np.clip(x_test_noisy, 0.0, 1.0)
\end{minted}
\para{}
Nun kann mithilfe der Matplotlib ein Blick auf die verrauschten Bilder
neben den Originalbildern geworfen werden. Wir erstellen einfach einen Plot, welcher
jeweils 10 Bilder beider Arrays rendert. \para{}
Dazu erstellen wir zürst eine \code{pyplot.figure}. Innerhalb dieser haben wir
jeweils 10 \code{subplots}, für die verrauschten MNIST-Bilder und die
Originale. Wir zeigen dann die entsprechenden Bilder an, nachdem wir sie in Form
gebracht haben. Wir müssen der Grafik auch angeweben, dass sie schwarz-weiss
Bilder anzeigen muss. Zu guter letzt zeigen wir den Plot einfach an.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  n = 10 # jeweils 10 Bilder
  plt.figure()
  for i in range(n):
    # verrauschte Bilder
    ax = plt.subplot(2,n,i+1+n)
    plt.imshow(x_test[i].reshape(28,28))
    plt.gray()

    # Original Bilder
    ax = plt.subplot(2, n, i+1)
    plt.imshow(x_test_noisy[i].reshape(28,28))
    plt.gray()

  plt.show()
\end{minted}
Wir erhalten so folgende Grafik:
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\textwidth]{noised_clipped.png}
  \caption{Verrauschte Bilder neben den Orignalbildern}
\end{figure}

\para{}
Nun haben wir die Daten vollständigt im richtigem Format. Damit können wir nun
zur Definition des Modells übergehen.



\subsection{Modell definieren}
Nun werden wir mithilfe von Keras das Modell unseres
Convolutional-Denoising-Autoencoders definieren.
Wir werden das Modell nach dem Vorbild der Theorie baün.
Wie wir wissen umfasst die Topologie eines CNNs die verschiedentsten
Hyperparameter. Wir werden der einfachheitshalber vorerst unbegründete Werte
dafür wählen. Später werden wir diese Hyperparameter noch einstellen.
\para{}
Wir beginnen unser Modell mit der Inputsschicht $l=0$, welche einfach ein Tensor
ist, welcher die Inputswerte hält. Die Schicht hat die gleiche Form, wie ein MNIST-Bild.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  input_data = tf.keras.Input(shape=(28,28,1))
\end{minted}
\para{}
Nach dieser Inputschicht folgt der Encoder des Autoencoders. Dieser besteht
sowohl aus Convolutional-Schichten, wie auch aus Pooling-Schichten, welche sich abwechseln.
Wir werden wie gesagt für alle Schichten vorerst die gleichen unbegründeten
Hyperparameter wählen. Für die Convolutional-Schichten wählen wir
eine Filtergrösse $f^l = 3$, eine Anzahl Filter von $c^l = 32$,
einen Stride $s=1$, Same-Padding und die ReLU-Aktivierungsfunktion $\varphi = \varphi^{\text{ReLU}}$.
Der Code für die Defition einer solchen Convolutional-Schicht lautet:
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  conv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3),
  strides=(1,1),padding='same', activation='relu')(input_data)
\end{minted}
Für die Pooling-Schichten wählen wir analog eine Feldgrösse $f = 2$, einen
Stride $s = 2$, damit jedes Feld einmal zusammengefasst wird und Same-Padding.
Der Code für eine dieser Pooling-Schichten lautet:
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  pool_layer = tf.keras.layers.MaxPooling2D(pool_size=(2,2),strides=(2,2),padding='same')(econv0)
\end{minted}
Nun folgt der Decoder.
Für ihn verwenden wir die gleichen Convolutional-Schichten wie im Encoder. Anstatt
Pooling-Schichten enthält er UpSampling-Schichten, bei welchen die
Feldgrösse $f = 2$ gewählt wurde. Somit werden $4 \times 4$-Felder zu einem
einzigen Pixel zusammengefasst. Als Interpolationsmethode benutzten wir den Nächsten-Nachbar-Algorithmus.
Eine solche UpSampling-Schicht definiert man also so:
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  dupsample0 = tf.keras.layers.UpSampling2D(size=(2,2), interpolation='nearest')(dconv0)
\end{minted}
Wir definieren jetzt alle Schichten und verknüpfen sie miteinander. Wir baün
einen Autoencoder, dessen Flaschenhals zwei Convolutional-Schichten tief ist.
Somit besteht der Encoder aus zwei Convolutional-Layer-Pooling-Layer-Paar, dann
folgt der Flaschenhals und aus zwei Convolutional-Layer-UpSampling-Layer-Paar.
Der ganze Graph wird durch folgenden Code gebildet:
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  # Encoder
  econv0 = tf.keras.layers.Conv2D(
  filters=32, kernel_size=(3,3), strides=(1,1),padding='same',activation='relu')(input_data)
  emaxpool0 = tf.keras.layers.MaxPooling2D(
  pool_size=(2,2),strides=None,padding='same')(econv0)
  econv1 = tf.keras.layers.Conv2D(
  filters=32, kernel_size=(3,3), strides=(1,1),padding='same',activation='relu')(emaxpool0)
  emaxpool1 = tf.keras.layers.MaxPooling2D(
  pool_size=(2,2), strides=None,padding='same')(econv1)

  encoded =  emaxpool1 # Flaschenhals der Form (7,7,32)

  # Decoder
  dconv0 = tf.keras.layers.Conv2D(
  filters=32, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu')(encoded)
  dupsample0 = tf.keras.layers.UpSampling2D(
  size=(2,2), interpolation='nearest')(dconv0)
  dconv1 = tf.keras.layers.Conv2D(
  filters=32, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu')(dupsample0)
  dupsample1 = tf.keras.layers.UpSampling2D(
  size=(2,2), interpolation='nearest')(dconv1)
  dconv2 = tf.keras.layers.Conv2D(
  filters=1,kernel_size=(3,3), strides=(1,1), padding='same', activation='sigmoid')(dupsample1)
  decoded = dconv2
\end{minted}
Wie im Code zu sehen, haben wir für alle Convolutional-Layers die ReLU
Aktivierungsfunktion gewählt, ausser der letzten Schicht. Für die letzte
Schicht gebauren wir die Sigmoid-Aktivierungsfunktion. Dies hat den Grund, dass
der Output unseres Netzwerk wieder ein Bild sein muss, dessen Grauwerte im
Invervall $[0,1]$ liegen müssen. Eine Sigmoidfunktion hat genau dieses
Intervall als Wertemenge.
\para{}
Nun müssen wir noch das Kerasmodell mit diesem Graphen definieren. Also
definieren wir das Modell einfach als der Abschnitt des Graphen zwischen der
Input und dem Decoding.
Und es danach mit kompilieren. Bei der Kompilierung wahlen wir als
Optimierungsverfahren einfach das Stochastische-Gradientenverfahren SGD. Als
Kostenfunktion wählen wir den Mittleren-Quadratischen-Fehler $C_{\text{MSE}}$.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  autoencoder = tf.keras.Model(input_data, decoded)
  autoencoder.compile(optimizer='sgd',loss='mean_squared_error') # SGD und MSE
\end{minted}
Mithilfe von \code{tf.keras.Model.summary} kann man eine Zusammenfassung des
Modells in Textform erhalten. So kann man Informationen bezüglich der Form der
verschiedenen Schichten extrahieren und überprüfen, ob alles stimmig ist.
Für unser Modell erhalten wir folgende Summary:
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{text}
  _________________________________________________________________
  Layer (type)                 Output Shape              Param #
  =================================================================
  input_1 (InputLayer)         [(None, 28, 28, 1)]       0
  _________________________________________________________________
  conv2d (Conv2D)              (None, 28, 28, 32)        320
  _________________________________________________________________
  max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0
  _________________________________________________________________
  conv2d_1 (Conv2D)            (None, 14, 14, 32)        9248
  _________________________________________________________________
  max_pooling2d_1 (MaxPooling2 (None, 7, 7, 32)          0
  _________________________________________________________________
  conv2d_2 (Conv2D)            (None, 7, 7, 32)          9248
  _________________________________________________________________
  up_sampling2d (UpSampling2D) (None, 14, 14, 32)        0
  _________________________________________________________________
  conv2d_3 (Conv2D)            (None, 14, 14, 32)        9248
  _________________________________________________________________
  up_sampling2d_1 (UpSampling2 (None, 28, 28, 32)        0
  _________________________________________________________________
  conv2d_4 (Conv2D)            (None, 28, 28, 1)         289
  =================================================================
  Total params: 28,353
  Trainable params: 28,353
  Non-trainable params: 0
  _________________________________________________________________
\end{minted}
Wir sehen hier schön, dass wir circa 28'000 Hyperparameter haben, welche wir zu
trainieren haben.

\subsection{Training}
Nun müssen wir das definierte Modell trainieren.
Mit Keras ist das mit einem Funktionsaufruf gemacht. Diese Funktion erwartet
einige Argumente. Zürst müssen wir die Features und die Labels angeben. Die
Features haben wir in der Variabel \code{x\_train\_noisy} gespeichert. Die Labels
sind die unverrauschten MNIST-Bilder, welche in Variable \code{x\_train} sind.
Desweiteren spezifizieren wir die grösse eines Mini-Batches als 128. Wir werden
für 100 Epochen trainieren und geben dies entsprechend an. Wichtig ist auch,
dass wir die Trainingssampels vor dem Training mischen, damit keine Mustern
innerhalb der Anordnung der Samples erlernt werden. Als letzen geben wir noch
den Testdatensatz an.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  autoencoder.fit(
  x=x_train_noisy, y=x_train, batch_size=128, epochs=100, shuffle=Trü,validation_data=(x_test_noisy,x_test))
\end{minted}
Wenn wir jetzt unser Modell trainieren, macht TensorFlow, falls die CUDA-Version
von TF installiert ist und eine geeignete
Nvidia-Grafikarte gefunden wurde, gebrauch von ihr. Somit sollte das Training
innerhalb einiger Minuten beendet sein.
Während des Trainings schreibt TensorFlow Informationen zum Fortschritt in die
Kommandozeile. Diese Infos haben folgenden Charakter und informieren über die aktülle Epoche und die
Werte der Kostenfunktion. Die Kosten für den Trainingsdatensatz werden mit
\code{loss} bezeichnet und die für den Testdatensatz mit \code{val\_loss}.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{text}
  Epoch 99/100
  60000/60000 [==============================] - 3s 57us/sample - loss: 0.1102 - val_loss: 0.1089
\end{minted}
Das Modell hat nach dem dem Training einen \code{val\_los} von $0.1089$,
bezüglich dem Testdatensatz.
\para{}
Jetzt, da das Modell trainiert ist, speichern wir es und alle seine
Modellparameter als Datei auf unserem Computer ab.
So müssen wir es nicht jedesmal wieder neu trainieren, sondern können einfach
die Modelldatei einlesen lassen.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  autoencoder.save('denoiser.model')
\end{minted}
Um das Modell wieder einzulesen verwendet man folgenden Code:
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  autoencoder = tf.keras.models.load_model('denoiser.model')
\end{minted}
Mit der Summary-Funktion kann man überprüfen, ob das richtige Modell geladen wurde.

\subsection{Modell ausführen}
Nun kann das trainierte Modell verwenden werden, um nun eben Bilder zu
entrauschen. Wir verwenden dafür Bilder aus dem Testdatensatz, da diese nicht
zum Training verwenden wurden. Mit der Funktion \code{tf.Keras.Model.predict}
erhält man alle Vorhersagen unseres Autoencoders zum ganzen Testdatensatz.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  denoised_imgs = autoencoder.predict(x_test)
\end{minted}
Mithilfe von der Matplotlib können wir nun diese Entrauschten Bilder mit den
ursprünliche MNIST-Bildern Seite an Seite darstellen. So können wir von Auge
beurteilen, wie gut die Ergebnisse sind.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  n = 10 # jeweils 10 Bilder
  plt.figure()
  for i in range(n):
    # verrauschte Bilder
    ax = plt.subplot(3, n, i+1)
    plt.imshow(x_test_noisy[i].reshape(28,28))
    plt.gray()

    # entrauschte Bilder
    ax = plt.subplot(3, n, i+1+n)
    plt.imshow(decoded_imgs[i].reshape(28,28))
    plt.gray()

    # Original-Bilder
    ax = plt.subplot(3, n, i+1+2*n)
    plt.imshow(x_test[i].reshape(28,28))
    plt.gray()

  plt.show()
\end{minted}
In der Grafik, welche wir erhalten, sind zuoberst die verrauchsten Bilder, in
der Mitte sind die Rekonstruktionen und ganz unten sind Original-Bilder.
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\textwidth]{denoised_clipped.png}
  \caption{Verrauschte Bilder, Rekonstruktionen und Orignalbilder}
\end{figure}
Die Resultate sind schon verblueffend güt.

\subsection{Hyperparameter einstellen}
Wir in der Theorie so oft erkärt, muss man um die Hyperparameter einzustellen
einfach verschiedene Werte ausprobieren. Wir werden nun unser Programm zu
modifizeren, dass er verschiedene Modell mit jeweils unterschiedlichen
Hyperparametern trainiert. Beim Training erfassen wir mithilfe von TensorBoard
Daten zum Lernfortschritt und den Kosten. Dann können wir mithilfe von
TensorBoard eine Auswertung ausführen und so die besten Hyperparameter finden.
\para{}
Zürst definieren wir einige Arrays, welche die Werte enthalten, welche wir als
Hyperparameter testen wollen. Die Anzahl Epochen stellen auch einen
Hyperparameter dar, diesen müssen wir aber nicht testen, da es auf der Hand
liegt, dass mit mehr Epochen als Trainingszeit auch die Resulate besser werden.
Wir werden für alle Hyperparameter 20 Epochen lang trainieren.
\para{}
Wir werden folgende Mini-Batch-Grössenen ausprobieren: 16, 32, 64, 128, 265.
Die Vermutung ist nahezulegen, dass mit kleineren Mini-Batches die
Gradientapproximationen besser mit dem reellen Gradienten überstimmen. Somit
werden die Resulate vermutlich besser werden. \\
Desweiteren werden wir folgende Anzahl Filter austesten: 16, 32, 64, 128, 256.
Wir werden auch die beiden Aktivierungsfunktionen Sigmoid und ReLU ausprobieren.
Wir werden für jede Schicht die gleiche wählen, ausser für die letzte. Die
letzte Schicht braucht immer eine Sigmoidfunktion, damit die Outputs das
richtige Format haben.

\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  EPOCHS = 20

  POSSIBLE_BATCH_SIZES = [ 16, 32, 64, 128, 256]
  POSSIBLE_NUMS_FILTER = [ 16, 32, 64, 128, 256]
  POSSIBLE_ACITVATIONFUNCTIONS = [ 'sigmoid', 'relu' ]
\end{minted}
Nun werden wir den gesamten Code zum Baün des Graphens, zur Kompilierung und
zum Training des Modells in verschachtele For-Loops stecken, welche alle
Kombinationen an Hyperparameter ausprobiert.

\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  for activation_function in POSSIBLE_ACITVATIONFUNCTIONS:
    for batch_size in POSSIBLE_BATCH_SIZES:
      for num_filters in POSSIBLE_NUMS_FILTER:
        # ganzer Modell-Code
\end{minted}

\subsection{TensorBoard Konfiguration}
Damit wir den Lernprozess und die Leistungen unseres Modells analysieren
können, werden wir nun TensorBoard konfigurieren.
Um TensorBoard einzurichten, müssen wir zürst eine entsprechende TensorBoard
Variable definieren. Hierbei müssen wir angeben wohin die TensorBoard-Daten
geschrieben werden. Da wir die verschiedenten Hyperparameter-Einstellungen
haben, welche wir alle vergleichen wollen, benennen wir die Dateien
aussagekräftig. \\
Desweiteren spezifizieren wir gewisse Optionen für die Datenerfassung.
Diese müssen bei der Komilierung des Modells unter \code{metrics} angegeben
werden. Wir interessieren uns an der Akkuratesse des Modells.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  autoencoder.compile(optimizer='sgd',loss=loss, metrics=['accuracy'])
  NAME = '{}-{}-batch-{}-filters-{}-denoiser'.format(loss, activation_function, batch_size, num_filters)
  tensorboard =
  tf.keras.callbacks.TensorBoard(log_dir='logs/{}'.format(NAME))
  autoencoder.fit(x=x_train_noisy, y=x_train, batch_size=batch_size, epochs=EPOCHS, shuffle=Trü,validation_data=(x_test_noisy,x_test),callbacks=[tensorboard])
\end{minted}

\subsection{Leistung unseres Modells}
Echt gut.

%%% TeX-command-extra-options: "-shell-escape"
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
