\chapter{Maschinelles Lernen}

Im ersten Kapitel sollen die Grundlagen für ein fundiertes Verständnis
von Maschinellem Lernen gelegt werden. Neben den verschieden Arten sollen
essentielle Begriffe und das Modellkonzept dargelegt werden. Darüber hinaus werden
die Funktionsweise des Trainings sowie daraus entstehende Phänomene erläutert.
Auf dieser Basis erfolgt in Kapitel zwei die Darlegung eines konkreten Modells.
\para{}
\keyword{Maschinellen Lernens} beschäftigt sich mit Algorithmen und
mathematischen Modellen, welche die Fähigkeit entwickeln, Probleme selbständig
zu lösen.
Hierbei wird nicht explizit einprogrammiert, wie das Modell das Problem zu lösen
hat, stattdessen wird das Modell trainiert, optimiert sich von selbst und findet selbst
einen Weg, die Aufgabenstellung zu lösen.
Die Grundidee dabei ist, dass Daten erfasst, generiert oder gemessen werden,
welche anschliessend analysiert werden sollen.
Innerhalb dieser Daten existieren gewisse
Gesetzmässigkeiten und Muster. Diese Muster sollen vom Modell
erkannt und verallgemeinert werden. Nach dem erfolgreichen Lernprozess,
kann das Modell Vorhersagen für neue Daten machen.
\para{}
Üblicherweise werden zwei Arten von Maschinellem Lernen unterschieden:
\begin{itemize}
\item{
    \keyword{Überwachtes Lernen} (engl.:\ supervised learning) ist ein
    Lernverfahren, bei welchem die Daten aus zwei Teilen bestehen, aus Inputs und
    Outputs. Man bezeichnet dabei die Outputs als Labels. Die Aufgabe des Modells
    ist es, eine \keyword{Korrelation} zwischen den Inputs und den Labels zu
    erlernen und so ihre Beziehung zueinander zu verstehen.
    Anhand der Informationen, welche die Inputs enthalten,
    können die Labels vorhergesagt werden. Die vorhergesagten Labels des Modells
    werden mit den wahren Labels abgeglichen. Mit dieser Überwachung werden die
    Fähigkeiten des Modells bewertet.
    \para{}
    Voraussetzung dafür ist, dass die Daten ``gelabelt'' sein müssen.
    Daher müssen bereits vorgägig Daten vorhanden sein, bei welchen die gewünschten
    Labels aufweisen. Zudem muss auch die erwähnte Korrelation bestehen. Falls
    kein Zusammenhang zwischen den Inputs und den Outputs existiert, kann das
    Modell auch keine Vorhersagen machen und damit auch nichts erlernen.
  }
\item{
    \keyword{Unüberwachtes Lernen} (engl.:\ unsupervised learning) ist ein anderes
    Lernverfahren, bei welchem diese Labels nicht vorhanden sind. Dem
    Modell stehen nur die Inputdaten zur Verfügung. Diese werden ebenfalls analysiert
    und das Modell soll Muster heraus extrahieren, welche sich von einem
    zufälligen Datenrauschen unterscheiden.
  }
\end{itemize}

Der Grossteil der Modelle des Maschinellen Lernens zählt zum Überwachten
Lernen, da es deutlich mehr Anwendungsmöglichkeiten bietet. Grundsätzlich steht
daher das Überwachte Lernen im Vordergrund der Arbeit. Allerdings gehört der
später erläuterte Autoencoder strenggenommen (aufgrund einer irreführender Zuordnung) zum Unüberwachten Lernen, auch wenn
es der gleichen Systematik des Überwachten Lernens folgt (siehe Sektion \ref{sec:autoencoder}).
\para{}
GUTE QUELLE (Pionier)
\cite{wiki:supervised_learning}
\cite{wiki:unsupervised_learning}

\section{Allgemeine Begriffe}

\subsection{Daten}

Um ein Modell zu trainieren, werden Daten benötigt. Diese Daten bestehen immer aus
\keyword{Inputs} und \keyword{Outputs}. Man unterscheidet hierbei zwischen zwei Arten
von Outputs. Die \keyword{Labels} sind die erwarteten Outputs, welche die
gewünschten Zielwerte sind. Die \keyword{Vorhersagen} sind die Outputs, die vom
Modell produziert werden und welche hoffentlich möglichst genau mit den Labels
übereinstimmen.
\para{}
Diese Daten für das Training kommen in der Form eines
\keyword{Trainingsdatensatzes} $\set{X}$ dar.
Es handelt sich um eine Menge von \keyword{Samples},
welche jeweils aus Inputs und Labels bestehen.
Die Inputs werden in einem Vektor
\[ \vec{x} = \trans{\begin{pmatrix} x_1 & x_2 & \cdots & x_m \end{pmatrix}} \]
und die Labels in einem Vektor
\[ \vec{\hat{y}} = \trans{\begin{pmatrix} \hat{y}_1 & \hat{y_2} & \cdots & \hat{y}_n \end{pmatrix}} \]
zusammengefasst. Somit ist ein Trainingssample ein Paar
$(\vec{x}_i,\vec{\hat{y}}_i)$ bestehend aus einem Inputvektor $\vec{x}$ und eines Labelvektor
$\vec{\hat{y}}$.
Die Vorhersagen werden ebenfalls in einem Vektor
\[\vec{y} = \trans{\begin{pmatrix} y_1 \quad y_2 \quad \cdots \quad y_n \end{pmatrix}} \]
zusammengefasst.
\para{}
Die Inputs beinhalten sogenannte \keyword{Features} (deutsch: Merkmale). Sie
zeichnen die Inputs aus und umfassen ihren gesamten Informationsgehalt.
Der Algorithmus soll
anhand dieser Features seine Vorhersagen machen.
Diese Vorhersagen werden dann mit den Labels abgeglichen und bewertet.
Anhand der Bewertungen wird eine Optimierung des Modells vorgenommen.
Unter korrekten Bedingungen (kein Overfitting (siehe \ref{sec:overfitting})) findet kein Auswendig lernen der Trainingsdaten statt,
sondern ein Generalisieren des Zusammenhangs anhand von Mustern und Gesetzmässigkeiten.
\para{}
Um eine endgültige Bewertung des Modells durchzuführen, wird ein Testdatensatz
$\set{T}$ genutzt, um Vorhersagen zu generieren. Dieser ist nicht Teil des Trainingsdatensatz.
Dies garantiert, dass kein Auswendiglernen möglich ist.
\para{}
BEISPIEL


\subsection{Modelle}
Ein \keyword{Modell} ist eine mathematische Funktion $\mathit{h}\colon \set{R}^m
\to \set{R}^n$, Hypothesenfunktion genannt, welche die Inputs auf die Outputs abbildet $\vec{y}=\mathit{h}(\vec{x})$.
Man kann diese Funktion als die Hypothese auffassen, welche das Modell bezüglich der Beziehung zwischen
den Inputs und den Labels aufgestellt hat.
Diese Modelle können verwendet werden, um entweder
Klassifizierungsprobleme oder Regressionsprobleme zu lösen. Falls es sich um
Letzteres handelt, spricht man von einem Regressionsmodell. Ein solches
Regressionsproblem soll im Rahmen dieser Arbeit gelöst werden.
\para{}
Das Verhalten eines Modells bestimmt sich durch seine \keyword{Modellparameter}
$\param_1, \param_2,\ldots,\param_k$. Sie determinieren, wie die Hypothese des Modells lautet.
Das Ziel ist es, die Modellparameter so einzustellen, dass die Vorhersagen
$\vec{y}$ des Modells besser mit den Labels $\vec{\hat{y}}$ der Trainingsdaten übereinstimmen.
Dies wird iterativ gemacht, indem immer wieder leichte Anpassungen an den
Parametern vorgenommen werden, bis das Modell die gewünschten Resultate liefert.
\para{}
Neben den gelernten Parametern, gibt es auch noch sogenannte \keyword{Hyperparameter}.
Diese können nicht erlernt werden, sondern müssen manuell vor dem Training gewählt werden und können den Lernvorgang erheblich beeinflussen.
Dies bedeutet, dass man ausprobieren muss, welche Werte der Hyperparameter
die besten Resultate liefern.
\para{}
Das wohl einfachste Beispiel eines Regressionsmodell ist eine Regressionsgerade. Diese ist
angemessen, falls ein einfacher linearen Zusammenhang der Form $y=\param_1x +
\param_0$ zwischen den Features und den Labels besteht.
Danach müssten nur noch $\param_0$ und $\param_1$ bestimmt werden.
BEISPIELMODELL AUTO
\para{}
Für Maschinelles Lernen haben sich gewisse Modelle besonders gut etabliert,
dazu zählen: Support Vector Machines, Evolutionäre Algorithmen, und Künstliche Neuronale Netze.
Diese Arbeit wird sich vorwiegend mit Neuronalen Netzen auseinandersetzen.
\\
\begin{figure}[h!]
  \centering


  \caption{Schema eines Modells}
\end{figure}

\section{Training}
\subsection{Verlust- und Kostenfunktionen}
Einsicht ist der erste Schritt zur Besserung. Diese Maxime gilt auch für das
Machine Learning.
Deshalb muss beim Training zuerst die Genauigkeit des Modells bewertet werden.
Dies wird mithilfe von sogenannten Kostenfunktionen bzw. Verlustfunktionen erreicht.
\para{}
Eine \keyword{Verlustfunktion} $L(y,\hat{y})$ soll ein Mass für die
Abweichung der Vorhersage $y$ von dem Label $\hat{y}$ sein.
Aus ihr bildet man die \keyword{Kostenfunktion} $C(\vec{y},\vec{\hat{y}})$, indem die
Verluste der einzelnen Outputs aufsummiert werden. Somit erhält man die Kosten
einer gesamten Vorhersage mit $m$ Outputs (siehe Gl. (\ref{eq:errorfunc})). Bei
gewissen Kostenfunktionen gibt es keine Verlustfunktion, da diese nicht auf die
einzelnen Outputpaare aufgeteilt werden kann. \\
Der Fehler $\bar{C}(\set{X})$ des gesamten Trainingsdatensatzes $\set{X}$, der
Grösse $p$, ergibt sich aus dem arithmetischen Mittel der einzelnen Kosten der
Vorhersagen (siehe Gl. (\ref{eq:meanerrorfunc})).
\\
\begin{minipage}[h!]{0.5\textwidth}
  \begin{equation}\label{eq:errorfunc}
    C \left(\vec{y},\vec{\hat{y}} \right) = \ds\sum_{i=1}^{m} L(y_i, \hat{y}_i)
  \end{equation}
\end{minipage}
\begin{minipage}[h!]{0.5\textwidth}
  \begin{equation}\label{eq:meanerrorfunc}
    \bar{C}(\set{X}) = \frac{1}{p}\ds\sum_{j=1}^{p} C\left(\vec{y}_j,\vec{\hat{y}}_j\right)
  \end{equation}
\end{minipage}
\para{}
Eine Kostenfunktion $C$ sollte folgende Eigenschaften aufweisen:
\begin{itemize}
\item{$C$ ist minimal, wenn $\vec{y} = \vec{\hat{y}}$}
\item{$C$ wächst mit $|\vec{y} - \vec{\hat{y}}|$}
\item{$C$ ist nach jedem $y_n$ partiell differenzierbar (erklärt in Anhang (\ref{ref:partielle_ableitungen}))}
\end{itemize}
\para{}

\cite{Nielsen}

\subsubsection{Mittlere quadratische Abweichung}
Die bekannteste Kostenfunktion ist die ``Mittlere quadratische Abweichung''.
(engl.:\ mean squared error). Sie ist definiert als das arithmetische Mittel
aller quadrierten Differenzen zwischen den Vorhersagen und Labels.
Zusätzlich halbiert man noch das arithmetische Mittel, damit bei der Ableitung der Faktor
$2$ wegfällt. Sie hat keine entsprechene Verlustfunktion, da man so das
arithemtische Mittel nicht ausgedrückt werden kann.
\\
Die Kostenfunktion kann mithilfe einer Summe berechnen werden, oder in
diesem Fall mithilfe einer Vektorsubtraktion Vorhersagen $\vec{y}$ von den Labels $\vec{\hat{y}}$.
\\
\begin{equation}\label{eq:MSE}
  C_{MSE} = \frac{1}{2n}\sum_{i=1}^{n}{(\hat{y}_i - y_i)}^2 = \frac{1}{2n}{(\vec{\hat{y}} - \vec{y})}^2
\end{equation}
\\
Sie erfüllt alle Anforderungen an eine Kostenfunktion:
\begin{itemize}
\item{Ihr Funktionwert ist 0 und minimal für $\vec{y} = \vec{\hat{y}}$}
\item{Sie ist proportional zu ${(\vec{\hat{y}}-\vec{y})}^2$}
\item{Ihre partielle Ableitung nach einem $y_i$ lautet: $C_{MSE}'=\frac{1}{n}(y_i-\hat{y_i})$}
\end{itemize}


\subsection{Stochastisches Gradientenverfahren}\label{sec:gradientenverfahren}
Beim Training eines Modells handelt es sich um ein Optimierungsproblem.
Das Modell macht die besten Vorraussagen, wenn die
Funktionswerte der Kostenfunktion am kleinsten sind.
Deshalb muss die gewählte Kostenfunktion $C$ minimiert werden.
Hierbei muss die Funktion $C$ nicht mehr in Abhängigkeit der Inputs und Outputs betrachtet
werden, sondern als Funktion der Modellparameter
$C(\param_1, \param_2, \ldots, \param_k)$. Denn diese Werte sollten angepasst
werden, um das Modell zu verbessern.
\para{}
Für diese Optimierung wird das sogenannte \keyword{Gradientenverfahren} (engl.: Gradient descent) verwendet.
Das Gradientenverfahren ist eine gängige Methode, um Funktionen $f: \set{R}^n \to
\set{R}$ zu minimieren
\footnote{Üblicherweise wird auf Gymnasialstufe vermittelt, dass die lokalen
  Extrema einer Funktion $f$ bestimmt werden
  können, indem die erste Ableitung $f'$ gebildet und gleich null gesetzt
  wird. Dies ist hier jedoch nicht möglich, da die Funktion
  $C'(\param_1,\param_2,\ldots,\param_n)$ zu kompliziert ist, um die
  Nullstellen analytisch zu bestimmen. Deshalb wird hier das numerische Gradientenverfahren
  verwendet.
}.
Falls dem Leser das Prinzip nicht vertraut ist, wird auf Anhang
(\ref{sec:anhang_gd}) verwiesen, in
welchem die Funktionsweise des Gradientenverfahrens erklärt wird.
\para{}
Das Gradientenverfahren zur Minimierung verläuft iterativ nach folgender
Gleichung:
\begin{equation}\label{eq:gd_ml}
  \vec{\param}_{t+1} = \vec{\param}_t - \eta \cdot \vecf{\nabla} \mathit{C}(\vec{\param}_t)
\end{equation}
Wie die initialen Modellparameter $\vec{\param}_{t=0}$ zu wählen sind, wird in Sektion
(\ref{sec:parameter_initalisieren}) erläutert.
\para{}
Die sogenannte \keyword{Lernrate} $\eta$ aus Gleichung (\ref{eq:gd_ml}) stellt
einen Hyperparameter dar. Sie ist der positive Proportionalitätsfaktor, welcher die Schrittgrösse des
Gradientenabstiegs bestimmt.
Je nach zu minimierender Funktion muss sie anders gewählt werden.
Dies geschieht durch Ausprobieren. Falls $\eta$ nicht gut gewählt wurde, ergeben
sich Probleme beim Training:
\begin{itemize}
\item{Falls $\eta$ zu klein ist, verläuft das Trainings unnötig langsam.
    Ausserdem kann es passieren, dass die Optimierung bei einem hohen lokalen Minimum stecken bleibt.}

\item{Falls $\eta$ zu gross ist, kann es passieren, dass man über das lokale
    Minimum hinaus schiesst und somit nur darum herum springt.}
\end{itemize}

\begin{figure}[h!]
  \centering
  \begin{tikzpicture}
    \draw[gray] (0,0) grid[step=0.5cm] (5,5);
    \draw[ultra thick,black,->] (-0.5,0) -- (5.5,0) node[right] {$x_1$};
    \draw[ultra thick,black,->] (0,-0.5) -- (0,5.5) node [above left] {$f(x_1)$};
  \end{tikzpicture}
  \caption{Visualisierung verschiedener Lernraten}
\end{figure}

Aus nachfolgend erläuterten Gründen wird für ML ein etwas angepasstes
Verfahren des Gradientenabstiegs verwendet: das \keyword{Stochastische
  Gradientenverfahren} (SGD).
Das Problem des herkömmlichen Gradientenverfahrens besteht darin, dass der
Gradient für den \textit{gesamten} Trainingsdatensatz berechnet werden muss.
Dies ist zwar ein exakter Prozess, aber ein extrem langsamer zugleich.
Bei grossen Datensätzen würde es sehr lange dauern, bis das Modell nur annähernd gute Vorhersagen machen könnte.
Somit steht die Genauigkeit in keinem Verhältniss zur Effizienz dieser Methode.
\para{}
Bei SGD wird der ``echte'' Gradient des gesamten Datensatzes mit dem Gradienten einiger Trainingssamples approximiert.
Dazu wird der Trainingsdatensatz in sogenannte \keyword{Mini-Batches} eingeteilt und der Gradient jeweils pro Mini-Batch berechnet.
Als Konsequenz finden deutlich mehr Iterationen in einer einzigen
Durchkämmung der Trainingsdaten statt. Eine solche vollständige Durchkämmung der Trainingsdaten
bezeichnet man als eine \keyword{Epoche}.
Oft wird mehrere Epochen lang trainiert bis das Modell genügend gute Resultate
liefert. Jedoch sollte auch nicht zu oft mit den gleichen Daten trainiert
werden, da es sonst zu Overfitting (siehe Sektion \ref{sec:overfitting}) kommen kann.
Der Gradienten eines genug grossen Mini-Batches ist zwar nicht ganz exakt, aber approximiert den Gradieten des gesamten Datensatzen genügend gut.
Sowohl die Mini-Batch Grösse, wie auch die Anzahl Epochen sind weitere Hyperparameter.
\para{}
Die partiellen Ableitungen der gesamten Trainingsdaten werden mit dem
arithmetischen Mittel der partiellen Ableitungen eines Mini-Batches der Grösse $q$ approximiert.
\\
\begin{equation}\label{eq:minibatch_deriv}
  \partderiv{\bar{C}}{\param_k} \approx \frac{1}{q}\sum_{i=1}^{q} \partderiv{C_i}{\param_k}
\end{equation}
\\
Eine Iteration des Stochastischen Gradientenverfahrens wird analog zu Gleichung (\ref{eq:gd_ml}) folgendermassen durchgeführt.
\\
\begin{equation}\label{eq:sgd}
  \param_{k,t+1} = \param_{k,t} - \frac{\eta}{q} \sum_{i=1}^{q} \partderiv{C_i}{\param_{k,t}}
\end{equation}

\cite{Nielsen}

\section{Trainingsphänomene}

\subsection{Konvergenz und Divergenz}
Beim Training eines Modells kann es entweder zu \keyword{Konvergenz} oder \keyword{Divergenz} kommen.
Falls die Vorhersagen im Verlaufe des Trainings immer besser mit den Labels
übereinstimmen, bzw. die Kostenfunktion immer kleiner wird, gilt das Modell als konvergierend.
Also findet das Gradientenverfahren erfolgreich ein lokales Minimum.
\para{}
Jedoch kommt auch vor, dass ein Modell nicht konvergiert oder vielleicht
sogar divergiert. Bei Divergenz werden die Kosten im Verlaufe des Training immer
grösser.
Dies kann verschiedene Gründe haben. Einige davon können sein:
\begin{itemize}
\item{zu wenig Trainingsdaten}
\item{zu schwache Korrelation zwischen Inputs und Labels}
\item{falsche Hyperparameter}
\item{falsches Modell}
\end{itemize}


\subsection{Underfitting und Overfitting}\label{sec:overfitting}
Falls ein Modell konvergiert, heisst das noch nicht, dass es die
Gesetzmässigkeiten innerhalb der Trainingsdaten richtig erlernt hat.
Im Wesentlichen kann es zu zwei Problemen: Overfitting oder Underfitting.
\para{}
\keyword{Overfitting} bezeichnet das Phänomen, dass ein Modell zwar Vorhersagen
erzeugt, welche jedoch zu stark an die gegeben Trainingssamples angepasst sind.
Dies liegt zumeist daran, dass das Modell zu viele Parameter besitzt oder
aber der Trainingsdatensatz zu wenige Samples dafür beinhaltet.
Somit übersteigt die Komplexität des Modells gewissermassen jede der Aufgabenstellung.
\para{}
Um das Phänomen zu verdeutlichen, wird ein lineares Regressionmodell betrachtet.
Dieses besitzt eine Hypothesenfunktion $h = a_0 + a_1 x + a_2 x^2 + \ldots + a_n
x^n$, welche ein Polynom $n$-ter Ordnung ist. Falls der
Trainingsdatensatz nun aus $n$ oder weniger Samples besteht, kann das Modell die
Regressionkurve der Hypothesenfunktion exakt durch jeden Datenpunkt legen.
Dies entspricht dem Verhalten eines Modells mit $n$ Modellparametern, welches
mit $n$ Samples trainiert wird, und dabei exakt jedes Samples auswendig lernt anstatt
dessen Gesetzmässigkeiten zu erkennen. \\
Beim Auswendiglernen misst das Modell dem Datenrauschen zu viel Bedeutung zu, welches durch die natürliche Varianz
innerhalb der Trainingsdaten entsteht. Somit nutzt es
irrelevante Modellparameter, um das Rauschen zu kopieren. \\
Dadurch kann das Modell zwar sehr gute Vorhersagen zum Trainingsdatensatz
$\set{X}$ machen, jedoch würde es schlechte Vorhersagen für einen
anderen Testdatensatz $\set{T}$ liefern, welchen es nicht auswendig lernen konnte.
\para{}
Somit kann Overfitting folgendermassen definiert werden: Eine
Hypothesenfunktion $h$ overfittet dann, wenn eine alternative Hypothesenfunktion
$h'$ exisitert, für welche die Kosten bezüglich dem Trainingsdatensatz
grösser sind $\bar{C}_{h'}(\set{X}) > \bar{C}_h(\set{X})$, jedoch die Kosten
für Testdatensatz kleiner sind $\bar{C}_{h'}(\set{T}) < \bar{C}_h(\set{T})$.
\para{}
Das Gegenteil von Overfitting ist \keyword{Underfitting}. Dabei handelt es sich um das Phänomen, dass eine
Hypothesenfunktion $h$ zu
wenige Modellparameter $\param_k$ besitzt, um die Komplexität der Aufgabenstellung zu bewältigen.
Die Parameter reichen nicht aus, damit das Modell die Korrelation zwischen
Inputs und Labels begreifen kann.
\para{}
Bezogen auf das lineare Regressionmodell, bedeutet dies, dass der Grad $n$ der
polynomen Hypothesenfunktion $h$ zu gering ist, um sich an die Datenpunkte der
Sampels anzuschmiegen.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{overunderfitting.jpg}
  \caption{Visalierung von Under- und Overfitting}
\end{figure}

\cite{wiki:overfitting}

% ------------------------------------------------------------

\chapter{Deep Learning und Künstliche Neuronale Netze}
Nach dem im ersten Kapitel die Basis für ein fundiertes Verständnis von
Maschinellem Lernen gelegt worden ist, soll es nun im zweiten Kapitel darum
gehen, ein spezifisches Modell, namentlich Künstliche Neuronale Netze, zu
erläutern. Darauf aufbauend wird in Kapitel drei die spezifische Architektur
eines Künstlichen Neuronalen Netzes, namentlich das Convolutional Neural Network,
dargelegt.
\para{}
Die wohl besten Resultate für die meisten Problemstellungen des Maschinellen Lernens (Bilderkennung,
Spracherkennung, etc.) werden durch \keyword{Künstliche Neuronale Netze} (KNN) (engl.: Neural Network) geliefert.
Man bezeichnet diesen Bereich des Maschinellen Lernens auch als \keyword{Deep
  Learning}\footnote{WEITERFÜHRENDE QUELLEN BÜCHER}.
\para{}
Künstliche Neuronale Netze sind vor allem biologisch durch Nervensystemen von
Lebewesen inspiriert.
Sie sind aber lediglich eine Abstraktion dieser Informationverarbeitung und
versuchen nicht eine möglichst genaue biologische Abbildung zu kopieren.
Es gibt nicht nur eine Art von Neuronalem Netz, sondern es existieren die
verschiedensten Architekturen, welche je nach Problemstellung ausgewählt werden
müssen. Diese Arbeit wird vor allem von zwei solcher Architekturen Gebrauch machen:
Convolutional Neural Networks und sogenannte Autoencodern.

\para{}
\cite{wiki:kuenstliches_neuronales_netz}

\section{Perzeptron}
Um den Aufbau und die Funktion eines Künstlichen Neuronalen Netzes besser zu
verstehen, wird im folgenden ein Vorgänger des KNN erklärt: das \keyword{Perzeptron}.
\para{}
Das einlagige Perzeptron wurde erstmals 1958 von Frank Rosenblatt vorgestellt (QUELLE). Dieses
besteht aus einem einzigen Künstlichen Neuron. Dieses künstliche Neuron
hat mehrere binäre Inputs und einen einzigen binären Output. Binär
bedeutet, dass der Wert nur entweder 0 (\textit{aus}) oder 1 (\textit{ein}) sein
kann. Des weiteren besitzt es mehrere sogenannte \keyword{Gewichte} $w_1, \ldots,
w_m \in \set{R}$, für jeden Input $x_i$ ein Gewicht $w_i$.
Diese sind reelle Zahlen, welche das Verhalten des Perzeptron bestimmen.
Die \keyword{gewichtete Summe}, also die Summe aller Produkte der Inputs mit
ihrem Gewicht, wird mit $\tilde{z}$ bezeichnet.
Sie ist das gleiche wie das Skalarprodukt des Gewichtevektors
$\vec{w} = \trans{\begin{pmatrix} w_1 & \cdots & w_n \end{pmatrix}}$ mit dem
Inputvektor $\vec{x}$. \\
\begin{equation*}
  \tilde{z} = \sum_{i=1}^{m} w_i x_i = \vec{w} \cdot \vec{x}
\end{equation*} \\
Zusätzlich besitzt das Perzeptron einen \keyword{Schwellenwert} $\tilde{b}$.
Zusammen mit den Gewichten, bilden sie die Modellparameter.
Das Perzeptron verhält sich so, dass, falls die gewichtete Summe $\tilde{z}$ grösser als der
Schwellenwert $\tilde{b}$ ist, das Neuron feuert, d.h.\ der Output 1 beträgt.
Andernfalls ist er 0 (siehe erster Teil der Hypothesenfunktion $h$ in Gleichung (\ref{eq:perzeptron_1})).
Es ist gängig, die Ungleichung der Bedingung in die Nullstellenform zu bringen
und $\tilde{b}$ durch die \keyword{Neigung} (engl.: bias)
$b = -\tilde{b}$ zu ersetzten. Somit lautet die Ungleichung: $\tilde{z} + b
> 0$. Der neue Term $\tilde{z} + b$ wird mit $z$ bezeichnet (siehe Rest der Gl. (\ref{eq:perzeptron_1})).
Die Neigung gibt an, wie stark das Neuron dazu neigt zu feuern.\\

\begin{equation}\label{eq:perzeptron_1}
  h(\vec{x}) =
  \begin{cases}
    1 & \quad \text{falls } \tilde{z} > \tilde{b}\\
    0 & \quad \text{ansonsten}
  \end{cases}
  \quad =
  \begin{cases}
    1 & \quad \text{falls } \tilde{z} + b > 0\\
    0 & \quad \text{ansonsten}
  \end{cases}
  \quad =
  \begin{cases}
    1 & \quad\text{falls } \vec{w} \cdot \vec{x} + b > 0\\
    0 & \quad\text{ansonsten}
  \end{cases}
\end{equation}
\para{}
\begin{figure}[h!]
  \centering
  \begin{tikzpicture}[>=latex]
    \path (3,0) node [circle,draw](neuron){Neuron};
    \path[red] (0,1.5) node(x1){$x_1$} (0,0) node(x2){$x_2$} (0,-1.5) node(x3){$x_3$};
    \path[black!40!green] (5,0) node(y1){$y$};
    \draw[->] (x1) -- node[above,sloped]{$w_1$} (neuron);
    \draw[->] (x2) -- node[above,sloped]{$w_2$} (neuron);
    \draw[->] (x3) -- node[above,sloped]{$w_3$} (neuron);
    \draw[->] (neuron) -- (y1);
  \end{tikzpicture}
  \caption{Perzeptron mit drei Inputs}
  \label{fi:perzeptron}
\end{figure}
\para{}
Für das Trainieren des Perzeptrons existieren spezielle Verfahren, welche hier
aber nicht weiter beleuchtet werden sollen. Dies aus dem Grund, weil das
Gradientenverfahren hier nicht verwendet werden kann.
Der Grund dafür soll später in Sektion (\ref{sec:künstlicheNeuronen}) erläutert werden.
\para{}
\cite{wiki:perzeptron}
\cite{Nielsen}

\subsection{Lernpotenzial eines Perzeptrons}
Nun stellt sich die Frage, was ein Perzeptron eigentlich erlernen kann und wofür
es nutzbar ist.
Das Perzeptron ist lediglich ein \keyword{linearer Klassifikator} der Form
$y = w_1x_1 + \cdots + w_m x_m$. Es ist also ein Klassifizierungsmodell und kein Regressionsmodell.
Es kann die Features in zwei Klassen 0 oder 1 einordnen, wobei der Output der
Hypothesenfunktion diese Klassifizierung angibt.
Überschreitet $y$ den Schwellenwert $\tilde{b}$, werden die Features der Klasse 1 zugeordnet, sonst
der Klasse 0.
Jedoch müssen diese Klassen linear separierbar sein.
\para{}
Lineare Separierbarkeit bedeutet, dass alle Featurevektoren $\vec{x}_1,\ldots,\vec{x}_p \in \set{R}^m$
innerhalb ihres Vektorraums $\set{R}^m$ durch eine Hyperebene in ihre Klassen aufteilbar sein müssen.
Falls das Perzeptron zwei Inputs besitzt, bedeutet dies, dass die Ortsvektoren
durch eine Gerade voneinander trennenbar sein müssen (siehe Abb.
(\ref{fig:linearer_Klassifikator})). \\
Falls die Features nicht linear separierbar sind,
kann das Perzeptron die Klassifizierung nicht erlernen.
BEISPIEL
\\
\begin{figure}[h!]
  \caption{erfolgreiche lineare Separierung (links) und das Versagen bei XOR (rechts)}
  \label{fig:linearer_Klassifikator}
\end{figure}
\para{}
\cite{wiki:perzeptron}
\cite{wiki:linear_separability}

\section{Erweiterung der künstlichen Neuronen}\label{sec:künstlicheNeuronen}
Ein Perzeptron ist, wie vorhin erklärt, nur in der Lage, lineare Klassifikationen
durchzuführen. Um nun auch Regressionsprobleme zu lösen, muss das Konzept
ausgebaut werden. Ausserdem brauchen wir ein Künstliches Neuron benötigt, welches sich
besonders gut als Baustein für KNNs eignet.


\subsection{Künstliche Neuronen im Allgemeinen}
Künstliche Neuronen sind immer so aufgebaut, dass sie einen oder mehrere Inputs
und nur einen einzigen Output besitzen. Zu jedem Input $x_i$ ist ein Gewicht
$w_{i}$ assoziert. Zuerst wird die gewichtete Summe der Inputs $\tilde{z}$ gebildet.
Die Neigung $b$ wird ebenfalls dazu addiert, um $z$ zu erhalten. Nun muss
die sogenannte \keyword{Aktivierung} $a$ gebildet werden. Sie ist der Output des Neurons.
Die Aktivierung $a = \varphi(z)$ ist das Resultat der
\keyword{Aktivierungsfunktion} $\varphi: \set{R} \to \set{R}$ angewendet
auf $z$. Die verschiedenen künstlichen Neuronen unterscheiden
sich fast nur in ihrer Aktivierungsfunktion.
\\
\begin{figure}[h!]

  \caption{ein künstliches Neuron und seine Bestandteile}
\end{figure}
\\

\para{}
\cite{Nielsen}
\cite{wiki:kuenstliches_neuron}

\subsection{Perzeptronen als künstliche Neuronen}
Nun nochmal ein Blick auf das Perzeptron im Angesicht der Aktivierungsfunktion.
Ein wesentlicher Unterschied des Perzeptrons gegenüber sonstigen künstlichen
Neuronen, besteht darin, dass seine Inputs und Outputs nur binäre Werte
annehmen können. Um dieses Verhalten des Perzeptrons zu erhalten,
muss eine Stufenfunktion als Aktivierungfunktion verwendet werden: die Heaviside-Funktion $\Theta$.
Sie hat einen einzigen Stufensprung bei $x=0$ vom Wert 0 auf 1 (siehe Abb. (\ref{fig:heaviside})).
\\
\begin{figure}[h!]
  \begin{minipage}[h!]{0.5\textwidth}
    \begin{equation*}
      \varphi^{\text{hlim}}(z) = \Theta(z) =
      \begin{cases}
        1 & \quad \text{falls } z \geq 0\\
        0 & \quad \text{falls } z < 0
      \end{cases}
    \end{equation*}
  \end{minipage}
  \begin{minipage}[h!]{0.5\textwidth}
    \centering
    \begin{tikzpicture}[scale=2.5]
      \draw[->] (-1.5,0) -- (1.5,0) node[right] {$x$}; % x-axes
      \draw[->] (0,-0.2) -- (0,1.2) node [above] {$y$}; % y-axes
      \draw[style=help lines,step=0.5] (-1.4,0) grid (1.4, 1.1);

      \foreach \x in {-1,-0.5,0.5,1}
      \draw[shift={(\x,0)}] (0pt,2pt) -- (0pt,-2pt) node[below,fill=pagecolor] {$\x$};

      \foreach \y in {0.5,1}
      \draw[shift={(0,\y)}] (2pt,0pt) -- (-2pt,0pt) node[left,fill=pagecolor] {$\y$};

      \draw[shift={(0,0)}] (0pt,0pt) node[below left,fill=pagecolor] {$O$};

      \draw[red,ultra thick] (-1.5,0) -- (0,0); % 0-red
      \draw[red,ultra thick] (0,1) -- (1.5,1); % 1-red
      \draw[red,ultra thick,dashed] (0,0) -- (0,1); % y-red
      \draw[draw=red,fill=white] (0,0) circle (0.05);
      \draw[draw=red,fill=red] (0,1) circle (0.05);
    \end{tikzpicture}
  \end{minipage}
  \caption{Definition und Graph der Heaviside-Funktion $\Theta$}
  \label{fig:heaviside}
\end{figure}

\cite{wiki:kuenstliches_neuron}
\cite{wiki:perzeptron}


\subsection{ReLU Neuronen}\label{sec:ReLU}
Der nächste Schritt nach einer Stufenfunktion als Aktivierungsfunktion, sind
lineare Aktivierungsfunktionen. Für diese können die Inputs nun beliebige reelle
Zahlen sein.
Jedoch sind solche lineare Neuronen in einem KNN von keinerlei Nutzen.
Dies ist dadurch begründet, dass eine Verkettung von linearen Neuronen
immer auf eine einzige lineare Funktion reduziert werden kann. Somit hat
die Verkettung keinen Mehrwert.
\para{}
Stattdessen verwendet man sogenannte ReLU Neuronen. Sie benutzen die
\keyword{Rectified Linear Unit} (\keyword{ReLU}) als Aktivierungsfunktion.
Diese ist eine
nur teilweise lineare Aktiverungsfunktion. Die Werte grösser als 0 werden
auf sich selbst linear abbgebildet und die Werte kleiner als 0 werden auf 0
abbgebildet (siehe Abb. (\ref{fig:relu})).
Eine sehr wichtige Eigenschaft der ReLU-Funktion ist, dass sie - im Gegensatz zu den vorhin
Heaviside-Funktion $\Theta$ - fast überall differenzierbar%
\footnote{%
  Eigentlich ist die ReLU-Funktion in $x=0$ wegen des Knicks nicht
  differenzierbar. Für die Gradientenberechnung definiert man jedoch einfach
  die Ableitung $\varphi'^{\text{ReLU}}(0) \coloneqq 0$. Dies ist mathematisch zwar nicht
  korrekt, löst aber das Problem.
}%
und strikt monoton
steigend ist. Erst für diese Aktivierungsfunktion kann das Gradientenverfahren
angewendet und somit das KNN trainiert werden.
\para{}
Da die ReLU-Funktion nur teilweise linear ist, gehört sie genaugenommen den
nicht-linearen Aktiverungsfunktionen an. Diese Nicht-Linearität erlaubt es dem
Neuron, deutlich komplexere Systeme zu modellieren bzw. deutlich komplexere
Probleme zu lösen. In Sektion (\ref{sec:UAT}) wird dargelegt, dass
eine Kompostion von nicht-linearen Neuronen jede beliebige Funktion approximieren kann.
\para{}
Das ReLU-Neuron werden wir an dieser Stelle zur Seite legen und erst wieder in
Kapitel (\ref{sec:CNN}) im Zusammenhang mit KNNs zur Bilderkennung wieder betrachten.
\para{}
\begin{figure}[h!]
  \begin{minipage}[h!]{0.5\textwidth}
    \begin{equation}
      \varphi^{\text{ReLU}}(z) =
      \begin{cases}
        z & \quad \text{falls } z > 0\\
        0 & \quad \text{falls } z \leq 0
      \end{cases}
      = \max(z,0)
    \end{equation}
  \end{minipage}
  \begin{minipage}[h!]{0.5\textwidth}
    \centering
    \begin{tikzpicture}[scale=2.5]
      \draw[->] (-1.5,0) -- (1.5,0) node[right] {$x$}; % x-axes
      \draw[->] (0,-0.2) -- (0,1.2) node [above] {$y$}; % y-axes
      \draw[style=help lines,step=0.5] (-1.4,0) grid (1.4, 1.1);

      \foreach \x in {-1,-0.5,0.5,1}
      \draw[shift={(\x,0)}] (0pt,2pt) -- (0pt,-2pt) node[below,fill=pagecolor] {$\x$};

      \foreach \y in {0.5,1}
      \draw[shift={(0,\y)}] (2pt,0pt) -- (-2pt,0pt) node[left,fill=pagecolor] {$\y$};

      \draw[shift={(0,0)}] (0pt,0pt) node[below left,fill=pagecolor] {$O$};

      \draw[red,ultra thick] (-1.5,0) -- (0,0);
      \draw[red,ultra thick] (0,0) -- (1,1);
      \draw[red,ultra thick,dashed] (1,1) -- (1.2,1.2);

      \draw[draw=red,fill=red] (0,0) circle (0.03);
    \end{tikzpicture}
  \end{minipage}
  \caption{Formel und Graph der ReLU-Funktion}
  \label{fig:relu}
\end{figure}

\para{}
\cite{wiki:kuenstliches_neuron}
\cite{Nielsen}

\subsection{Sigmoid Neuronen}
Ein weiteres nicht-lineares Neuron sind sogenannte Sigmoid-Neuronen.
Den Namen haben sie von ihrer Aktivierungsfunktion: der Sigmoidfunktion $\sigma$.
\para{}
Auch sie können in einem KNN aufgrund ihrer Nicht-Linearität zur Approximation jeder Funktion
verwendet werden. Die Sigmoidfunktion gilt als meist verwendete
Aktivierungsfunktion in KNNs. Dies ist damit begründet, dass sie sehr stark von
der Linearität abweicht und auf diese Weise am schnellsten komplexe Sachverhalte
modellieren kann. (QUELLE)
\para{}
Die Sigmoid-Funktion besitzt eine einzige Wendestelle $\sigma''(x=0)=0$ und hat
zwei Asymptoten, eine $\ds\lim_{x \to -\infty} \sigma(x)=0$
und eine zweite $\ds\lim_{x \to \infty} \sigma(x)=1$ (siehe Abb.
(\ref{fig:sigmoid})). Desweiteren zeichnet sie sich durch eine vergleichsweise
einfache Ableitung aus.
\\
\begin{figure}[h!]
  \begin{minipage}[h!]{0.5\textwidth}
    \begin{align*}
      \varphi^{\text{sig}}(z) &= \sigma(z) = \frac{1}{1 + e^{-z}}\\
      \sigma'(z)&=\sigma(z)(1-\sigma(z))
    \end{align*}
  \end{minipage}
  \begin{minipage}[h!]{0.5\textwidth}
    \centering
    \begin{tikzpicture}[scale=2.5]
      \draw[->] (-1.5,0) -- (1.5,0) node[right] {$x$}; % x-axes
      \draw[->] (0,-0.2) -- (0,1.2) node [above] {$y$}; % y-axes
      \draw[style=help lines,ystep=0.5,xstep=0.25] (-1.4,0) grid (1.4, 1.1);

      \foreach \x/\xtext in {-1/-4,-0.5/-2,0.5/2,1/4}
      \draw[shift={(\x,0)}] (0pt,2pt) -- (0pt,-2pt) node[below,fill=pagecolor] {$\xtext$};

      \foreach \y in {0.5,1}
      \draw[shift={(0,\y)}] (2pt,0pt) -- (-2pt,0pt) node[left,fill=pagecolor] {$\y$};

      \draw[shift={(0,0)}] (0pt,0pt) node[below left,fill=pagecolor] {$O$};

      \draw[red,ultra thick,x=0.25cm] plot[domain=-6.0:6.0] (\x,{1/(1+exp(-\x)) });
    \end{tikzpicture}
  \end{minipage}
  \caption{Definition, Ableitung und Graph der Sigmoid-Funktion $\sigma$}
  \label{fig:sigmoid}
\end{figure}

\para{}
\cite{wiki:kuenstliches_neuron}
\cite{wiki:sigmoidfunktion}


\section{Topologie der Künstlichen Neuronalen Netze}
Nun sollen die Sigmoiden-Neuronen als Bausteine für Künstliche
Neuronale Netze Verwendung finden. Dazu werden sie miteinander verbunden und bilden so ein Netz,
ähnlich wie ein Nervensystem.
\para{}
Diese Neuronen sind in verschiedenen Schichten (engl.: Layers)
arrangiert. Die erste ist die \keyword{Inputschicht}. Sie beinhaltet die
Inputneuronen. Diese sind eigentlich keine richtigen
Neuronen, sondern eher Platzhalter für ihr jeweiliges Feature $x_i$. Als letztes kommt die
\keyword{Outputschicht} mit den Outputneuronen, welche jeweils einen Outputwert $y_i$
besitzen. Dazwischen liegen die \keyword{Zwischenschichten} (engl.: hiddenlayers). Von ihnen kann es
beliebig viele geben, und in ihnen können beliebig viele Neuronen liegen.
Falls viele Zwischenschichten verwendet werden, bezeichnet man das Netzwerk als
``deep''. Daher rührt auch der Begriff des Deep Learnings.
Den Aufbau eines KNN bezeichnet man als \keyword{Topologie} des Netzes. Die
Topologie umfasst viele Hyperparameter. Darunter sind zum Beispiel die Anzahl
der Zwischenschichten, wie auch
die Anzahl der Neuronen pro Schicht.
\para{}
Jedes Neuron aus einer Schicht ist mit jedem Neuron aus der nächsten Schicht über
Verbindungen gekoppelt. Alle Verbindungen besitzten ein Gewicht analog zu den Inputs des
Perzeptrons. Die Aktivierung, also der Output, eines Neurons wandert entlang den jeweiligen
Verbindungen zu allen Neuronen der nächsten Schicht und dienen als deren Input.
\para{}
In Abbildung (\ref{fig:nn_layers}) ist ein Beispiel eines Neuronalen Netzes
abgebildet. In diesem Fall besitzt es sowohl 4 Inputs, als auch 4 Outputs. Es hat
ausserdem 3 Zwischenschichten. Die erste und die dritte haben jeweils 3 Neuronen
und die zweite besitzt 4 Neuronen. \\

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{knn1.pdf}
  \caption{Schichten eines KNNs}
  \label{fig:nn_layers}
\end{figure}

\cite{wiki:kuenstliches_neuronales_netz}
\cite{Nielsen}

\section{Lernverhalten}
Die Hoffnung beim Trainieren von KNNs besteht darin, dass das Modell für jede
weitere Schicht eine höheres Abstraktionsniveau erreicht. Würde man zum
Beispiel ein Netzwerk zur Gesichtserkennung trainieren, könnte man sich den
Erkennungsprozess folgendermassen vorstellen: Die erste Zwischenschicht erkennt
Kanten und Konturen. Die zweite vereint diese Merkmale zu Ecken und primitiven
geometrischen Formen. Die dritte Schichte sollte dann schon komplexere
geometrische Formen erkennen, welche gewissen Gesichtmerkmalen, wie der Nase,
ähneln.
Die letzten Schichten soll dann alle diese
Merkmale zusammensetzen und so ein Gesicht als Ganzes erkennen.

\section{Vorwärtspropagierung}
Jetzt, da der Aufbau eines KNNs erläutert wurde, soll nun die mathematische Funktionsweise
des Modells erklärt werden. Der Prozess der Berechnung der Outputwerte wird
\keyword{Vorwärtspropagierung} genannt. Dieses Verfahren gibt dieser Art von KNN auch den
Namen: Feedforward Neural Network. Für das Verständnis müssen einige Konventionen zur
Bezeichnung der Teile eines KNNs getroffen werden. Es sollten vor allem noch
Abbildungen (\ref{fig:nomenklatur1}) und (\ref{fig:nomenklatur2}) zum
Verständnis der Nomenklatur studiert werden.
\begin{itemize}
\item{$l$ ist der Index einer Schicht. Die Indexierung beginnt bei 0.}
\item{$L$ ist der letzte Schichtindex und somit auch die gesamte Anzahl an
    Schichten (ohne die Inputschicht).}
\item{$|l|$ ist die Anzahl Neuronen in der $l$-ten Schicht.
    \footnote{
      Diese Schreibweise hat nichts mit dem Betrag zu tun, sondern wird
      gewählt, da sie sehr platzsparend ist.
    }
  }
\item{$n_j^l$ bezeichnet das $j$-te Neuron in der $l$-ten Schicht.}
\item{$z_j^l$ ist die gewichtete Summe der Inputs des $j$-ten Neuron in der $l$-ten Schicht.}
\item{$a_j^l$ ist die Aktivierung/Output des $j$-ten Neurons in der $l$-ten Schicht.}
\item{$b_j^l$ ist die Neigung für das $j$-te Neuron in der ($l+1$)-ten Schicht.
    \footnote{
      Diese Konvention wurde gewählt, damit die folgenden Gleichungen simpler sind.
    }
  }
\item{$w_{j,k}^l$ ist das Gewicht der Verbindung vom $k$-ten Neuron
    in der $l$-ten Schicht zum $j$-ten Neuron in der ($l+1$)-ten Schicht.
    \footnote{
      Man beachte die Reihenfolge!\\
      Diese Konvention scheint zwar auf den ersten Blick unintuitiv, macht jedoch
      Sinn für die Matrixindezierung.
      (\ref{sec:backpropagation}).
    }
  }
\item{$\varphi$ ist die gewählte Aktivierungsfunktion (diese ist immer eine
    nicht-lineare Aktivierungsfunktion)}
\end{itemize}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{knn2.pdf}
  \caption{zum Verständnis der Nomenklatur}
  \label{fig:nomenklatur1}
\end{figure}
\para{}
\begin{figure}[h!]
  \centering
  \begin{tikzpicture}[>=latex]
    \path (-2,1.5) node [draw,circle,inner sep=0,minimum size=1.25cm](n11){$n^1_1$};
    \path (-2,-1.5) node [draw,circle,inner sep=0,minimum size=1.25cm](n12){$n^1_2$};
    \path (2,1.5) node [draw,circle,inner sep=0,minimum size=1.25cm](n21){$n^2_1$};
    \path (2,-1.5) node [draw,circle,inner sep=0,minimum size=1.25cm](n22){$n^2_2$};
    \draw[->] (n11) -- node[above,sloped]{$w^1_{1,1}$} (n21);
    \draw[->] (n11) -- node[above,pos=0.75,sloped]{$w^1_{2,1}$} (n22);
    \draw[->] (n12) -- node[above,pos=0.75,sloped]{$w^1_{1,2}$} (n21);
    \draw[->] (n12) -- node[above,sloped]{$w^1_{2,2}$} (n22);
  \end{tikzpicture}
  \caption{zum Verständnis der Gewichtebeschriftungen}
  \label{fig:nomenklatur2}
\end{figure}
\para{}
Die Vorwärtspropagierung beginnt bei den Inputneuronen, welche jeweils
einen Inputwert in sich tragen. Diese Werte werden, um für eine kohärente Nomenklatur zu sorgen,
analog zu den Aktivierungen der anderen Neuronen mit $a_j^0$ bezeichnet, wobei
$j$ der Index des Neurons ist. \\
Nun müssen die restlichen Aktivierungen der Neuronen bis und mit den Ouputneuronen berechnet werden. Dies geschieht rekursiv anhand der
Aktivierungen der voherigen Schicht und zwar folgendermassen (ersichtlich in
Gleichung (\ref{eq:gewichtete_summe_normal})).
\para{}
Zuerst läuft eine Summe über alle Neuronen $n_k^{l}$ der jetztigen Schicht
$l$. Dabei wird die gewichtete Summe der Aktivierungen $a_k^{l}$ mit den
assozierten Gewichten $w_{j,k}^l$ gebildet. Hierbei ist das Gewicht jenes, welches das
$k$-te Neuron der $l$-ten Schicht mit dem $j$-ten Neuron der ($l+1$)-ten Schicht
verbindet (siehe Abb. (\ref{fig:nomenklatur2})).
Zusätzlich gehört zu der gewichteten Summe auch die jeweilige Neigung $b_j^l$, welche
dazu addiert wird. Diese gewichtete Summe wird mit $z_j^{l+1}$ bezeichnet.
\\
\begin{equation}\tag{FP1}\label{eq:gewichtete_summe_normal}
  z_j^{l+1} = \sum_{k=1}^{|l|} w_{j,k}^l a_k^l + b_j^l
\end{equation}
\\
Auf diese Summe wird dann die Aktivierungsfunktion $\varphi$ angewandt.
Das ist dann die Aktivierung $a_j^{l+1}$ des $j$-ten Neurons in der ($l+1$)-ten Schicht.
\\
\begin{equation}\tag{FP2}\label{eq:aktivierung_normal}
  a_j^{l+1} = \varphi\left(\sum_{k=1}^{|l|} w_{j,k}^l a_k^{l} + b_j^l \right) = \varphi \left( z_j^{l+1} \right)
\end{equation}
\par\bigskip
Für Deep Learning braucht man vor allem sogenannte Deep Neural Networks. Diese
zeichnen sich dadurch aus, dass sie sehr viele Zwischenschichten besitzen.
Deshalb bezeichnet man sie als ``deep''.
Bei solchen Netzwerken ist es nicht unüblich,
dass sie sehr viele Neuronen und Verbindungen (über 100'000) besitzen.
Um hier nicht den Überblick zu verlieren bzw. damit nicht zu viele Indizes notwendig
sind, macht man Gebrauch von \keyword{Linearer Algebra}. Man verwendet
Matrizen und Vektoren, um die vielen Variablen zusammenzufassen.
Ausserdem besteht ein weiterer Vorteil darin, dass Computer mithilfe von Vektor-
und Matrixoperationen die Berechnungen parallelisieren können und in kürzerer
Zeit und mit weniger Ressourcen viele Berechnungen gleichzeitig ausführen können.
Dies beschleunigt das Training der Modelle um
ein Vielfaches. Später in Sektion (\ref{sec:tensorflow}) wird dies nochmals thematisiert.
\para{}
Die Inputs $\vec{x}$, Vorhersagen $\vec{y}$ und Labels $\vec{\hat{y}}$ wurden
bereits zu Beginn als Vektoren geschrieben.
Nun sollen noch die Modellparameter und die restlichen Komponenten eines KNNs als Vektoren und Matrizen zusammengefasst werden.
Sowohl alle gewichteten Summen $z_j^l$, wie auch alle Aktivierungen $a_j^l$
einer Schicht $l$, werden in Vektoren $\vec{z}^l \in \set{R}^{|l|}$ und
$\vec{a}^l \in \set{R}^{|l|}$ zusammengefasst.
Auch alle Neigungen $b_j^l$ für eine Schicht ($l+1$) bilden einen Vektor
$\vec{b}^l \in \set{R}^{|l+1|}$.
\para{}
Zu guter letzt, wird noch eine \keyword{Gewichtsmatrix} $\mat{W}^l \in
\set{R}^{|l+1| \times |l|}$
definiert. Sie enthält alle Gewichte, welche die $l$-te
Schicht \textit{zu} der ($l+1$)-ten Schicht verbindet.
Das heisst, der Eintrag in der $j$-ten Zeile und in
der $k$-ten Spalte ist $w_{j,k}^l$ und verbindet so das Neuron $n_k^{l}$ zu
dem Neuron $n_j^{l+1}$.
\\
\begin{align*}
  \vec{z}^l &=  \trans{\begin{pmatrix} z_1^l & z_2^l & \cdots & z_{|l|}^l \end{pmatrix}} \\
  \vec{a}^l &=  \trans{\begin{pmatrix} a_1^l & a_2^l & \cdots & a_{|l|}^l \end{pmatrix}} \\
  \vec{b}^l &=  \trans{\begin{pmatrix} b_1^l & b_2^l & \cdots & b_{|l+1|}^l \end{pmatrix}} \\
\end{align*}
\begin{equation*}
  \mat{W}^l =
  \begin{pmatrix}
    w_{1,1}^l & w_{1,2}^l & \cdots & w_{1,|l|}^l \\[0.3em]
    w_{2,1}^l & w_{2,2}^l & \cdots & w_{2,|l|}^l \\[0.3em]
    \vdots & \vdots & \ddots & \vdots \\[0.3em]
    w_{|l+1|,1}^l & w_{|l+1|,2}^l & \cdots & w_{|l+1|,|l|}^l
  \end{pmatrix}
\end{equation*}
\\
Mit diesen Definitionen kann Gleichung (\ref{eq:gewichtete_summe_normal}) in
Matrixform geschrieben werden. Dies, weil die Matrixmultiplikation von $\mat{W}^l$ mit
$\vec{a}^{l}$ einen Vektor $\vec{\tilde{z}}^{l+1}$ ergibt, welcher alle gewichteten
Summen $\tilde{z}_j^{l+1}$ ohne die jeweilige Neigung enthält.
\\
\begin{equation*}
  \mat{W}^l \vec{a}^{l} = \trans{\begin{pmatrix}\ds \sum_{j=1}^{|l|} w_{1,j}^l a_j^l &\ds \sum_{j=1}^{|l|} w_{2,j}^l a_j^l & \cdots &\ds \sum_{j=1}^{|l|} w_{|l+1|,j} a_j^l \end{pmatrix}} = \vec{\tilde{z}}^{l+1}
\end{equation*}
\\
Nun muss noch der Neigungsvektor $\vec{b}^l$ dazu addiert werden, damit die
Gleichung (\ref{eq:gewichtete_summe_matrix}) ensteht, mit welcher der
Vektor der gewichteten Summen $\vec{z}^{l+1}$ gebildet werden kann.
\\
\begin{equation}\tag{FP1a}\label{eq:gewichtete_summe_matrix}
  \vec{z}^{l+1} = \mathbf{W}^{l} \vec{a}^{l} + \vec{b}^{l}
\end{equation}
\\
Im letzten Schritt wird die Aktivierungsfunktion auf $\vec{z}^{l+1}$
angewendet, um den Aktivierungsvektor $\vec{a}^{l+1}$ zu bilden.
Hierfür muss aber noch ein neues mathematisches
Konzept eingeführt werden: die Vektorisierung einer Funktion.
\para{}

\begin{defbox}{Vektorisierung einer Funktion}
  Die Vektorisierung einer skalaren Funktion $f$, geschrieben als
  $\vecf{f}[\vec{v}]$ hat als Argument einen Vektor $\vec{v}$, auf dessen
  Komponenten jeweils \textit{einzeln} die Funktion $f$ angewendet wird. Dieser neue
  Vektor ist der Rückgabewert der Funktion. Er besitzt die gleichen Dimensionen
  wie der Argumentvektor.
  \\
  \begin{equation*}
    \vecf{f}[\vec{v}]=
    \begin{pmatrix}
      f(v_1)\\
      \vdots \\
      f(v_n)\\
    \end{pmatrix}
  \end{equation*}
\end{defbox}
\para{}
Nun kann die vektorisierte Aktivierungsfunktion $\vecf{\varphi}$ auf
$\vec{z}^{l+1}$ angewendet werden.

\begin{equation}\tag{FP2a}\label{eq:aktivierung_matrix}
  \vec{a}^{l+1} = \vecf{\varphi} \left[\mat{W}^{l} \vec{a}^{l} + \vec{b}^{l} \right] = \vecf{\varphi} \left[ \vec{z}^{l+1} \right]
\end{equation}

\para{}
\cite{Nielsen}

\subsection{Initialisierung der Modellparameter}\label{sec:parameter_initalisieren}
Ein weiterer Schritt, der vollzogen werden muss, bevor das Training beginnen kann, ist das
Initialisieren aller Modellparameter, in diesem Fall die Gewichte und Neigungen.
Dies ist ein sehr essentieller Schritt, da diese Initialwerte die
Leistungsfähigkeit des Modells erheblich beinflussen.
\para{}
Wie in Sektion (\ref{sec:gradientenverfahren}) gezeigt, muss am Anfang des
Gradientenverfahrens ein Startpunkt $\vec{\param}_{t=0} = \trans{\begin{pmatrix}
    \param_1 & \cdots & \param_k \end{pmatrix}}$ innerhalb des Gradientenfeldes
$\vecf{\nabla}C$ gewählt werden, von welchem aus der Gradientenabstieg beginnt.
Dieser Startpunkt entscheidet darüber, in welches lokale Minimum konvergiert
wird und bestimmt somit auch die bestmögliche Exaktheit der Vorhersagen. Falls
schlechte Initialwerte gewählt wurden, konvergiert der Punkt in ein hohes lokales
Minimum, was grosse Kostenfunktionswerte und schlechte Vorhersagen verursacht.
\para{}
Es ist nicht möglich im Vorhinein zu wissen, welche Initialwerte gute Resultate
liefern. Es müssen verschiedene Werte ausprobiert werden. Dafür initialisert man gängierweise die
Hyperparameter mit Zufallswerten. Zu diesem Zweck werden nicht irgendwelche
Zufallsvariablen verwendet, sondern es gelangt die Gauss'sche Normalverteilung
$\mathcal{N}(\mu,\sigma^2)$ bzw. ihre Dichtefunktion zur Anwendung.
\[\ds \phi(x\ |\ \mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \text{exp} \left\{-\frac{{(x-\mu)}^2}{2\sigma^2}\right\} \]
\para{}
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{gauss.pdf}
  \caption{Graph der Dichtefunktion $\phi(x\ |\ \mu=0,\sigma^2=1)$ mit ihren
    wichtigsten Eigenschaften}%
\end{figure}
In den Anfängen des Maschinellen Lernens wurde häufig die normierte
Normalverteilung verwendet. Jedoch hatte dies zur Konsequenz, dass mit jeder Schicht die
Outputs eine stetig grösser werdende Standardabweichung $\sigma$ aufwiesen.
In der Konsequenz verlansamte sich der Lernprozess ohne nahe genug am lokalen
Minimum zu sein. Dies wird auch als Saturierung der Neuronen bezeichnet (FUSSNOTE?).
\para{}
In der Konsequenz wurde eine neue Technik entwickelt: die Glorot-Initalisierung
(auch Xavier-Initalisierung genannt). Dabei gelangt erneut eine Normalverteilung
mit Erwartungswert $\mu = 0$ zur Anwendung. Die Standardabweichung $\sigma$ wird
anhand der Grösse einer Schicht skaliert.
Für eine Schicht $l$ wird der Durchschnitt zwischen der Anzahl Inputs
und der Anzahl Neuronen einer Schicht berechnet $r = \frac{|l-1| + |l|}{2}$. Der Kehrwert
davon ist die Varianz der Normalverteilung.
Die Gewichte werden folgendermassen initialisert:
\begin{equation}
  w_{t=0}^l \sim \mathcal{N}\left(\mu = 0, \sigma^2 = \frac{2}{|l-1| + |l|}\right)
\end{equation}
Durch dieses Verfahren bleibt die Varianz innerhalb einer Schicht erhalten.
Teilweise wird diese Verfahren auch für die Neigungen $b$ angewendet. Eine
andere Möglichkeit besteht in der Initalisierung der Neigungen mit 0, wie Keras
das etwa auch macht.
\begin{equation}
  b_{t=0}^l = 0
\end{equation}

\para{}
\cite{wiki:normal_distribution}
\cite{Nielsen}
\cite{book:hands-on}

\section{Rückwärtspropagierung}\label{sec:backpropagation}
Ein KNN wird üblicherweise, wie die meisten Modelle, mithilfe des Stochastischen
Gradientenverfahrens trainiert.
Die wahre Herausforderung besteht darin, die partiellen Ableitungen der
Kostenfunktion bezüglich der Modellparameter zu berechnen.
Anders gesagt müssen alle Terme
$\ds\partderiv{C}{w_{j,k}^l}$, wie auch alle Terme $\ds\partderiv{C}{b_k^l}$,
bestimmt werden.
Das Verfahren zur Ermittlung dieser Ausdrücke ist so spezifisch und aufwendig,
dass das SGD für KNNs einen eigenen Namen besitzt: die sogennante
\keyword{Rückwärtspropagierung} (engl.: backpropagation, auch
Fehlerrückführung). \\
Für das Verständnis von ML ist es nicht essentiell, die Rückwärtspropagierung zu
verstehen. Jedoch kann so nachvollzogen werden, wie SGD in der Praxis funktioniert.
\para{}
Grob umrissen besteht der Grundgedanke darin, die Werte der Kostenfunktion
rückwärts durch das Modell zu schicken und dabei die partiellen Ableitungen
mithilfe der Kettenregel zu bestimmen. Daher hat das Verfahren auch seinen Namen.
\para{}
Für eine ausführliche Herleitung ist auf Anhang
(\ref{sec:anhang_bp}) zu verweisen, in welchem auf das Konzept eines
Computational Graphs eingegangen wird.

\section{Universal Approximation Theorem}\label{sec:UAT}
Es stellt sich nun die Frage, was ein Neuronales Netz alles erlernen kann.
Diese Frage kann mithilfe des \keyword{Universal Approximation Theorem} (UAT)
beantwortet werden. Es handelt sich um einen mathematischer Beweis dafür, dass ein KNN
grundsätzlich in der Lage, ist jede kontinuierliche Funktion beliebig exakt zu
approximieren.
\para{}
Etwas genauer ausgedrückt, besagt der UAT, dass ein KNN mit einer einzigen
Zwischenschicht, welche eine endliche Anzahl Neuron besitzt, sich jeder kontinuierlichen
Funktion annähern kann. Voraussetzung dafür ist, dass es sich bei den
Neuronen um nicht-lineare Neuronen handelt.
Da sich die meisten Klassen von Problemen als eine Funktion formulieren
lassen, bedeutet dies, dass ein KNN theoretisch jedes Problem lösen kann!
Beim UAT handelt es sich aber nur um eine theoretische
Aussage über das Lernpotenzial eines KNNs. Jedoch trifft das Theorem keinerlei Aussage
darüber, ob ein KNN wirklich erfolgreich darin ist, die jeweilige Funktion zu erlernen.
Ein mögliches Hindernis könnte in Overfitting bestehen.
\para{}
Da der eigentliche Beweis mathematisch ziemlich anspruchsvoll ist, wird er im
Rahmen dieser Arbeit nicht weiter behandelt.
\para{}
\cite{Nielsen}
\cite{wiki:uat}


\pagebreak
\chapter{Convolutional Neural Networks}\label{sec:CNN}
Nachdem in Kapitel zwei das Wesen von Künstlichen Neuronalen Netzen
charakterisiert wurde, soll es nun in Kapitel drei darum gehen, eine spezfische
Architektur einen KNNs, welche sich besonders gut für Bilderverarbeitung eignet,
zu erklären. Es handelt sich um das Convolutional Neural Network.
\para{}
Viele Anwendungen von Machine Learning sind mit einer Bild- oder
Audioverarbeitung verbunden, wie z.B Bildklassifizierung, Gesichts- oder
Spracherkennung.
Vor allem für hochauflösende Bilder sind die KNNs, wie wir sie soeben
kennengelernt haben, jedoch nicht geeignet. Sie sind zum Teil gar nicht in der
Lage, eine Korrelation zwischen den Inputs und Outputs zu erlernen.
Um diesen Umstand zu erklären, wird ein kleines Beispielmodell erläutert:
\para{}
\label{sec:CNN_parameter_problem}
Es soll ein KNN entworfen werden, welches eine Photographie danach klassifizieren
soll, ob ein Hund darauf sichtbar ist oder nicht. Für dieses
Gedankenexperiment wird ein relativ niedrig aufgelöstes Bild mit $256 \times 256$
Pixel gewählt (dies entspricht weniger als $0.07$ Megapixel, im Verlgeich dazu, hat ein iPhone XS hat eine Kamera mit
12 Megapixel). Um die verschiedenen Farben zu codieren, besitzt jeder Pixel drei
Farbkomponenten: R (rot), G (grün)
und B (blau). Somit hat dieses Bild insgesamt $256 \times 256 \times 3 = 196'608$
Komponenten. Jede Komponente ist ein Feature, welches das KNN zu verarbeiten hat. So bestünde
die erste Schicht des Netzwerkes aus fast $200'000$ Neuronen. Um diese Schicht
nun mit seiner Nachbarsschicht zu verbinden, welche die gleichen Grösse besitzt, werden
$196'608 \times 196'608 = 38'654'705'664$ Verbindungen und damit gleich
viele Gewichte benötigt! Für ein Netwerk ohne eine einzige Zwischenschichten gäbe es
also über 38 Milliarden Modellparameter zu erlernen! Dass dies nicht realistisch ist,
liegt wohl auf der Hand.
\para{}
Nicht nur die Anzahl der Modellparameter sind ein Problem für KNNs in der
Bildverarbeitung, sondern es existieren noch weiter Probleme.
Ohne auf diese Einzugehen sollte nun klar sein, dass eine andere Modellarchitektur notwendig ist, um Machine
Learning auf Bilder anwenden zu können. Für derartige Anwendungen wurde eine modifizierte
Version eines KNNs entwickelt: das \keyword{Convolutional Neural Network} (CNN).
Im Allgemeinen sind CNNs immer dann geeignet, wenn es Daten zu verarbeiten gilt, welche eine
rasterartige Form aufweisen, wie z.B Bilder.
Diese Art von Netzwerk macht Gebrauch von Konzepten aus der klassischen
Bildverarbeitung, wie sie beispielsweise mit Photoshop gemacht wird.
Wie beim Perzeptron und bei den klassischen KNNs wurde auch hier die Architektur
von der Biologie inspiriert.
Der folgende Abschnitt wird die Funktionsweise eines solchen CNNs erklären.
\para{}
\cite{Goodfellow-et-al-2016}
\cite{deeplearning.ai:cnn}
\cite{wiki:cnn}


\section{Bilder als Tensoren}\label{sec:tensor}
CNNs verarbeiten Bildern. Diese Bilder stellen den Input für die Modelle dar.
Um mit diesen Bildern erfassen zu können, ist es sinnvoll, sie als sogenannte
\keyword{Tensoren} vom Rang 3 zu untersuchen, anstatt sie als Anordnungen von
Pixeln zu betrachten. Um zu verstehen, was ein Tensor dritten Ranges ist, müssen
soll zunächst erläutert werden, was mit einem Tensor im Allgemeinen gemeint ist.

\begin{defbox}{Tensor}
  Ein Tensor $\ten{T}$ ist eine Verallgemeinerung von Skalaren, Vektoren und Matrizen auf
  $n$ Dimensionen. Es handelt sich wie bei Matrizen um
  eine Zahlenanordnung. Dabei wird die Anzahl Dimensionen, innerhalb welcher die
  Zahlen liegen, als Rang oder Stufe $n$ des Tensors bezeichnet. Vorstellen kann man sich einen Tensor
  als ein Hyperrechteck mit $n$ Dimensionen, innerhalb dessen die Zahlen in
  einem Raster angeordnet sind. Diese Zahlen sind die Elemente des Tensors.
  Ein Tensor nullten Ranges ist ein Skalar, eine Tensor ersten Stufe ist ein
  Vektor und ein Tensor mit Rang 2 ist eine normale 2D-Matrix.
  \begin{gather*}
    1 \in \set{R} \text{ (Skalar)} \quad \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix}
    \in \set{R}^3 \text{ (Vektor)} \quad
    \begin{pmatrix}
      1 & 2 & 3 \\
      4 & 5 & 6 \\
    \end{pmatrix} \in \set{R}^{2 \times 3} \text{ (Matrix)}
  \end{gather*}
\end{defbox}

\begin{defbox}{Tensor 3. Ranges}
  Ein Tensor $\ten{T} \in \set{R}^{h \times w \times d}$ mit Rang 3 ist eine 3-dimensionale Zahlenanordnung. Man kann sich
  diesen Tensor als eine 3D-Matrix vorstellen; ein Volumen, innerhalb
  dessen die Elemente in einem Raster angeordet sind.
  Analog zum Volumen bezeichnet man die Form des Tensors mit Höhe, Breite und
  Tiefe. \\
  Ein Tensor von der Form $\set{R}^{3 \times 3 \times 3}$ könnte folgendermassen
  aussehen:
  \para{}
  \includegraphics[width=0.3\textwidth]{tensor.pdf}

\end{defbox}
\para{}
Ein Bild kann somit als Tensor dritten Ranges $\ten{B} \in \set{R}^{h
  \times w \times c}$ betrachtet werden, in der Form $(\text{Bildhöhe} \times \text{Bildbreite}
\times \text{Anzahl Farbkomponenten})$.
Die Elemente der Matrix nehmen dann die Werte der Pixelkomponenten an.
Ein schwarz-weisses Bild hat nur eine Komponente, welche die Helligkeit angibt.
Somit wäre es eine normale 2D-Matrix $\mat{B} \in \set{R}^{h \times w}$ (siehe
Abb. (\ref{fig:bildmatrix})).
\para{}
\begin{figure}[h!]
  \begin{tikzpicture}

  \end{tikzpicture}
  \caption{Beispiel einer Bildmatrix}
  \label{fig:bildmatrix}
\end{figure}

\cite{deeplearning.ai:cnn}
\cite{wiki:tensor}

\section{Topologie von CNNs}
Ein CNN besteht, wie ein KNN auch, aus mehreren Schichten. Jede dieser Schichten erhält
als Input ein Bild und hat auch wieder eines als Output.
Ein wichtiger Unterschied des CNNs ist, dass es aus unterschiedlichen Typen von
Schichten besteht. Grundsätzlich lassen sich vier Arten von Schichten unterscheiden:
\begin{itemize}
\item{\keyword{Fully-connected-Schichten}}
\item{\keyword{Convolutional-Schichten}}
\item{\keyword{Pooling-Schichten}}
\item{\keyword{Upsampling-Schichten}}
\end{itemize}
Die Fully-Connected-Schicht ist bereits bekannt und verköpert die klassische Schicht
eines KNNs, bestehend aus Neuronen. Auf diese werden wir daher nicht weiter eingehen,
da sie bereits im vorherigen Kapitel erläutert wurde.
\para{}
Die Convolutional-Schicht ist eine neuartige Schicht, welche es im KNN nicht
gibt. Sie ist die Schicht, welche für das Training relevant ist,
da sie die Modellparameter beinhaltet. Sie extrahiert die relevanten Features
aus den Inputbildern und lernt so die Bilder zu verstehen.
\para{}
Die Pooling-Schichten, wie auch die Upsampling-Schichten, beinhalten keine
Modellparameter und sind deshalb für das Training nicht direkt relevant.
Diese Schichten werden lediglich benötigt, um die verarbeiteten Bilder neu zu
skalieren. Dies ist sinnvoll, da durch die Extraktion gewisser Features die
restlichen Features wegfallen. Somit muss weniger Information pro Bild gespeichert
werden und die Bilder sollten schrumpfen, um Overfitting vorzubeugen. Die Pooling-
Schichten reduzieren die Bilder und die Upsampling-Schichten erweitern sie.
Dies wird in Sektion (\ref{sec:autoencoder}) für die Entwicklung von Autoencoder
hilfreich werden. \\
Grundsätzlich folgt auf eine Convolutional-Schicht entweder eine Pooling-
oder eine Upsampling-Schicht. Somit bilden sie gewissermassen eine Einheit.
\para{}
In Abbildung (\ref{fig:cnn_topology}) ist ein Schema eines CNNs abbgebildet.
\begin{figure}[h!]

  \caption{Schichtung eines CNNs}
  \label{fig:cnn_topology}
\end{figure}

\para{}
\cite{Goodfellow-et-al-2016}
\cite{deeplearning.ai:cnn}
\cite{wiki:cnn}


\section{Convolutional-Schichten und Filter}
Zuerst wir nun die Convolutional Schicht betrachtet. Diese Schicht macht ausgiebig
Gebrauch von sogenannten Filtern. Diese werden in diesem Abschnitt ausführlich behandelt.

\subsection{Filter in der Bildverarbeitung}
\keyword{Filter} (auch Kerne) sind in der Bildverarbeitung sehr verbreitet. Jeder kennt sie entweder
von Photoshop, von Instagram oder sonstigen Bildbearbeitungsprogrammen.
Auch CNNs machen Gebrauch von solchen Filtern, in diesem Fall, um die Features eines Bildes zu
erlernen.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.4\textwidth]{eagle-color.jpg}
  \includegraphics[width=0.4\textwidth]{eagle-amaro.jpg}
  \caption{der Instagramfilter ``Amaro'' auf ein Beispielbild angewandt \cite{res:eagle_image}}
\end{figure}

\para{}
Ein Filter ist eine Region, die deutlich kleiner ist als das verarbeitete Bild
$\ten{B}$, welche über alle Pixel wandert, diese manipuliert und so wieder ein neues Bild
$\tilde{\ten{B}}$ erzeugt.
Mathematisch gesehen handelt es sich bei einem solchen Filter um einen Tensor,
den sogenannten \keyword{Filtertensor} $\ten{F}$ oder auch Faltungstensor (siehe Abb.
(\ref{fig:filter2d}) und Abb. (\ref{fig:filter3d})). Solche Filter sind immer quadratisch, wobei ihre
Zeilen- und Spaltenlänge mit $f$ bezeichnet wird. Dabei ist $f$ immer eine ungerade Zahl, damit ein
Element, das sogenannte \keyword{Zentralelement} (engl.: center element), bezeichnet mit $(\ten{F})_C$,
immer im Zentrum des Filters liegt. \\

\begin{figure}[h!]
  \centering
  \begin{minipage}{0.4\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{filter2d.pdf}
    \captionof{figure}{eine 2D-Filtermatrix mit rotem Zentralelement}
    \label{fig:filter2d}
  \end{minipage}\hspace{1cm}%
  \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{filter3d.pdf}
    \captionof{figure}{ein 3D-Filtertensor mit rotem Zentralelement}
    \label{fig:filter3d}
  \end{minipage}
\end{figure}
\para{}
Das Verhalten des Filters wird durch seine Tensoreinträge bestimmt.
Die Elemente können so gewählt werden, dass bei Anwendung des Filters auf
ein Bild, dieser bestimmte Features hervorhebt. Solche Features
könnten zum Beispiel Ränder oder Kanten sein, welche akzentuiert werden.
\para{}
Nun wird ein beispielshafter Kantendetektionfilter betrachtet. Er besitzt die
Filtergrösse $f=3$ und seine Einträge lauten folgenermassen:
\begin{equation*}
  \mat{F}_K =
  \begin{pmatrix}
    -1 & 0 & +1 \\
    -2 & 0 & +2 \\
    -1 & 0 & +1 \\
  \end{pmatrix}
\end{equation*}

Angewendet auf ein schwarz-weisses Bild, wird ein neues Bild erzeugt, bei welchem alle erkannten
Kanten weiss eingefärbt werden und die restlichen Bereiche schwarz sind.
Der Filter erkennt dabei jede Stelle als Kante, welche zwei Regionen mit
genug grossem Kontrast voneinander trennt.
In Abbildung (\ref{fig:sobel_filter}) wurde der erwähnte Kantendetektionfilter auf ein
Beispielbild angewendet. Links ist das Ursprungsbild und rechts ist das
generierte Bild zu sehen
\footnote{
  Dieses Bild wurde mithilfe von GIMP (GNU Image Manipulation Program) erzeugt. Dieses
  Programm bietet die Möglichkeit eine Filtermatrix auf ein Bild anzuwenden.
}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.4\textwidth]{eagle-gray.jpg}
  \includegraphics[width=0.4\textwidth]{eagle-gray-filtered.jpg}
  \caption{Sobel-Filter angewandt auf Beispielsbild \cite{res:eagle_image}}
  \label{fig:sobel_filter}
\end{figure}

\para{}
\cite{wiki:sobel_operator}
\cite{deeplearning.ai:cnn}
\cite{wiki:kernel}


\subsection{Filter in CNNs}
Aus didaktischen Gründen betrachten wird nun zunächst
nur ein 2D-Filter betrachtet, welcher sich für graustufige Bilder
$\mat{B} \in \set{R}^{h \times w}$ eignet. Somit ist der hierfür ausgewählte
Filter eine 2D-Matrix, bezeichnet mit $\mat{F} \in \set{R}^{f \times
  f}$. Man bedenke, dass es sich eigentlich bei jeder Filtermatrix
um einen 3D-Tensor handelt.
\para{}
Wie bereits dargelegt, können Filter genutzt werden, um gewisse
Features eines Bildes hervorzuheben und andere Features zu ignorieren. Dabei
bestimmen die Filtereinträge, welche Features extrahiert werden. Somit besteht
die Aufgabe darin, die richtigen Filtereinträge zu bestimmen, damit die
gewünschten Features erkannt werden und auf den gewünschten Output abgebildet
werden. Naheliegend sollte jetzt der Gedanke sein, die Filtereinträge als die
Modellparameter eines CNNs zu definieren.
Mithilfe von SGD soll erneut diese Parameter
erlernt werden.
\para{}
Da die Filter die gleiche Funktion haben, wie die Gewichte in einem KNN,
bezeichnet man die Filter in einem CNN mit $\mat{W}$. Die Grösse des Filters
stellt dabei einen Hyperparameter dar.

\begin{equation*}
  \mat{W} = \begin{pmatrix}
    w_{1,1} & \cdots & w_{1,f} \\
    \vdots & \ddots & \vdots \\
    w_{f,1} & \cdots & w_{f,f}
  \end{pmatrix}
\end{equation*}


\subsection{Filteroperation intutitiv}\label{sec:filteroperation_intuitiv}
Wie wendet man nun einen solchen Filter auf ein Bild an? Um das Prozedere einer
Filteroperation leichter verständlich zu machen, soll es erneut für ein
graustufiges Bild $\mat{B} \in \set{R}^{h \times w}$ erläutert werden. Zu einem
später Zeitpunkt wird auch die Anwendung auf farbige
Bilder erklärt (siehe Sektion (\ref{sec:filteroperation_mathematisch})).
\para{}
Bei einer Filteroperation wendet man einen Filter $\ten{W}$ auf einen Bildtensor
$\ten{B}$ an und erhält so ein neues Bild $\tilde{\ten{B}}$. Man schreibt dafür:
$\tilde{\ten{B}} = \ten{W} * \ten{B}$.
\para{}
Um nun diese Operation zu erklären, wird als Beispiel eine
2D-Filtermatrix $\mat{W} \in \set{R}^{3 \times 3}$ verwendet, bei welcher
$f = 3$ gewählt wurde.
Wie bereits oben erwähnt wandert der Filter über das Bild. Dabei befindet
sich der Filter immer über einer Region des Bildes, welche gleich gross ist
wie der Filter selbst (in diesem Fall ($3 \times 3$)). Auf diese entsprechen den Filtermatrixeinträgen immer eindeutig
Bildmatrixeinträge. Diese Region des Bildes bezeichnet man als das
\keyword{Rezeptives Feld} des Filters (siehe Abb. (\ref{fig:receptive_field})).
Das Rezeptive Feld, bezeichnet mit $\hat{\mat{B}} \in \set{R}^{3 \times 3}$, ist also eine
Untermatrix der Obermatrix $\mat{B}$.
\para{}
Jedem Element des Filters entspricht ein Element des Rezeptiven Feldes. Dem
Zentralelement $(\mat{W})_c$ ist dabei gerade dem sogenannten Quellenpixel
(engl.: source pixel) $(\ten{B})_s$ des Bildes zugeordnet. Ihn verwenden wir für eine
Namenskonvention für das Rezeptive Feld. Das Rezeptive Feld wird mit
$\hat{\mat{B}}^{(y,x)}$ bezeichnet, wobei die hochgestellten Indizes $(y,x)$ die Position
des Quellenpixel $(\mat{B})_S$ im Bild $\mat{B}$ angibt.

\begin{figure}[h!]

  % Zentralelement einzeichnen mit Beschriftung
  \caption{ein Filter und sein rezeptives Feld}
  \label{fig:receptive_field}
\end{figure}

\para{}
Der Filter beginnt nun oben links über das Bild zu wandern. Somit
hat der Filter zu anfangs das Rezeptive Feld $\hat{\mat{B}}^{(2,2)}$, denn der
Filter darf nicht über die Ränder des Bildes hinausragen.
Nun werden die Elemente des Filters mit den Elementen des Rezeptiven Feldes
verrechnet, welche die gleichen Dimensionen besitzen. Dabei wird das
Hadamard-Produkt (das elementweise Produkt) der beiden Matrizen miteinander
gebildet. Daraus resultiert eine neue Matrix $\mat{W} \odot
\hat{\mat{B}}^{(2,2)} = \mat{P} \in \set{R}^{3 \times 3}$.
Anschliessend werden alle Elemente der neuen Matrix $\mat{P}$
aufsummiert. Dieser Wert ist dann der Grauwert des ersten Pixels
$(\tilde{\mat{B}})_{1,1}$ des neu enstandenen
Bildes (siehe Gl. (\ref{eq:filter1})). In diesem Sinne stellt der Filter eine
Gewichtung der Nachbarspixel des Ursprungsbild dar, welche
bestimmt, wie die neuen Pixel aussehen.
\\
\begin{equation}\label{eq:filter1}
  (\tilde{\mat{B}})_{1,1} = \sum_{y=1}^3 \sum_{x=1}^3 (\mat{P})_{y,x}
\end{equation}
\\
Nun wird der Filter um ein Element nach rechts verschoben und die gleiche
Prozedur angewendet, um den zweiten Pixel zu berechnen. Dies
wird so lange vollzogen, bis eine ganze Zeile der Bildmatrix durchstreift wurde.
Danach wird der Filter wieder ganz nach links verschoben und er bewegt sich ein
Element nach unten. Das geschieht bis das ganze Bild verrechnet wurde.
Dabei werden die Elemente des ursprünglichen Bildes durchaus mehrfach
verrechnet, da es zu einer Überlappung der vorherigen Position des Filters und
der verschobenen Position kommt.
\para{}
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.7\textwidth]{conv.pdf}
  \caption{Schema, wie ein Filter über ein Bild läuft}
\end{figure}
\para{}
Es ist zu bemerken, dass das neue Bild $\tilde{\mat{B}} \in \set{R}^{(h-2) \times (w-2)}$ nicht mehr die gleichen Masse
aufweist wie das Ursprungsbild $\mat{B} \in \set{R}^{h \times w}$. Das liegt daran, dass pro
Lage des Filters jeweils nur ein Pixel des neuen Bildes entsteht. Man kann
sich vorstellen, dass an der Position des Quellenpixels jeweils ein neuer
Pixel generiert wird. Jedoch muss der Filter immer eine vollständige Region als
Rezeptives Feld besitzen. Dadurch kommt das Zentralelement nie auf die Ränder des
Bildes zu liegen, wodurch sie entfallen.
\para{}
\cite{deeplearning.ai:cnn}
\cite{wiki:convolution}

\subsection{Filteroperationen als diskrete Faltungen}\label{sec:filteroperation_mathematisch}
Eigentlich handelt es sich bei der Anwendung eines Filters auf ein Bild um
eine spezifische mathematische Operation: eine \keyword{diskrete Faltung} (engl.:
\keyword{convolution}).
\footnote{
  Streng genommen wird in CNNs nicht eine diskrete Faltung durchgeführt, sondern
  eine sogenannte \keyword{Kreuzkorrelation}. Der Unterschied besteht darin, dass bei einer
  Kreuzkorrelation die Faltungsmatrix nicht horizontal und vertikal
  gespiegelt wird. Dies im Gegensatz zu der eigentlichen Faltung, bei welcher
  diese Spiegelung statt findet. Jedoch wird
  begrifflich das Wort Faltung für beide Operationen verwendet. Im Verlauf dieser Arbeit
  wird mit dem Wort Faltung immer die Kreuzkorrelation gemeint sein.
}
Daher rührt auch der Name des Convolutional Neural Networks.
Da die Faltung als mathematische Operation relativ kompliziert ist, wird sie im
Rahmen dieser Arbeit nicht in vollem Umfang betrachtet.
Ihre Bedeutung soll auf die Faltung von Filtern beschränkt werden.
\para{}
\begin{defbox}{Diskrete Faltung}
  Definintion Faltung
  \begin{equation}
    (f * g)(x) = \int_{-\infty}^{\infty} f(\tau) g(x-\tau) \text{d}\tau
  \end{equation}
\end{defbox}
\para{}
Mithilfe der Faltungsoperation können die Schritte aus Sektion
(\ref{sec:filteroperation_intuitiv}) zusammengefasst werden.
Bei der Filteroperation handelt es sich nämlich um eine diskrete Faltung des
Filtertensors $\ten{W}$ über den Bildtensor $\ten{B}$. Somit kann
folgende Formel verwenden werden, um einen Pixel $\tilde{\mat{B}}_{y,x}$ des neuen Bildes zu berechnen.
\\
\begin{equation}
  (\tilde{\mat{B}})_{y,x} = (\mat{W} * \mat{B})_{y,x} = \sum_{v=1}^{f} \sum_{u=1}^{f} (\mat{W})_{v,u}(\mat{B})_{y+v-1,x+u-1}
\end{equation}
\para{}
\begin{examplebox}{Faltung Beispielrechnung}
  \begin{equation*}
    \mat{B} =
    \begin{pmatrix}
      10 & 10 & 10 & 0 & 0 & 0 \\
      10 & 10 & 10 & 0 & 0 & 0 \\
      10 & 10 & 10 & 0 & 0 & 0 \\
      10 & 10 & 10 & 0 & 0 & 0 \\
      10 & 10 & 10 & 0 & 0 & 0 \\
      10 & 10 & 10 & 0 & 0 & 0 \\
    \end{pmatrix}
    \text{ und } \mat{W} =
    \begin{pmatrix}
      1 & 0 & -1 \\
      1 & 0 & -1 \\
      1 & 0 & -1 \\
    \end{pmatrix}
  \end{equation*}

  Für die Berechnung des ersten Pixels $(\tilde{\mat{B}})_{1,1}$ führen wir
  folgende Rechnung aus.
  \begin{align*}
    (\tilde{\mat{B}})_{1,1} &= (\mat{W} * \mat{B})_{1,1} = \sum_{v=1}^{f} \sum_{u=1}^{f} (\mat{W})_{u,v} (\mat{B})_{u,v} \\
                            &= 10 \cdot 1 + 10 \cdot 0 + 10 \cdot -1 + 10 \cdot 1 + 10 \cdot 0 + 10 \cdot -1 + 10 \cdot 1 + 10 \cdot 0 + 10 \cdot -1 \\
                            &= 0
  \end{align*}

  Das Gesamtergebniss lautet dann:
  \begin{equation*}
    \tilde{\mat{B}} = \mat{W} * \mat{B} =
    \begin{pmatrix}
      0 & 30 & 30 & 0 \\
      0 & 30 & 30 & 0 \\
      0 & 30 & 30 & 0 \\
      0 & 30 & 30 & 0 \\
    \end{pmatrix}
  \end{equation*}
\end{examplebox}
\para{}
Für farbige Bilder ist das Vorgehen ähnlich. Es werden nun
Tensoren dritten Ranges anstatt von Tensoren zweiten Ranges verwendet.
Betrachten wird der
allgemeinen Bildtensor $\ten{B}_{h \times w \times c}$, wobei $c$ die Anzahl
Farbkomponenten (engl.: channels) ist.
Nun benutzt man einen Filter $\ten{W}_{f \times f \times c}$ mit beliebiger Grösse
$f$ aber mit der gleicher Tiefe $c$ wie das Ursprungsbild $\ten{B}$.
Dadurch muss der Filter nicht entlang der Tiefe des Bildes wandern, weil er
die gleiche Tiefe aufweist. Das hat zur Folge, dass das verarbeitete
Bild $\tilde{\mat{B}}$ immer eine Matrix ist.
Die allgemeine Filtergleichung lautet:
\\
\begin{equation}\tag{FO}
  (\tilde{\mat{B}})_{y,x} = (\ten{W} * \ten{B})_{y,x} = \sum_{v=1}^{f} \sum_{u=1}^{f} \sum_{w=1}^c (\ten{W})_{v,u,w} (\ten{B})_{x+u-1,y+v-1,w}
\end{equation}

\para{}
\cite{Goodfellow-et-al-2016}
\cite{deeplearning.ai:cnn}
\cite{wiki:cnn}

\subsection{Mehrere Filter}
Eine Filteroperation ist nicht auf einen einzigen Filter beschränkt. Es können
mehrere Filter auf das gleiche Ausgangsbild angewendet werden und zusammen ein
Endbild erzeugen.
\para{}
Die Anzahl Filter werden mit $c$ bezeichnet.
Nun wird pro Filter $\ten{W}_i$ eine Faltung über das Ursprungsbild $\ten{W}$ gemacht, wobei
jede Faltung eine neue Matrix $\tilde{\mat{B}}_i = \ten{W}_i * \ten{B}$ liefert.
Dabei ist $i$ der Index des Filters. All diese gefalteten Bilder
$\tilde{\mat{B}}_i$ sind zwei-dimensionale Matrizen, unabhängig davon, wieviele
Komponenten das Ursprungsbild hatte. Aus diesem Grund können die einzelnen
Matrizen $\tilde{\mat{B}}_i$ aufeinander gelegt werden und somit einen grossen 3D-Endtensor
$\tilde{\ten{B}}$ bilden.
Hierfür müssen alle Filter $\ten{W}_i$ gleich gross sein.
\begin{figure}[h!]
  \caption{aufeinander gelegte gefaltene Bilder}

\end{figure}
Die Tiefe des neuen Bildes $\tilde{\ten{B}}$ ist jetzt gerade die Anzahl Filter $c$.
Schon vorher wurde die Tiefe des Bildes (bzw. die Anzahl
Farbkomponenten) mit $c$ bezeichnet. Somit steht $c$ sowohl die Anzahl Filter
einer Schicht, wie auch für die Tiefe des verarbeiteten Bildes.

\para{}
\cite{Goodfellow-et-al-2016}
\cite{deeplearning.ai:cnn}

\subsection{Padding}
Wie bereits erwähnt schrumpfen die Bilder (an den Rändern), wenn man einen Filter auf sie anwendet.
Zur Erinnerung: Dies liegt daran, dass pro Lage des Zentralelements jeweils nur ein Pixel
des neuen Bildes entsteht. Das Zentralelement kommt jedoch nicht nicht auf allen
Ursprungspixeln zu liegen. Auf diese Weise fallen die Ränder weg. Der Umfang des
Wegfalls hängt von der Filtergrösse $f$ ab. Folgende Formel gilt für die
neue Grösse $n_1$ des Bildes, berechnet anhand der alten Grösse $n_0$. $n$ bezeichnet
eine der zwei Seitenlängen.
\begin{equation}
  n_1 = n_0 - f + 1
\end{equation}

Das Problem hierbei ist, dass nach einigen Faltungen das Bild extrem geschrumpft
ist und im Grenzfall kleiner als der Filter wird. Das darf natürlich nicht
passieren. Zusätzlich kommt es zu einem relativen Informationverlust an den
Rändern, im Vergleich zum Rest des Bildes. Dies rührt daher,
das es im Innern des Bildes zu mehr Überlappungen der Filterlagen kommt und dies
bei den Rändern seltener auftritt.
\para{}
Diese unerwünschte Phänomen lässt sich mit sogenanntem \keyword{Padding} beheben. Padding ist ein
Vorgang, welcher vor der eigentlichen Faltung stattfindet. Dabei werden
zusätzliche Ränder (Zeilen und Spalten) an das Ursprungsbild angebracht. Der
Tensor wird der Länge und der Breite und nicht der Tiefe entlang an den
Enden erweitert. Die neuen Elemente werden dabei auf den Wert $0$ gesetzt.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.2\textwidth]{padding.pdf}
  \caption{Padding $p=1$ in blau}
\end{figure}

Das Padding $p$ ist eine Zahl, welche angibt, wieviele Elemente an allen Rändern
hinzugefügt werden. Padding $p = 1$ bedeutet, dass an allen Kanten jeweils eine
Reihe bzw. Spalte hinzugefügt wird.
Begrifflich unterscheidet man zwischen zwei Arten von Padding:
\begin{itemize}
\item{\keyword{Valid-Padding}: Es werden keine züsäzlichen Elemente angebracht. $p$ ist also 0.}
\item{\keyword{Same-Padding}: Es werden so viele Reihen und Spalten angebracht, dass
    die Grösse des Bildes nach der Faltung unverändert bleibt.}
\end{itemize}
\para{}
Um Same-Padding durchzuführen, muss $p$ so gewählen werden, dass es den Wegfall durch
die Filteroperation gerade kompensiert. Da das Bilder durch das Padding der Länge und
Breite nach jeweils um zwei $p$ verlängert wird, erhält man den Ausdruck $n_1 =
n_0 - f + 1 + 2p$. Wenn diese Formel nach $p$ auflöst,
erhält man folgende Formel für das Wählen des Same-Paddings $p$.
\\
\begin{equation}
  p = \frac{f-1}{2}
\end{equation}

\cite{deeplearning.ai:cnn}

\subsection{Stride}
Bis jetzt wurde bei den Filterfaltungen der Filter pro Verschiebung immer nur
um einen Pixel bewegt. Dies ist nicht zwingend. Grundsätzlich können Filter auch
mit anderen Schrittgrössen verschoben werden.
Diese Schrittgröss bezeichnet
man als \keyword{Stride} $s$. Falls $s = 2$ gewählt wird, bedeutet das, dass der
Filter sich pro Hadamard-Produkt um zwei Elemente verschiebt. Somit wurde eine
Position übersprungen. Dies hat zur Folge, dass das neue Bild
deutlich kleiner wird, denn das Zentralelement überspringt somit auch
diese Felder und bildet so deutlich weniger Pixel.
\para{}
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.2\textwidth]{stride.pdf}
  \caption{Abbildung zum Stride: jetztige Position in blau, nächste Position mit
    $s=1$ in rot, nächste Position mit $s=2$ in grün}
\end{figure}
\para{}
Folgende Formel beschreibt die Dimensionen des neuen Bildes unter
Berücksichtigung der Filtergrösse $f$, dem Padding $p$ und dem Stride $s$.
\\
\begin{equation}
  n_1 = \frac{n_0 + 2p - f}{s} + 1
\end{equation}

\cite{deeplearning.ai:cnn}

\subsection{Vorzüge von Filtern}
Es stellt sich nun die Frage, weshalb sich Filter für Maschinelles
Lernen mit Bildern besonders eignen.
Wie bereits erwähnt besteht die Aufgabe von Filtern darin, bestimmte Features
eines Bildes hervorzuheben und die restlichen auszublenden. Die gleiche
Aufgabe erfüllen die Neuronen in einem KNN. Auch sie sollen Features der
Inputdaten erlernen, wobei gewisse Neuronen auf gewisse Features reagieren.
Weshalb verwendet man also nicht einfach KNNs? (abgesehen
von dem Problem mit der grossen Menge an Modellparametern, siehe Sektion \ref{sec:CNN_parameter_problem})
\para{}
Man muss erkennen, dass Bilddaten sich deutlich von sonstigen Daten unterscheiden.
Bildfeatures bzw. Pixel sind nur im Kontext ihrer Nachbarn relevant. Denn
ein Pixel ist erst dann eine Kante oder eine Ecke, wenn er zwei verschiedene
Farbregionen voneinander trennt. Oder ein Gegenstand wird erst durch eine ganze Anzahl
von Pixeln und deren relative Position zueinander charakterisiert.
Man bezeichnet diesen Umstand als \keyword{lokalisierte Features}.
\para{}
Desweiteren sind Bildausschnitte nicht immer gleich
ausgerichtet. Wenn man ein Gesicht auf einem Bild erkennen möchte, sollte es aber
keine Rolle spielen, wo es sich auf dem Bild befindet, welche Grösse es
hat und welche Ausrichtung es aufweist. Um diese Eigenschaften irrelevant für das
Modell zu machen, muss es gewisse \keyword{Invarianzen} erfüllen.
\para{}
Die Eigenschaft, welche CNNs für lokalisierte Features und
Invarianzen geeignet macht, bezeichnet man als \keyword{Parameter-Sharing}.
Es bezeichnet den Umstand, dass auf
mehrere oder alle Features die gleichen Modellparameter wirken. Bei CNNs wird
dies durch die Bewegung des Filters bzw. seiner Einträge über (fast) alle
Pixel bewerkstelligt. Somit ist
es egal, wo und wie sich die lokalisierten Features befinden. Dies führt dann eben zu
den gewünschten Invarianzen: Translations-, Rotations- und
Helligkeitsinvarianz.
\para{}
Eine weiterer Vorteil durch dieses Parameter-Sharing ist, dass die Inputbilder
beliebige Dimensionen besitzen können, da die Filter ihre Bewegung lediglich an
die Grösse des Bildes anpassen müssen.
\para{}
\cite{deeplearning.ai:cnn}

\subsection{Convolutional-Schicht}
Nun soll all das zusammengefasst werden, was soeben über Filter geschrieben
wurde, um die Convolutional-Schicht zu definieren.
\para{}
Die Convolutional-Schicht $l$ beginnt mit ihrem Input, also den
Aktivierungen $\ten{A}^{l-1} \in \set{R}^{h^l \times w^l \times c^l}$, welche die vorherige Schicht ($l-1$) produziert hat.
Falls es sich um die erste Schicht handelt, erhält sie den Input $\ten{X}$ des Netzes.
\para{}
Die Schicht besitzt $c^l$ Varianten an Filtern $\ten{W}^l_i \in
\set{R}^{f^l \times f^l \times c^{l-1}}$. Diese Filter haben alle die gleichen
Eigenschaften bezüglich: der Grösse $f^l$, der Tiefe $c^{l-1}$, dem Padding
$p^l$ und dem Stride $s^l$. Sie unterscheiden sich nur in den Modellparametern
$w^l_{i\,|\,\alpha,\beta,\gamma}$.
\\
\begin{equation*}
  \ten{W}^l_i =
  \begin{bmatrix}
    \begin{pmatrix}
      w_{i\,|\,1,1,1}^l & \cdots & w_{i\,|\,1,f,1}^l \\
      \vdots & \ddots & \vdots \\
      w_{i\,|\,f,1,1}^l & \cdots & w_{i\,|\,f,f,1}^l
    \end{pmatrix}
    & \cdots &
    \begin{pmatrix}
      w_{i\,|\,1,1,c^l}^l & \cdots & w_{i\,|\,1,f,c^l}^l \\
      \vdots & \ddots & \vdots \\
      w_{i\,|\,f,1,c^l}^l & \cdots & w_{i\,|\,f,f,c^l}^l
    \end{pmatrix}
  \end{bmatrix}
\end{equation*}
\\
Nun wird jeder Filter $\ten{W}_i^l$ einzeln über das Bild
$\ten{A}^l$ gefaltet, wodurch mehrere neue 2D-Bilder $\tilde{\mat{A}}_i^l$ entstehen.
\\
\begin{equation}
  \tilde{\mat{A}}_i^l = \ten{W}_i^l * \ten{A}^l
\end{equation}
\\
Die Faltung berechnet sich aus folgender Gleichung:
\\
\begin{equation}\tag{FO}
  (\tilde{\mat{A}}_i)_{y,x}^l = (\ten{W}_i^l * \ten{A}^l)_{y,x} = \sum_{v=1}^{f} \sum_{u=1}^{f} \sum_{w=1}^c (\ten{W}^l_i)_{v,u,w} (\ten{A}^l)_{x+v-1,y+u-1,w}
\end{equation}
\\
Jede dieser Matrizen ist ein Querschnitt $\tilde{\mat{A}}^l_{:,:,i}$ entlang der
Tiefe des neuen 3D-Tensors $\tilde{\ten{A}}^l \in \set{R}^{h^{l+1} \times w^{l+1} \times c^l}$.
Hierbei ist die Tiefe $c^l$ gerade die Anzahl der Filter $c^l$. Die Höhe
$h^{l+1}$ und die Breite $w^{l+1}$ werden jeweils gemäss nachstender Formel berechnet,
anhand der Höhe $h^l$ und der Breite $w^l$ des Ausgangsbildes:
\\
\begin{equation}
  n^{l+1} = \frac{n^l + 2p^l - f^l}{s^l} + 1
\end{equation}
\\
Da die Faltungsoperation linear ist, wird wiederum eine
nicht-lineare Aktivierungsfunktion benötigt, um das Modell zu befähigen, nicht-lineare Probleme zu
lösen zu können.
Deshalb wird in einem letzten Schritt die vektorisierte Aktivierungsfunktion
$\vecf{\varphi}$ auf den Tensor $\tilde{\mat{A}}^l$ angewendet. So ergibt sich die
neue Aktiverung $\ten{A}^{l+1}$ der nächsten Schicht. Empirisch lässt sich nachweisen,
dass sich für CNNs die ReLU-Aktivierungsfunktion aus Sektion (\ref{sec:ReLU}) besser eignet als die
Sigmoidfunktion (QUELLE). Deshalb wird in der Regel die ReLU-Funktion für CNNs verwendet.
\begin{equation}
  \ten{A}^{l+1} = \vecf{\varphi}[\tilde{\ten{A}}^l]
\end{equation}
\para{}
\cite{wiki:cnn}
\cite{deeplearning.ai:cnn}
\cite{Goodfellow-et-al-2016}
\cite{Nielsen}

\section{Dimensionalitätskontrolle}
Beim Anwenden einer Convolutional Schicht gehen Informationen verloren, da nur
die relevanten Features hervorgehoben werden und der Rest verworfen wird. Jedoch
schrumpft das Bild entweder gar nicht (bei Same-Padding) oder es schrumpft nur
vergleichsweise leicht (bei Valid-Padding). Dies ist ein Problem, da die Information in
deutlich weniger Pixeln bzw. Tensorelementen codiert werden könnte. In der Konsequenz
zeigt sich ein unötig hoher Ressourcenverbrauch und eine erhöhte Gefahr für
Overfitting (siehe Sektion (\ref{sec:overfitting})). Um diesem Effekt
entgegenzuwirken, gibt es einerseits sogenannte
\keyword{Pooling-Schichten} und als Gegenstück dazu
\keyword{Upsampling-Schichten}. Ersteres wird verwendet, um die Dimensionalität
der Bilder zu vermindern, und letzteres, um die Dimensionalität der Bilder zu
erweitern. Somit ermöglichen diese Schichten eine kontrollierte Art, (im Gegensatz zum
Padding) die Dimensionalität willentlich zu bestimmen.

\subsection{Pooling-Schicht}
Die Pooling-Schicht verringert die Dimensionalität durch das Zusammenfassen eines Feldes
von Tensorelementen zu einem einzigen Tensorelement.
Dabei wandert das Feld nur entlang der Länge und Breite des Bildes und
nicht etwa entlang der Tiefe. Dies bedeutet, dass das Pooling auf jede Farbkomponente
einzeln angewendet wird, wodurch sich die Tiefe des Bildes nicht ändert.
\para{}
Für die Beschreibung einer Pooling-Schicht finden die gleichen Begriffe
Anwendung, wie bei der Convolutional-Schicht und den Filtern.
Die Grösse des Elementfeldes, welches zusammengefasst wird, bezeichnet man analog zur
Filtergrösse mit $f^l$. Das Stride $s^l$ bezeichnet auch beim Pooling, wie
gross die Schrittgrösse beim Verschieben des Feldes ist. Meistens wählt man den
Stride $s^l$ gerade gleich der Feldgrösse $f^l$, damit alle Pixel
zusammengefasst werden. Kleiner als $f^l$ kann $s^l$ nicht gewählt werden.
\para{}
Man unterscheidet zwischen zwei Arten von Pooling:
\begin{itemize}
\item{\keyword{Average-Pooling}: Das Elementenfeld wird zusammengefasst, indem
    das arithmetische Mittel der Elemente gebildet wird.}
\item{\keyword{Max-Pooling}: Das Elementenfeld wird zusammengefasst, indem das
    Element mit dem höchsten Wert beibehalten wird und die anderen verworfen werden.}
\end{itemize}
In der Praxis verwendet man eigentlich nur Max-Pooling, da es deutlich bessere
Resultate erzielt.
\para{}
\begin{examplebox}{MaxPooling Beispiel}
  Es sei eine MaxPooling-Schicht gewählt mit $f^l = 2$ und $s^l = 2$.
  Diese fasst also jedes $(2 \times 2)$-Feld zu einem Element zusammen. Da aus
  vier Elementen jeweils eines wird, entspricht dies einer Informationsreduktion
  von 75\%.
  \para{}
  \begin{equation*}
    \begin{pmatrix}
      5 & 2 & 4 & 3 \\
      8 & 9 & 5 & 1 \\
      3 & 8 & 6 & 7 \\
      8 & 1 & 4 & 2 \\
    \end{pmatrix}
    \to_{\text{MaxPool}}
    \begin{pmatrix}
      9 & 5 \\
      8 & 7 \\
    \end{pmatrix}
  \end{equation*}
\end{examplebox}
\para{}
Es ist festzuhalten, dass eine Pooling-Schicht keinerlei Modellparameter
besitzt. Somit gibt es nichts zu trainieren. Sie besitzt lediglich einige
Hyperparameter, wie die Feldgrösse $f^l$ und den Stride $s^l$.
\para{}
Um zu berechnen, was die neuen Dimensionen der Bildes nach dem Pooling sind, gelten die
gleichen Formeln wie für die Filteroperationen:
\\
\begin{equation}
  n_1 = \frac{n_0 - f}{s} + 1
\end{equation}
\\


\cite{deeplearning.ai:cnn}
\cite{Goodfellow-et-al-2016}

\subsection{Upsampling-Schicht}
Die Upsampling-Schicht bildet das Gegenstück zur Pooling-Schicht, da sie die
Dimensionalität des Bildes erhöht. Auf den ersten Blick erscheint unklar,
weshalb man die Dimensionalität erhöhen will, da die eigentliche
Motivation dieser Schichten in der Verhinderung von Overfitting besteht. Jedoch
sollte mit dem Kapitel (\ref{sec:autoencoder}) klar werden, weshalb solche Schichten
einen Nutzen haben können.
\para{}
Man verwendet auch hier wieder den Begriff der Feldgrösse $f^l$. Dieses Mal
beschreibt der Wert, wie stark das Bild hochskaliert wird. Beim Upsampling
wird ein Element zu einem $(f^l \times f^l)$-Feld hochgerechnet und erweitert so die
Dimensionalität.
\para{}
Es stellt sich natürlich die Frage, was für Werte das neue Feld erhält.
Dafür ist eine Interpolationsart zu wählen.
Zwei Arten sind besonders verbreitet:
\begin{itemize}
\item{\keyword{Bilineare Interpolation}}
\item{\keyword{Nächste-Nachbar-Interpolation} (engl.: nearest-neighbor-interpolation)}
\end{itemize}
Hier soll nur die Nächste-Nachbar-Interpolation betrachten werden. Sie ist
vergleichsweise trivial, denn alle neuen Feldelemente nehmen einfach den Wert des alten
Elements an. Der Wert auf diese Weise also vermehrt.
\para{}
\cite{deeplearning.ai:cnn}
\cite{Goodfellow-et-al-2016}

\pagebreak
\chapter{Autoencoder}\label{sec:autoencoder}
In Kapitel drei wurde die Architektur eines Convolutional Neural Networks dargelegt.
Nun soll eine weitere Architekur erörtert werden, welche das letzte Bindeglied zur
praktischen Programmierung eines Convolutional-Denoising-Autoencoder verköpert.
Dieses ist notwendig, um die theoretische Funktionsweise eines Autoencoders zu verstehen.
Zielsetzung ist, den Autoencoder mit dem Convolutional
Neural Network zu fusionieren, um auf diese Weise einen einen
Convolutional-Autoencoder zu erhalten. Zu
guter letzt soll dann noch eine Anwendung eines solchen
Convolutional-Autoencoder betrachtet werden, in der Form eines Denoiser.
\para{}
\bigskip
Ein \keyword{Autoencoder} ist eine Architektur, welche ein KNN nicht
nicht bezüglich der Schichtenarten, sondern auf der Ebene der Netzform beschreibt.
Die Aufgabe eines Autoencoders besteht darin, eine neue Repräsentation einer Datenmenge
zu erlernen. Diese neuartige Darstellung soll mit weniger Daten möglichst die gleiche
Information codieren. Somit entwickelt das Modell eine \keyword{effiziente
  Daten-Codierung} für einen Datensatz. Dadurch kann er zur
\keyword{Dimensionalitätsreduktion} verwendet werden.
Neben dieser neuen Repräsentation erlernt das Netzwerk darüberhinaus, wie es
aus dieser Codierung wieder die Ursprungsdaten reproduzieren kann.
\para{}
Autoencoder gehören nicht dem klassischen überwachtem Lernen an. Strenggenommen
zählen sie zum unüberwachtem Lernen, da in den Trainingsdaten keine Labels
enthalten sein müssen. Jedoch ist diese Begrifflichkeit etwas irreführend,
weil nur deshalb keine Labels benötigt werden, da sie den Features entsprechen!
Somit wird das Netzwerk trotzdem darauf trainiert, gewisse gewünschte Outputs
zu liefern.
\para{}
\cite{book:autoencoder}

\section{Topologie}
Ein Autoencoder besteht wie das klassische KNN aus Neuronen.
Wie bereits erwähnt sind die ``Labels'' eines Autoencoders gleich
den Features. Somit versucht ein Autoencoder, die Werte der Inputneuronen
möglichst exakt in die Outputneuronen zu kopieren.
Diese Operation wäre an sich ziemlich bedeutungslos, da mit den
Daten nichts passiert. Das Netzwerk würde lediglich erlernen, eine
Identitätsfunktion zu imitieren.
Aus diesem Grund muss man gewisse Einschränkungen einführen. Diese bringen das Netzwerk dazu,
interessante Methoden zu entwickeln, um die Features \textit{approximativ} zu rekonstruieren.
\para{}
Die angesprochene Einschränkung besteht darin, dass dem Autoencoder nicht
beliebig viel Kapatzität für die Codierung der Features gewährt wird.
Es stehen dem Netzwerk nur wenige Zahlenwerte zur Verfügung, um die Features
in der neuen Repräsentation zwischenzuspeichern, bevor sie wieder in die
ursprüngliche Form zurück transformiert werden.
Die Kapazitätseinschräkung wird durch die Topologie des Autoencoders bezweckt.
\para{}
\bigskip
Der einfachste Autoencoder besteht aus drei Schichten, welche sich wie folgt
gliedern lassen:
\begin{itemize}
\item{eine Inputschicht mit $d$ Neuronen}
\item{eine Zwischenschicht mit $p$ Neuonen, bezeichnet als \keyword{Flaschenhals}}
\item{eine Outputschicht mit $d$ Neuronen}
\end{itemize}
In Abbildung (\ref{fig:basic_autoencoder}) ist
ein beispielhafter Autoencoder abbgebildet, mit $d = 5$ Features und einem
Flaschenhals der Grösse $p = 2$.
\begin{figure}[h!]
  \centering
  \includegraphics[height=0.4\textwidth]{ae1.pdf}
  \caption{Schichten eines kurzen Autoencoders}
  \label{fig:basic_autoencoder}
\end{figure}
\para{}
Die Inputschicht enthält die Features $\vec{x} \in \set{R}^d$. Dagegen enthält die Outputschicht
die \keyword{Rekonstruktionen} der Features, welche wir mit $\vec{\hat{x}} \in \set{R}^d$
bezeichnen. Als logische Konsequenz müssen die beiden Schichten die gleiche
Anzahl $d$ an Neuronen besitzen.
Die Flaschenhalsschicht hingegen besitzt nur $p$ Neuronen, wobei $p \ll d$ ist.
Somit ist sie deutlich kleiner als die anderen beiden Schichten und bildet
damit die \keyword{Kapazitätsbeschränkung}.
\para{}
Damit wurde ein Autoencoder mit nur einer Zwischenschicht betrachtet.
Gängiger ist jedoch das Netz aus mehreren Zwischenschichten zu konstruieren. Dabei
werden die Schichten zum Flaschenhals hin immer schmaler und die Schichten nach
dem Flaschenhals wieder dicker. In Abbildung (\ref{fig:big_autoencoder})
ist dies gut ersichtlich.
Aus Vereinfachungsgründen soll dennoch die Funktionsweise eines Autoencoder mit
nur einer Zwischenschichten hergeleitet werden.
\para{}
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{ae2.pdf}
  \caption{Schichten eines tieferen Autoencoders}
  \label{fig:big_autoencoder}
\end{figure}

\section{Funktionsweise}
Ein Autoencoder möchte die Features $\vec{x}$ von der Inputschicht in die
Outputschicht kopieren. Jedoch zwingt der Flaschenhals den Autoencoder dazu, eine
neue Repräsenation der Features zu erlernen. Dies aus dem Grund, weil die
Features in ihrer jetzigen Form keinen Platz im Flaschenhals finden, da $p \ll d$.
Daher muss das Modell die Informationen der Features,
auf nur einige wenige relevante Eigenschaften komprimieren.
Das Modell muss gewissermassen einen Code entwickeln.
Diese neue Repräsentation nennt man das \keyword{Encoding}. \\
In einem zweiten Schritt muss der Autoencoder aus diesem Encoding wiederum
versuchen, die ursprünlichen Features zu rekonstruieren. Diese Rekonstuktion
bezeichnet man als das \keyword{Decoding}.

\paragraph{Encoder und Decoder}
Daher kann man einen Autoencoder in zwei Teilmodelle untergliedern. Das
eine Modell erzeugt das Encoding und das andere das Decoding. Dafür wird
die Hypothesenfunktion $h$ in ein Funktionenpaar $(\phi,\psi)$ aufgespaltet.
\begin{itemize}
\item{$\phi: \fspace{X} \to \fspace{E}$ Encoder-Funktion}
\item{$\psi: \fspace{E} \to \fspace{X}$ Decoder-Funktion}
\end{itemize}
Die erste Teilfunktion $\phi$ ist für das Encoding zuständig. Sie
bildet den Inputraum $\fspace{X} \subseteq \set{R}^d$ auf
den Encodingraum $\fspace{E} \subseteq \set{R}^p$ ab, welcher die neue
Repräsentation $\vec{x^*}$ enthält. Der Inputraum $\fspace{X}$ enthält
sämtliche Features $\vec{x}$.
\para{}
Das Gegenstück ist die Teilfunktion $\psi$. Sie bildet den Encodingraum
$\fspace{E}$ zurück auf den Inputraum $\fspace{X}$ ab.
\para{}
Der Output des Autoencoders $\vec{\hat{x}}$ wird somit durch die aufeinander
folgende Anwendung der Teilfunktionen auf die Features $\vec{x}$ gebildet:
\begin{equation}
  \vec{\hat{x}} = \psi(\phi(\vec{x})) = (\psi \circ \phi)(\vec{x})
\end{equation}

Durch das trainieren dieses Modells, wird die Kostenfunktion minimiert. Als
Beispiel kann der Mittlere-quadratischen-Fehler $C_{MSE}$ dienen.
Wird er als Kostenfunktion verwendet, so wird folgender Ausdruck minimiert:
\begin{equation}
  \min_{\phi,\psi} {\|\vec{x} - \vec{\hat{x}}\|}^2 = \min_{\phi,\psi} {\|\vec{x} - (\psi \circ \phi)(\vec{x})\|}^2
\end{equation}
Anderst ausgedrückt wird versucht Funktionen $\phi$ und $\psi$ zu finden, welche
die Differenz zwisceh den ursprünglichen Features $\vec{x}$ und den Rekonstruktionen
$\vec{\hat{x}}$ minimieren. Dabei muss der Autoencoder
eine Dimensionalitätsreduktion betreiben. Er entwickelt einen Code, um die Features
in der Flaschenhalsschicht komprimiert zu repräsentieren. Durch die
Kapazitätseinschräkung $p \ll d$ muss $\fspace{E}$ eine niedrig-dimensionale
Codierung der Features darstellen. Dem Modell ist es nicht möglich, die Inputs
identisch wiederherzustellen, da bei der Abbildung $\phi$ Informationen verloren
gegangen sind. Dadurch sollte der Code im optimalen Fall nur noch die Merkmale
der Features umfassen, welche für die umfangreichste Information codieren.
Dieser Code kann extrahiert werden, um eine Repräsentation der Features zu
erhalten, welche nicht mehr mehrere hundert Merkmale umfasst, sondern nur noch einige
wenige. Jegliche natürliche Schwankung in den Werten der Features geht
verloren, da sie nicht im Code enthalten ist, weil sie nicht verallgmeinert
werden kann.

\paragraph{Der Code}
Bei der Suche dieser Funktionen muss der Autoencoder einen reduzierten Code
entwickeln, um die Features neu zu repräsentieren.
Durch diese Komprimierung ist es dem Modell unmöglich, die Rekonstruktion
perfekt zu vollziehen. Sie ist immer approximativ, da bei der Komprimierung
Informationen verloren gegangen sind. Am effektivsten wird die Kostenfunktion
minimiert, wenn das Modell einen Code entwickelt, welcher die Features mit dem
höchsten Informationsgehalt abstrahiert. Die Features, welche unwichtig sind
oder nur durch zufälliges Rauschen der Daten enstanden sind, können daher verworfen werden.
\para{}
Es ist wichtig zu erkennen das der Code, welcher der Autoencoder entwickelt,
\keyword{datenspezifisch} ist. Das heisst, er kann nicht wie z.B ein
Bildkompressionalgorithmus auf beliebige Daten angewendet werden. Stattdessen
ist er nur sinnvoll für die Kompression von Daten zu verwenden, mit welchen er trainiert
wurde.
BEISPIEL EINER ABSTRAKTION (a la Menschliches Gesicht beschreiben).

\section{Anwendungen}
Die Eigenschaft, welche einen Autoencoder für die verschiedensten
Anwendungen geeignet macht, ist die Dimensionalitätsreduktion.
Wir werden nun einige Anwendungen grob betrachten.

\subsection{Kompression}
Es handlet sich um eine verlustbehaftete Kompression.
Dimensionalitätsreduktion. Falls lineare Neuronen, ähnlich zur Hauptkomponentenanalyse
$\frac{p}{d}$ des ursprünglichen Speicherbedarfs. Diese Komprimierung ist dabei Datenspezifisch.

Datenverkehr beschleunigen, weil weniger Daten.

\subsection{Neue Datenrepräsentation}
neues Datenformat

\subsection{Visualisierung von Daten}
Visualisierung von hochdimensionaler Daten in $\set{R}^2$ und $\set{R}^3$


Im übernachsten Kapitel werden wir eine Anwendung noch genauer Betrachten die
des Denoisers.

\section{Convolutional-Autoencoder}
Ein Convolutional-Autoencoder ist, wie es der Name schon sagt, ein Autoencoder,
welcher anstatt der normalen Fully-Connected-Schichten eines KNNs, die Schichten eines CNN verwendet.
Dieser ist interessant, weil er zur Kompession von Bildern verwendet werden kann.
Wie bereits erwähnt, ist auch hier die Kompression datenspezifisch.
Beispielsweise könnte man einen Convolutional-Autoencoder mit einem Datensatz
von menschlichen Gesichtern trainieren. Dieser würde dann einen
datenspezifischen Code für diese Gesichter entwickeln. Man kann sich
vorstellen, dass das Modell nicht mehr die Farbwerte der Features im Encoding
speichert, sondern stattdessen diese Features abstrahiert. Somit könnten
Eigenschaften, wie die Augenfarbe, Breite der Nase, Mundposition und vieles mehr
im Flaschenhals gespeichert werden. Diese Informationen brauchen offenkundig
weniger Kapazität als die gesamten Pixelwerte.
\para{}
Des weiteren sind Convolutional-Autoencoder von Interesse, da sie zur weiteren
Anwendung eines Autoencoder leiten: den Denoising-Autoencoder.
Dieser soll im nächsten Abschnitt betrachtet werden.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{conv-autoencoder.png}
  \caption{Darstellung eines Convolutional Autoencoders}
\end{figure}


\cite{yt:autoencoder_faces}

\section{Denoising-Autoencoder}
Ein Denoising-Autoencoder oder einfach Denoiser verkörpert einen leicht abgeänderterten
Autoencoder. Er wird vorallem verwendet, um ein Rauschen aus Bildern zu entfernen.
\para{}
Im UNterschied zu klassischen Autoencodern benötigt er einen Datensatz mit
wahren Labels, da er nicht die Features kopiert.
Seinen Input besteht aus den verrauschten Bilder und die Labels beinhalten die
unverrauschten Bilder. Somit wird er darauf trainiert, das Rauschen zu entfernen.
Man mag sich nun die Frage stellen, weshalb hierfür ein Autoencoder und
kein normales CNN verwendet wird. Empirisch hat sich gezeigt, dass sich
Denoiser besser für das Entrauschen von Bilder geeignet sind (QUELLE).
Das hat nachvollziehbare Gründe. Bei der
Entwicklung des Codes für Encoding-Repräsentation, kann das Rauschen nicht
codiert werden, da es völlig zufällig auftritt. Es existieren keine Muster oder
Gesetzmässigkeiten, welche anderst codiert werden könnten. Somit muss der
Autoencoder diese Rausch-Features verwerfen. Gegenüber sonstigen CNNS hat er ein leichteres Spiel.
\para{}
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{cae.pdf}
  \caption{Visualisierung eines Convolutional-Denoising-Autoencoder}
\end{figure}

\subsection{Generierung der verrauschten Bilder}
Um den Autoencoder zu trainieren, müssen verrauschte Daten generieren werden.
Dies geschieht, indem auf die Features ein additives Gauss'sches Rauschen
anwendet wird.
Dafür verwendet man üblicherweise eine Gauss'sche Normalverteilung
$\mathcal{N}(\mu = 0, \sigma^2 = 1)$ mit Erwartungswert $\mu = 0$ und Varianz
$\sigma^2 = 1$. Dieser wird jeweils ein Zufallswert entnommen und addiert ihn zu
jedem Feature hinzu.
Auf diese Weise lässt sich ein verrauschter Datensatz entwickeln.
\begin{gather*}
  r \sim \mathcal{N}(\mu = 0, \sigma^2 = 1) \\
  \tilde{x} = x + r
\end{gather*}


% ------------------------------------

\chapter{Frameworks für Maschinelles Lernen}
In diesem Kapitel werden wir zwei Frameworks betrachten, welche eine
Implementation der ML-Algorithemen realisiert haben. Wir werden sie in ihrer
groben Funktionsweise untersuchen. Diese Tools werden wir im
nächsten Kapitel benutzten um dann ein konkretes Modell zu programmieren.

\section{TensorFlow}\label{sec:tensorflow}
\begin{wrapfigure}{l}{3cm}
  \includegraphics[width=3cm]{tf_logo.png}
  \caption{TF-Logo}
\end{wrapfigure}
\keyword{TensorFlow}, kurz TF, ist ein von Google entwickeltes \keyword{Framework} für
\keyword{datenstromorientierte Programmierung}. Es ist im Grunde genommen eine
Mathematik-Programmbibliothek für numerische Berechnungen, dessen Hauptanwendungsbereich im Maschinellen Lernen
liegt. Die Vorteile von TF bestehen vor allem darin, dass die praktische
Anwendung des Frameworks sehr
einfach ist und fast keine mathematische Vorkenntnisse voraussetzt. Somit kann
man sehr einfach ein ML-Modell definieren, trainieren und verwenden. Ausserdem
besitzt TF sehr performante Implementierungen, welche Algorithmen, wie die
Vorwärtspropagierung und die Rückwärtspropagierung, sehr schnell ausführen
lässt. Somit geht das Training von ML-Modellen deutlich schneller als bei
selbstgeschriebenen Implementationen.
TF ist gratis und ein Open-Source-Projekt. Es kann auf Github gefunden werden
(siehe QR-Code). \\
\qrcode[height=2cm]{https://github.com/tensorflow/tensorflow}
\para{}
In der Industrie ist TensorFlow sehr weit verbreitet. Es wird von den
verschiedensten Unternehmen für ihre ML-Anwendungen gebraucht; darunter waren:
Google, Twitter, AirBnB, Intel, CocaCola, Snapchat und noch viele mehr.
\para{}
Grundsätzlich wurde TF für Maschinelles Lernen entwickelt. Speziell eignet es
sich für Deep Learning mit künstlichen neuronalen Netzen. Es ist ein
Interface, welches das definieren von Modellen erlaubt, welche dann mit den
eingebauten ML-Algorithmen trainiert werden können. \\
Trotz dieser eigentlichen Grundidee von TF, sollten die Abstraktionen eigentlich
für beliebige Anwendugen, welche mit numerischen Berechnungen zu tun haben,
sich eignen.
\para{}
Im folgenden werden die grobe Funktionsweise von Tensorflow untersuchen. Um
genau zu sein betrachten wir Tensorflow-Core Version r1.14.
\footnote{Momentan befindet sich TF 2.0 in der Beta. Es ist eine grosse
  Überarbeitung des Frameworks. Es ist zu empfehlen, sobald TF 2.0 offiziel
  veröffentlich wurde, es zu verwenden.}
Wir werden TF nur sehr grob betrachten, da wir später nicht direkt damit
arbeiten werden, sondern mit der High-Level-API Keras. Trotzdem ist er hilfreich
ein grobes Verständniss über TF zu besitzen.
\para{}
Die wichtige Resource um TensorFlow zu lernen ist wohl die offizielle TensorFlow
Website. Dort findet man auch die Dokumentation.
\para{}
\qrcode[height=2cm]{https://www.tensorflow.org/api/stable}

\para{}
\cite{book:tensorflow}

\subsection{Client und Master}
Wie die meisten grossen Programmbibliotheken ist Tensorflow in Backend und
Fontend aufgeteilt. Das Frontend von Tensorflow bezeichnet man als
\keyword{Client} und das Backend als \keyword{Master}.
\para{}
\begin{infobox}{Fontend und Backend}
  Das \keyword{Frontend} ist der Teil eines Programmes, mit welchem der Benutzer
  interagiert. Es stellt eine Schnittstelle zu dem sogenannten Backend eines
  Programmes dar. Es exponiert so gewisse Funktionalitäten des Programmes dem
  Benuzter gegenüber, ohne dass er dabei etwas von der zugrundeliegenden Logik mitbekommt.
  \para{}
  Das \keyword{Backend} ist das Gegenstück zum Frontend. Es implementiert
  jedigliche Programmlogik, welche das gewünschte Verhalten der Applikation
  hervorruft. Der Benutzer hat keinen direkten Zugriff auf diesen Teil des
  Programmes und bekommt somit nichts von dessen Existenz mit.
\end{infobox}
\para{}
Der Client - also das Frontend - von TF ist in \keyword{Python} geschrieben.
\footnote{
  Tensorflow besitzte mehrere Frontends, welche in den verschiedensten
  Programmiersprachen geschrieben sind. Wir werden jedoch nur das
  Python-Frontend betrachten, da es am besten von TF unterstützt ist.
}
Python ist allgemein als eine sehr einsteigerfreundliche Programmiersprache bekannt. Sie
ermöglicht es dem Benutzer mit wenigen Zeilen Code und ziemlich intuitiv
TensorFlow zu verwenden. Wenn wir also mit TensorFlow programmieren, werden wir
mit Python-Code arbeiten.
\para{}
Der Master - also das Backend - von TF ist dagegen in \keyword{C++} (und CUDA)
geschrieben. Dies hat den Grund, dass C++ im Gegensatz zu Python sehr performant
ist. Also ist die Zeit zum Auführen eines Programmes sehr kurz. Dafür ist der
Programmcode deutlich anspruchsvoller und verboser. Da der Benutzer jedoch keine
direkte Interaktion mit dem Master hat, bekommt er davon nichts mit.

\subsection{Verwendung --- der Client}
Nun möchten wir betrachten wie man TensorFlow zu verwenden hat, damit wir
später ein ML-Modell damit basteln können. Dafür werden gewisse
Python-Kenntnisse vorausgesetzt. Wir betrachten nun also
den Client von TF. Man verwendet verschiedene Konzepte um
programmatisch mit TF zu arbeiten. Im folgenden Abschnitt werden wir diese
Konzepte behandeln.

\subsubsection{Graph}
TensorFlow ist datenstromorientiert. Das bedeutet, dass man eine Berechnung
(engl.: computation) durch einen gerichteten Graphen beschreibt. Zuerst stellt
man diesen Graph auf und fügt ihn zusammen und erst nach der Definition führt man
ihn aus.
Es handelt es sich dabei um einen \keyword{Computation Graph}, wie wir ihn schon in Sektion (\ref{sec:backpropagation}) kennengelernt
haben. In TensorFlow bezeichnet man diesen Graphen als einen \keycode{tf.Graph}.
\para{}
Er besteht aus einer Menge an Knoten, welche miteinander über Pfade
verbunden sind. Die Knoten halten Werte in sich, welche über die Pfade,
verrechnet werden und in den nächsten Knoten eingespeist werden. Die Pfade sind also die Operationen, welche auf die Werte
angewandt werden. So führt der Graph eine Datenstrom-Berechnung aus.
Jeder Knoten hat null oder mehr Inputs und null oder mehr Outputs.
\para{}
Man muss einen Graph nicht explizit definieren (kann es aber, falls man mehrere
Graphen braucht), da beim defnieren der Operationen sie implizit dem
sogenannten \keyword{Default Graph} hinzugefügt werden.

\subsubsection{Tensoren}
Die zentrale Dateneinheit in TensorFlow sind Tensoren, wie wir sie bereits in
Sektion (\ref{sec:tensor}) kennengelernt haben.
Auch die Knoten des Graphs reprasentieren Tensoren, welche aber keine feste Wert
enthalten, da sie erst beim Ausführen des Graphens, mit Werten befüllt werden.
Nach der Ausführung der Graphs, kann man die Zahlenwerte auslesen.
\para{}
Man verwenet in TF deshalb Tensoren, weil so die Daten beliebige Dimensionalitäten
und Formen annehmen können. Es können sowohl Listen von Zahlen (Vektoren),
wie auch Bilder (Tensoren dritter Ordnung) verrechnet werden. \\
Nun sollte auch klar sein, weshalb wir bei der Herleitung der Gleichungen, auch
immer eine Matrixschreibweise erarbeitet haben.
\para{}
In Tensorflow bezeichnet man einen Tensor mit \keycode{tf.Tensor}.
Er ist definiert durch seine Form (Stufe inbegriefen) und den Datentyp seiner
Elemente (z.B. tf.float32).
\para{}
Jeder tf.Tensor ist mit einer einzigen Graph-Ausführung assoziert. Sie
besitzen ihren Wert nur für den jeweiligen Durchlauf, danach wird er verworfen.
Man sagt die Tensoren sind ``immutable'', da die Werte nicht erhalten bleiben
und so nicht modifiziert werden können.
\para{}
Die soeben beschriebene Tensoren agieren einfach als Durchflussstellen innerhalb
des Graphen. Sie müssen nicht explizit definiert werden, da sie automatisch
eingefügt werden, wenn der Graph durch seine Operationen definiert wird.
\para{}

Neben dieser Art von Knoten, gibt es noch verschiedene Inputknoten.
stellen den Input in den Graphen dar und müssen explizit definiert werden. Von ihnen gibt es verschiedene Arten,
welche wir nun betrachten werden.

\paragraph{tf.constant}
Der \keycode{tf.Constant} Tensor ist ein Input-Knoten in den Graphen.
Er hat immer einen konstanten Wert als Output. Dieser wird beim Erstellen
angegeben.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  <Varname> = tf.constant(<Wert>)
\end{minted}
\para{}
Folgender Python-Code ist ein Beispiel, welcher drei konstante Tensoren mit unterschiedenlichen
Formen erstellt. Als Argument, gibt man die Elemente des Tensors, worüber auch
die Form ersichtlich an.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  skalar = tf.constant(1.0)
  vector2 = tf.constant([1.0, 2.0])
  matrix2x2 = tf.constant([[1.0, 2.0], [3.0, 4.0]])
\end{minted}
\[\text{skalar}=1 \quad \text{vector2}=\begin{pmatrix}1&2\end{pmatrix} \quad
  \text{matrix2x2}=\begin{pmatrix}1&2\\3&4\end{pmatrix}\]

\paragraph{tf.placeholder}
Um den Graphen für Maschinelles Lernen zu verwenden, braucht man eine
Möglichkeit ihn mit Inputs für die Modelle zu befüttern. Um dies möglichst
komfortabel zu machen, gibt es den sogenannten
\keycode{tf.Placeholder} Tensor. Wie der Name es schon sagt, ist er ein
Platzhalter für die Inputs, welcher beim Start der Graphausführnug eingespeist
werden. Man muss einzig seine Dimensionen und seinen Datentyp (z.B. tf.float32) bei der Erstellung angeben.
\para{}
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  <Varname> = tf.placeholder(dtype=<Datentyp>, shape=<Form>)
\end{minted}
Für den Datentyp wählt man eigentlich immer
\mintinline{python}{dtype=tf.float32}. Somit kann man mit Gleitkommazahlen rechnen.
Die Form gibt man einfach als $n$-Tupel von Integern an, welche die Anzahl
Komponenten in der jeweiligen Dimension angeben. Somit bedeutet die Form
\mintinline{python}{shape=(4,2,3)} einen Tensor im Tensorraum $\set{R}^{4 \times
  2 \times 3}$.

\paragraph{tf.variable}
Ein weiteres Kriterium um Maschinelles Lernen mit dem tf.Graph durchzuführen,
ist dass man Modellparameter einführen kann. Dafür ist der
\keycode{tf.Variable} Tensor geeignet. Er ist im Gegensatz zu den anderen
Tensoren ``mutable''. Das bedeutet, dass seine Werte unabhängig von einer
einzigen Graphenausführung existieren und erhalten bleiben. Seine Elementen
können modifiziert werden. Somit sind sie geeignet, für Modellparameter.
Die Werte bleiben grundsätzlich jede Ausführung erhalten und können beim
Trainingsdurchlauf angepasst werden.
\para{}
Folgender Python-Code kreirt eine tf.Varaible mit einem Anfangswert und ändert ihren Wert danach.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  <Varname> = tf.Variable(<Initialwert>, name=<optionaler Name>)
  <Varname>.assign(<neuer Wert>)
\end{minted}
Bei der neuen Wertebesetzung kann auch der alte Wert des Tensors referenziert
werden, um den Wert zum Beispiel zu erhöhen:
\mintinline{python}{var.assign(var + 2.0)}.

\subsubsection{Operationen}
Die Knoten des tf.Graph - also die Tensoren - werden über die Pfade aufeinander
abbgebildet. Die Operationen, welche sie aufeinander abbildet, bezeichnet man
als \keycode{tf.Operation}. Eine tf.Operation besitzt jeweils einen Namen und repräsentiert eine
abstrakte Berechnung. Sie hat jeweils null oder mehr Tensor-Objekte als
Input, wie auch als Output.
\para{}
Man definiert meistens keine neuen Operationen, sondern verwendet einfach die vordefinierten.
Ein Beispiel dafür wäre die \keycode{tf.matmul(a,b)}, welche eine
Matrixmultiplikation zwischen \mintinline{python}{a} und \mintinline{python}{b}
ausführt. Im folgenden Code wird das Resulat im neuen Tensor
\mintinline{python}{c} gespeichert.
``MatMul'' bezeichnet).
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  c = tf.matmul(a, b)
\end{minted}
\para{}
Mit diesem Konzept können wir nun endlich den eigentlichen Graph definieren.
Dafür muss man lediglich ein paar Inputtensoren erstellen, und diese mithilfe
von Operationen verrechnen. Natürlich kann man die Resulate der Operationen
dann weiter verrechnen, bis man den gewünschten Grapen hat.
\para{}
Hier ein Beispiel eines solchen Graphs.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  # Inputknoten
  a = tf.constant([[1.0, 2.0], [3.0, 4.0]])
  b = tf.constant([[1.0, 1.0], [0.0, 1.0]])
  # Operationen
  c = tf.matmul(a, b)
  d = tf.placeholder(dtype=tf.float32, shape(2,2))
  e = tf.add(c, e)
\end{minted}

\begin{figure}
  \caption{Visualisierung des beschriebenen \keycode{tf.graph}}
\end{figure}
\para{}


\subsubsection{Session}
Der Client kommuniziert mit dem Master über eine sogenannte
\keycode{tf.Session}.
Sie ist die Schnittstelle zwischen Frontend und Backend und ermöglicht es einen tf.Graph
zu erstellen und ihn auszuführen.
Um die Session zu erstellen verwendet man die Funktion \keycode{tf.Session()}.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  sess = tf.Session()
\end{minted}
Es ist Best-Practise eine Session, wie eine Datei über das
\keycode{with}-Statment zu öffen und zu schliessen.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  with tf.Session() as sess:
  # Code
\end{minted}
\para{}
Um den Graphen nun zu berechnen, muss man lediglich die Session ausführen, das
geschieht über die Methode \keycode{tf.Session.run()}.
Diese Funktion erwartet als Argument einerseits den Knotenpunkt, welchen man
berchnen möchte. Dessen Wert ist der Rückgabewert der Funktion.
Andererseits erwartet die Funktion noch einen Python-Dictionary, den sogenannten
\keycode{feed\_dict}, welcher die \code{tf.placeholder} mit ihren Werten befüllt.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  result = sess.run(<Knotenpunkt>, feed_dict=<Placeholder-Dictionary>)
\end{minted}
Es folgt nun ein Codebeispiel, welches die Forwärtspropagierung eines
Sigmoide Neurons mit drei Inputs implementiert. Es wird es mit den Inputs $x = \trans{\begin{pmatrix}1&2&3\end{pmatrix}}$ ausgeführt.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  import tensorflow as tf

  # Inputs
  x = tf.placeholder(dtype=tf.float32, shape=(3))

  # Modellparameter
  w = tf.Variable(tf.random_uniform([3], -1, 1), name='weight')
  b = tf.Variable(tf.random_uniform(1, -1, 1), name='bias')

  # Forwärtspropagierung
  z = tf.add(tf.tensordot(x, w, 1), b)
  a = tf.sigmoid(z)
  y = a

  with tf.Session() as sess:
  res = sess.run(y, feed_dict = {x: [1, 2, 3]})
  print(res)
\end{minted}

\subsubsection{Optimizer}
Um ein ML-Modell in TF zu implementieren, muss man das ganze Modell mit
allen Operationen als Graphen definieren. Dies hört sich im Moment noch sehr
aufwendig an, ist es jedoch nicht. Wir werden nämlich die High-Level-API Keras
zur Hilfe ziehen, bei der Graphen erstellt.
Keras stellt Implementationen von verschiedenen KNN-Schichten zu verfügen, damit man sie nicht mehr selber
erstellen muss. Diese kann man dann einfach zu einem Graphen zusammenfügen.
Aber dazu später mehr.
\para{}
Die Ausführung des erstellen Graphen stellt dann einfach eine
Vorwärtspropagierung dar. Bevor jedoch das Modell brachbare Resualte liert,
muss man es trainieren. Dafür verwendet man einen \keycode{tf.train.Optimizer}.
\para{}
Der erste Schritt ist dafür eine Kostenfunktion zu definieren. Es sthene dabei
Verschiedne aus \keycode{tf.losses} zur Verfügung. Als Argument nimmt die
Kostenfunktion die Label-Tensoren \code{labels} und die Vorhersagen-Tensoren \code{predictions}.
Beispielsweise könnte man den Mittleren Quadratischen Fehler wählen:
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  cost = tf.losses.mean_squared_error(labels=y_hat, predictions=y)
\end{minted}
Nun erstellt man den Optimizer. Dabei stehen wieder mehrere zur Auswahl. Der
\keycode{tf.train.GraidentDescentOptimizer} ist eine Implementation des
Gradientenverfahren. Er nimmt als Arugment die Lernrate $\eta$.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  optimizer = tf.train.GradientDescentOptimizer(learning_rate=<>)
\end{minted}
Nun kann man den Optimizer damit beauftragen die Kostenfunktion \code{loss} zu
minimieren. Ausserdem muss man dem Optimizer eine List von \code{tf.Variable}s
geben, welche die zu optimierenden Modellparameter sind.
Man erhält so man eine Optimierungs-Operation, welche man dann
ausführen kann.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  opt_op = opt.minimize(cost, var_list=<Hyperparameter-Liste>)
  opt_op.run()
\end{minted}
\para{}
Nun folgt ein kleines Beispielmodell, welches trainiert wird.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  import tensorflow as tf

  # Inputs
  x = tf.placeholder(dtype=tf.float32, shape=(3))

  # Modellparameter
  w = tf.Variable(tf.random_uniform([3], -1, 1), name='weight')
  b = tf.Variable(tf.random_uniform(1, -1, 1), name='bias')

  # Forwärtspropagierung
  z = tf.add(tf.tensordot(x, w, 1), b)
  a = tf.sigmoid(z)
  y = a

  cost = tf.losses.mean_squared_error(labels=y_true, predictions=y_pred)


  with tf.Session() as sess:
  print(sess.run(cost)) # cost is just another node
  optimizer = tf.train.GradientDescentOptimizer(0.01)
  train = optimizer.minimize()
  res = sess.run(y, feed_dict = {x: [1, 2, 3]})
  print(res)

  # Daten
  x = tf.constant([[1], [2], [3], [4]], dtype=tf.float32)
  y_true = tf.constant([[0], [-1], [-2], [-3]], dtype=tf.float32)

  # Modell
  linear_model = tf.layers.Dense(units=1)
  y_pred = linear_model(x)

  # Evalutaion
  sess = tf.Session()
  init = tf.global_variables_initializer()
  sess.run(init)

  print(sess.run(y_pred))

  # Loss
  loss = tf.losses.mean_squared_error(labels=y_true, predictions=y_pred)

  print(sess.run(loss))


  # Training
  optimizer = tf.train.GradientDescentOptimizer(0.01)
  train = optimizer.minimize(loss)


  for i in range(100):
  _, loss_valü = sess.run((train, loss))
  print(loss_valü)

\end{minted}


\subsection{Implementation --- der Master}
TF ist gerade deshalb sehr attraktiv für ML, da es sehr performant (schnell
ausführbar) ist und dadurch die Trainingszeit für die Modelle relativ
kurz haltet. Dies wurde durch verschieden Designentscheidungen von TF erreicht.
\para{}
Falls der Leser am Verständniss dieser Entscheide interessiert ist und an der
eigentlichen Implemention von TF (dem Master) kann er im Anhang (\ref{sec:anhang_tf}) dazu
Informationen finden.



\subsection{Tensorboard}
TensorBoard ist ein externes Zusatzprogramm zu TensorFlow. Mit dessen Hilfe
man Daten zum Modell-Graph und zu dem Trainingsprozess sammeln kann
und diese dann Visualisieren kann. \\
Es ist möglich eine Visualisierung des Modell-Graphs zu betrachten. Desweiteren
kann man Funktionsgraphen der Kostenfunktionentwicklung betrachten.
\para{}
TensorBoard ist als WebServer geschrieben und kann über einen WebBrowser
benutzt werden.
Man muss TensorBoard als Backend/Callback in TensorFlow aktivieren, damit die
nötigen Daten während der Graphausführung gesammelt werden.

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{tensorboard.png}
  \caption{das TensorBoard-Webinterface}
\end{figure}

\subsubsection{Hyperparameter einstellen}
Der wichtigste Aspekt für uns von TensorBoard stellt die Tatsache dar, dass man
die Trainingsfortschritte visualisieren kann. Man kann verschiedene
Funktionsgraphen der Kostenfunktion betrachten. Es ist sogar möglich mehrere
solcher Graphen miteinander zu vergleichen. Somit kann man ein Modell, bei
welchem man mehrere Trainings mit jeweils verschiedene Hyperparameter gemacht
hat, miteinander vergleichen. So ist es ziemlich einfach gute Hyperparameter
herauszusuchen. Genau dieses Verfahren werden wir dann bei unserem eigenen
Modell anwenden. \\
Der Graph ist interaktiv, was bedeutet, dass man mit ihm interagieren kann und
so detailiertere Angaben erhalten kann. Zum Beispiel kann man so auch die
beanspruchte Trainingszeit des Modells erfassen, welche meistens von Interesse ist.
\para{}
In der untenstehenden Abbildung ist ein Graph von TensorBoard dargestellt,
welcher die Entwicklung der Kostenfunktion für den Testdatensatz verschiedener
Modell anzeigt. In diesem Beispiel hätte die orange Kurve ganz unten am besten
Abgeschnitten. Dieses Modell hat die kleinsten Kosten verursacht.

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{tensorboard_graph.png}
  \caption{Trainingsvisualisierungen in TensorBoard}
\end{figure}
\para{}

\pagebreak
\section{Keras}
\begin{wrapfigure}{l}{3cm}
  \includegraphics[width=3cm]{keras_logo.jpg}
  \caption{Keras-Logo}
\end{wrapfigure}
\keyword{Keras} ist ein Open-Source Deep-Learning Framework geschrieben in
Python. Keras wurde entwickelt, um eine einheitliche Schnittstelle für
verschieden Backends, wie TensorFlow, Microsoft Cognitive Toolkit und Theano zu
bieten. Seit TF Version 1.4 ist Keras ein fester Bestandteil der TensorFlow-Core-API.

Keras ermöglicht ein sehr simples und benutzerfreundliches defnieren von
Deep-Learning-Modeln welche dann sehr einfach trainiert werden können.
Wir werden also anstatt TF direkt zu verwenden, über Keras das Modell definieren.
Deshalb ist ein tiefgehendes Verstätniss von TF überflüssig.

\subsection{Funktionsweise}
Keras baut auf TensorFlow auf. Es verwendet die verschiedenene Konzepte von TF,
wie Variablen, Operationen und Graphen und abstrahiert sie.
Dadurch bietet Keras eine einfacheres Vorgehen dem Benutzer an.
\para{}
Die gängigste Vorgehensweise besteht darin, ein sogenanntes
\keyword{Sequential-Modell} zu bauen.
Dies ist ein Modell, welches einfach ein linearer Stapel von verschiedenen
Schichten ist. Keras stellt verschiedene Arten von KNN-Schichten bereits zur
verfügung, welche einfach aneinander gereiht werden können und so einen Graph bilden.
\para{}
Es gibt verschiedene Arten ein Modell zu definieren. Entweder definiert man eine
\code{tf.keras.Sequential} Variable, welcher dann mit der Methode \code{add()}
neue Schichten hinzugefügt werden.
Oder man speichert die verschiedenene Schichten in Variablen und gibt dann die
Anfangsschicht und die Schlussschicht bei der Erstellung des Modellvariable an.

\subsection{Schichten}
Bei der Initalizsierung der Modellparameter, benutzt Keras standardmässig
anderes Vorgehen als in der Theorie in Sektion \ref{sec:parameter_initalisieren} erklärt. Es benutzt nämlich eine
Glorot-Initalisierung mit einer Uniformen Verteilung. Wir werden spezifizieren
bei der Schicht-Erstellung, dass wir die in der Theorie erklärte Variante
verwenden wollen.

Wir werden nun kurz die verschiedenen Schichttypen von Keras durchgehen, welche wir in
der Theorie schon vorgestellt haben.
\para{}


\cite{net:keras_init}

\paragraph{Input-Schicht}
Die Input-Schicht ist ziemlich selbsterklärend. Sie ist einfach die
Platzhalter-Schicht, welche alle Features enthält.
Man definiert sie folgendermassen:
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  tf.keras.Input(shape=(<Form>)
\end{minted}

\paragraph{Dense-Schicht}
Wir beginnen mit dem Fully-Connected-Schicht. Diese wird in Keras als
\code{Dense} bezeichnet. Der Name kommt davon, dass sie dicht mit jedem Neuron
der vorherigen Schicht verbunden ist.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  tf.keras.layers.Dense(<Anzahl Neuronen>,
  activation=<Aktivierungsfunktion>)(<vorherige Schicht>, kernel_initalizer='glorot_normal')
\end{minted}

\paragraph{Conv2D-Schicht}

\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  tf.keras.layers.Conv2D(filters=<#Filters>, kernel_size=<Filtergrösse>,
  strides=<>, padding=<>, activation=<>, )
\end{minted}
Für alle Schichten eines CNNs gilt folgendes Datenformat:
Input muss folgende Form haben $(samples, channels, rows, cols)$. Output hat
analoge Form $(samples, filters, new\_rows, new\_cols)$

\paragraph{MaxPool2D-Schicht}

\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  tf.keras.layers.MaxPool2D(pool_size=<Feldgrösse>, strides=<>, padding=<>)
\end{minted}


\paragraph{UpSampling2D-Schicht}

\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  tf.keras.layers.UpSampling2D(size=<>, interpolation=<>)
\end{minted}


\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  model = tf.keras.Sequential()
  # Adds a densely-connected layer with 64 units to the model:
  model.add(layers.Dense(64, activation='relu'))
  # Add another:
  model.add(layers.Dense(64, activation='relu'))
  # Add a softmax layer with 10 output units:
  model.add(layers.Dense(10, activation='softmax'))
\end{minted}


\subsection{Training und Evaluierung}

Nach dem das Modell definiert wurde, muss man es Kompilieren.
Dies macht man mit der Methode \code{tf.keras.Model.compile} diese erwartet als
Arugmente, den Optimzier und eine Kostenfunktion. Man kann optional noch gewisse
Metriken angeben, welche wahrend des Trainings erfasst werden sollen.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  model.compile(optimizer=tf.train.AdamOptimizer(0.001),
  loss='categorical_crossentropy',
  metrics=['accuracy'])
\end{minted}
Die Optimizer sind die gleichen wie die in TensorFlow-Core. Sie befinden sich
alle in tf.train. Die Kostenfunktionen sind auch die gleichen wie in TF-Core.
Zum beispeil tf.loss.mean\_squared\_error.
\para{}
Hier ein Beispiel:
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  model.compile(optimizer='sgd', loss='mean_squared_error')
\end{minted}
Das Kompilieren bildet dann aus den gegeben Schichten den eigentlichen
Computational Graph.

\subsubsection{Fit}
Nach der Kompilierung kann das Modell trainiert werden. Dies wird durch einen
einzigen Methodenaufruf gemacht. Mann muss nur die \code{fit} Methode aufrufen.
Diese erwartet als Parameter ein NumPy-Array aller Inputdaten, ein NumPy-Array
aller Labels, die Anzahl Epochen und die Minibatch-Grösse.
Optional kann man noch einen Testdatensatz angeben, welcher benutzt wird um die
Kosten zu bestimmen.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  model.fit(<inputs>, <labels>, epochs=<>, batch_size=<>)
\end{minted}

\subsubsection{Predict}
Sobald das Modell trainiert wurde, kann man es dazu verwenden, Vorhersagen für
neue Daten zu machen. Dazu benutzt man die \code{Predict}-Methode. Sie erwartet
einfach ein Array mit Inputs. Als Rückgabewert hat sie die Vorhersagen zu den
gegeben Inputs.

\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  result = model.predict(data, batch size=32)
\end{minted}

\subsection{Keras Beispielmodell}

\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  inputs = tf.keras.Input(shape=(32,))  # Returns a placeholder tensor

  # A layer instance is callable on a tensor, and returns a tensor.
  x = layers.Dense(64, activation='relu')(inputs)
  x = layers.Dense(64, activation='relu')(x)

  predictions = layers.Dense(10, activation='softmax')(x)

  Instantiate the model given inputs and outputs.

  model = tf.keras.Model(inputs=inputs, outputs=predictions)

  # The compile step specifies the training configuration.
  model.compile(optimizer=tf.train.RMSPropOptimizer(0.001),
  loss='categorical_crossentropy',
  metrics=['accuracy'])

  # Trains for 5 epochs
  model.fit(data, labels, batch_size=32, epochs=5)
\end{minted}


% ------------------------------------

\chapter{Entwicklung eines Denoising-Autoencoders}
In diesem Kapitel soll nun das bisherige Wissen zusammengeführt werden, indem
unter Anwendung von TensorFlow und Keras ein konkretes
Anwendungsbeispiel programmiert wird.

Dabei wird darauf eingegangen, welche Schritte die
Entwicklung eines solchen Modells umfasst und wie dabei am besten vorgegangen
wird.

\section{Das konkrete Modell}
Wir werden nun einen Convolutional-Denoising-Autoencoder programmieren.
Dieser kann verwendet werden um das Bildrauschen von Bildern zu eliminieren und
sie so wieder erkenntlich zu machen.
Zu Demonstrationszwecken werden handgeschriebene Ziffern vom MNIST-Datenset
verwendet, weil die Funktionsfähigkeit auf dieser Basis sehr einfach erkennbar ist.

\begin{figure}
  \caption{grobes Schema des Modells}
\end{figure}

\para{}
\subsection{Daten}
Als Trainingsdaten verwenden wir den \keyword{MNIST}-Datensatz. Er ist der wohl
bekannteste Datensatz für beispielhaftes Maschinelles Lernen.
Er besteht aus schwarz-weiss Bildern von handgeschrieben Ziffern.
Zusätzlich würde es auch noch Labels zu den Ziffern geben, um das Modell beispielweise
für Ziffernerkennung zu trainieren. Jedoch werden
wir diese nicht brauchen, da wir einen Autoencoder entwickeln, dessen Labels den
Inputs entsprechen.
\para{}
Der Datensatz besteht aus einem Trainingsdatensatz von 60'000 Sampels und einem Testdatensatz
von 10'000 Sampels. Alle Ziffern sind bereits korrekt formatiert, da ihre Grösse
auf $28 \times 28$ Pixel normalisiert wurde und sie im Bild zentriert sind.

\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{mnist.png}
  \caption{ein Auszug an MNIST-Bildern (invertierte Farben) \cite{res:mnist_images}}
\end{figure}

\para{}
\cite{net:mnist}

\section{Setup}
Zunächst ist es wichtig, das Entwicklungsumfeld richtig zu konfigurieren. Das
bedeutet konkret, dass Python zusammen mit den verschiedenen Programmbibliotheken
installieren werden muss.
\para{}
Die Installationsschritte werden in dieser Arbeit für eine arch-basierte
Linuxdistribution erklärt, welche den \keyword{Pacman}-Package-Manager verwendet.
\para{}
Falls der Leser ein anderes Betriebssystem verwendet möchte, ist auf die offizielle
TensorFlow Website für die Installationsschritte zu verweisen:
\para{}
\qrcode[height=2cm]{https://www.tensorflow.org/install}

\paragraph{Python3}
Damit TensorFlow und Keras verwenden werden können, bedingt dies, dass Python3
installiert sein muss.
Folgender Befehl muss in der Kommandozeile ausgeführt werden, um die
Installation vorzunehmen.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize]{bash}
  sudo pacman -S python
\end{minted}

\paragraph{TensorFlow}
Für die Installation von TensorFlow muss unter Arch Linux das entsprechende
Package installieren werden.
Falls der Leser über eine Nvidia-Grafikkarte verfügt, welche eine ``Compute
Capability'' von mehr als 3.5 besitzt, kann er von der
GPU-Hardwarebeschleunigung Gebrauch machen.
Somit ist die CUDA-Version von TF mihtilfe des folgenden Kommandos zu installieren:
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize]{bash}
  sudo pacman -S python-tensorflow-cuda
\end{minted}
Pacman installiert nun automatisch zuerst alle Programmabhängigkeiten, wie
CUDA, cuDNN und das TensorFlow-Backend, bevor das eigentliche
Python-TensorFlow installiert wird.
\para{}
Falls keine geeignete Grafikkarte vorliegt, ist es ebenso möglich die normale
TF-Version ohne Unterstützung für die GPU zu verwenden.
Dies kann durch nachstehendes Kommando vorgenommen werden.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize]{bash}
  sudo pacman -S python-tensorflow
\end{minted}
\para{}
Dabei muss Keras nicht explizit installiert werden, da es in TensorFlow implementiert ist.

\paragraph{Python-Module}

Für das Python-Programm werden zwei Packages benötigt, welche
nicht in der Standardbibliothek von Python enthalten sind:
\begin{itemize}
\item{NumPy: Ein Package, welches verschiedene mathematische Konzepte
    implementiert; vor allem Vektor- und Matrix-Arithmetik}
\item{Matplotlib: Ein Package zum Erstellen von Plots und Grafiken}
\end{itemize}

Man installiert beide mit folgenden Kommandos.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize]{bash}
  sudo pacman -S python-numpy
  sudo pacman -S python-matplotlib
\end{minted}

\section{Entwicklung}
Nun beginnt die eigentliche Programmierung.

\subsection{Testprogramm}
Zunächst wird ein kleines Testprogramm geschrieben, welches überprüft, ob
alle Programmabhängigkeiten korrekt installiert wurden und verwendet werden können.

In den ersten Zeilen des Pythonprogamms werden die Importstatments
geschreiben, um NumPy, Matplotlib und TensorFlow verfügbar zu machen.
Keras muss wie gesagt nicht explizit geladen werden, da es in TensorFlow
enthalten ist.
Nun können die Versionnummern der verschiedenen Programmbibliotheken
in die Konsole geschrieben werden, um zu überprüfen, ob alles richtig konfiguriert ist.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  import numpy as np # NumPy wird impotiert
  import matplotlib as mpl # Matplotlib wird impotiert
  import tensorflow as tf # TensorFlow wird impotiert

  print(np.__version__) # Schreibt die NumPy-Version nach stdout.
  print(mpl.__version__) # Schreibt die Matplotlib-Version nach stdout.
  print(tf.__version__) # Schreibt die TensorFlow-Version nach stdout.
\end{minted}
Falls der Output folgenden Charakter hat (die Versionsnummern müssen nicht
die gleichen sein) und keine Fehlermeldungen erfolgen, sollte alles funktionieren.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{text}
  1.17.0
  3.1.1
  1.14.0
\end{minted}
\para{}

\subsection{Trainingsdaten}
Nun haben wir mit dem kleinen Testprogramm verifiziert, dass alle
Programmabhängigkeiten funktionieren. Auf dieser Basis können wir mit dem
eigentlichen Programm beginnnen.
\para{}
In den folgenden Auschnitten wird der Code laufend weiter ausgebaut.
Auf diese Weise zeigt sich, was er bewirkt.
Das eigentliche Programm beginnt mit folgenden Importstatements.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  import numpy as np
  import matplotlib.pyplot as plt
  import tensorflow as tf
\end{minted}

\subsubsection{Laden des MNIST-Datensatzes}
Der erste Schritt besteht darin, die Datensätze zu laden. Wie bereits
erwähnt, wird dabei vom MNIST-Datensatz Gebrauch gemacht. Da dieser stark verbreitet ist,
existiert eine Funktion in Keras, welche automatisch diese Daten herunterlädt
und sie als NumPy-Arrays zur Verfügung stellt.
Die Funktion gibt die verschiedenen Komponenten der Daten in folgendem Format zurück \code{(x\_train, y\_train),
  (x\_test, y\_test)}.
Da nur Interesse an den Features \code{x} besteht, werden die überflüssigen
Labels \code{y} mithilfe der Wegwerf-Variable \code{\_} verworfen.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  (x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()
\end{minted}
Nun sind die Features fürs Training in der Variable \code{x\_train} und die
Features des Trainingsdatensatzes in der Variable \code{x\_test} gespeichert.

\subsubsection{Formatieren der Daten}
In einem nächsten Schritt müssen die Daten transformiert werden, damit sie die
richtige Form für das Modell haben.
Wie bereits erwähnt, ist der MNIST-Datensatz einfach verwendbar,
da alle Bilder die gleichen Masse aufweisen und die Ziffern zentriert sind.
\para{}
Jedoch sind die Grauwerte zu normalisieren, denn im Moment liegen sie noch im
Intervall $[0, 255]$ und sind vom Typ Integer.
Das Modell kann am besten mit Kommazahlen, welche im Intervall $[0,1]$ liegen,
umgehen. Um diese Anpassung vorzunehmen, kann folgender Code angefügt werden.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  x_train = x_train.astype('float32') / 255.0 # Normalisierung
  x_test = x_test.astype('float32') / 255.0 # Normalisierung
\end{minted}
Desweiteren werden im Modell ConvLayers verwendet. Diese in Keras
implementierten ConvLayers erwarten Inputs in der Form $(m, w, h, c)$, wobei
$m$ die Anzahl der Bilder ist, $w$ die Bildbreite, $h$ die Bildhöhe und $c$ die
Anzahl der Farbkomponenten. Die Bildbreite $w$ und -höhe $h$ von 28 Pixeln wird beibehalten, wie auch
die Anzahl Farbkomponeten $c=1$. Die Anzahl Bilder $m$ lässt sich aus der Länge
des Arrays entnehmen \code{len(x\_train)}. Mithilfe vom NumPy lässt sich das Array wie folgt umformen.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  x_test = np.reshape(x_test, (len(x_test), 28, 28, 1)) # neue Form: (60'000, 28, 28, 1)
  x_train = np.reshape(x_train, (len(x_train), 28, 28, 1)) # neue Form: (10'000, 28, 28, 1)
\end{minted}
Die Daten besitzen nun die richtige Form für das Modell.

\subsubsection{Generieren der verrauschten Bilder}
Den Input in das Modell stellen nicht die normalen MNIST-Bilder dar, sondern
eine verrauschte Variante von diesen. Sie werden nun generiert, indem ein
additives Gauss'sches Rauschen auf sie angewendet wird.
\para{}
Zuerst wird eine Matrix $\mat{R} \in \set{R}^{28 \times 28 \times 1}$ erstellt, welche die gleiche Form, wie die
Bilder besitzt. Diese Matrix wird mit Zufallswerten gefüllt. Für die
Zufallswerte wird eine normalisierte Gauss'sche Normalverteilung
$\mathcal{N}(\mu = 0, \sigma^2 = 1)$ mit Erwartungswert $\mu = 0$ und Varianz
$\sigma^2 = 1$ verwendet. Da jedes Bild eine eigene Rauschmatrix verlangt,
wird zu diesem Zweck eine Liste $(\mat{R}_1,\mat{R}_2,\ldots,\mat{R}_m$) der Länge $m$ an
Rauschmatrizen erzeugt. Dafür kann die NumPy Funktion
\code{np.random.normal(loc=<$\mu$>, scale=<$\sigma^2$>,size=<Form>)} verwendet werden.
Die somit erhaltenen Rauschmatrizen werden dann mit einer Rauschkonstante
\code{noise\_factor} multiplziert und anschliessend wird das Produkt auf die
MNIST-Bilder addiert.
Nach dem Hinzufügen der Rauschwerte müssen die Grauwerte noch auf das Intervall
$[0,1]$ zurecht gestutzt werden. Auch hierfür kann eine NumPy-Funktion
\code{np.clip(var, min, max)} angewendet werden. Diese Schritte werden sowohl mit dem
Trainingsdatensatz, als auch mit dem Testdatensatz vollzogen. \\
Somit lautet der Code:
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  noise_factor = 0.5

  # für x_train
  noise_matrices = np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)
  noise_matrices *= noise_factor
  x_train_noisy = x_train + noise_matrices
  x_train_noisy = np.clip(x_train_noisy, 0.0, 1.0)

  # für x_test in Kurzfassung
  x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)
  x_test_noisy = np.clip(x_test_noisy, 0.0, 1.0)
\end{minted}
\para{}
Mithilfe der Matplotlib ein Blick auf die verrauschten Bilder im Vergleich zu
den Originalbildern geworfen werden.
Dafür wird ein Plot erstellt, welcher jeweils 10 Bilder beider Arrays zeigt.
\para{}
Für diesen Zweck wird eine \code{pyplot.figure} erstellt. Innerhalb dieser
werden jeweils 10 \code{subplots} definiert, sowohl für die verrauschten Bilder, als
auch für die Originale.
Bevor die Bilder angezeigt werden können, müssen sie in die richtige Form für
die Matplotlib gebracht werden. Sie sollen die Form $(28 \times 28)$ aufweisen.
Ausserdem ist der Plot als schwarz-weiss Grafik zu spezifizieren.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  n = 10 # jeweils 10 Bilder
  plt.figure()
  for i in range(n):
    # Originalbilder
    ax = plt.subplot(2, n, 1+i)
    img_clean = x_test[i].reshape(28, 28)
    plt.imshow(img_clean)
    plt.gray()

    # verrauschte Bilder
    ax = plt.subplot(2, n, 1+n+i)
    img_noisy = x_test_noisy[i].reshape(28, 28)
    plt.imshow(img_noisy)
    plt.gray()
  plt.show()
\end{minted}

Das Ergebniss ist in Abbildung (\ref{fig:noisy_clean_mnist}) sichtbar.
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\textwidth]{noised.png}
  \caption{Verrauschte Bilder neben den Orignalbildern}
  \label{fig:noisy_clean_mnist}
\end{figure}
\para{}
Die Daten liegen nun vollständig und im richtigem Format vor.
Somit kann zur Definition des Modells übergegangen werden.

\subsection{Modell definieren}
Mithilfe von Keras kann das Modell eines Convolutional-Denoising-Autoencoder
definiert werden. Dies geschieht nach dem Vorbild der Theorie.
Wie bereits erklärt umfasst die Topologie eines CNNs die verschiedensten Hyperparameter.
Für diese werden der Einfachheit halber zunächst unbegründeten Werte verwendet.
Zu einem späteren Zeitpunkt werden die passenden Hyperparameter durch iteratives Ausprobieren ermittelt.
\para{}
Das Modell beginnt mit der Inputschicht $l=0$, welche ein Tensor ist, der
die Inputwerte enthält. Die Schicht hat logischerweise die gleiche Form, wie ein MNIST-Bild.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  input_data = tf.keras.Input(shape=(28, 28, 1))
\end{minted}
\para{}
Nach dieser Inputschicht folgt der Encoder des Autoencoders. Dieser besteht
sowohl aus Convolutional-Schichten, wie auch aus Pooling-Schichten, welche sich abwechseln.
Für alle Schichten werden vorerst unbegründeten Hyperparameter gewählt.
Für die Convolutional-Schichten wird
eine Filtergrösse $f^l = 3$, eine Anzahl Filter von $c^l = 32$,
ein Stride $s=1$, Same-Padding und die ReLU-Aktivierungsfunktion $\varphi =
\varphi^{\text{ReLU}}$ verwendet.
Da Keras standardmässig die Initialisierung der Modellparameter mit einer
Uniformen Verteilung vornimmt, muss der \code{kernel\_initalizer} explizit auf
\code{'glorot\_normal'} gesetzt werden. Dadurch findet die Initalisation gleich
wie in Sektion \ref{sec:parameter_initalisieren} statt. \\
Der Code für die Defition einer solchen Convolutional-Schicht lautet:
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  conv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3),
  strides=(1, 1), padding='same', activation='relu',
  kernel_initializer='glorot_normal')
\end{minted}
Für die Pooling-Schichten wird analog eine Feldgrösse $f = 2$, ein
Stride $s = 2$ und Same-Padding gewählt. Dies hat zur Wirkung, dass jedes $(2
\times 2)$-Feld zu einem einzigen Element zusammengefasst wird. \\
Der Code für eine dieser Pooling-Schichten lautet:
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  pool_layer = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='same')
\end{minted}
Für den Decoder werden die gleichen Convolutional-Schichten wie für den Encoder
verwendet. Anstelle von Pooling-Schichten verwendet er UpSampling-Schichten,
welche jeweils nach einer Convolutional-Schicht folgen.
Für die UpSampling-Schichten wurde eine Feldgrösse $f = 2$ gewählt. Somit wird
ein einziger Pixel auf ein $2 \times 2$-Felder hochskaliert.
Als Interpolationsmethode wird der Nächste-Nachbar-Algorithmus verwendet.
Die beschriebene UpSampling-Schicht wird folgendermassen definiert:
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  upsampling_layer = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='nearest')
\end{minted}
Nun folgt die Deklarierung aller Schichten und deren Verknüpfung. \\
Das Modell weist einen Flaschenhals auf, welcher zwei Convolutional-Schichten
tief ist. Das bedeutet, dass der Encoder aus zwei
Convoutional-Layer-Pooling-Player-Paaren besteht. Danach folgt der einschichtige
Flaschenhals. Im Anschluss liegt der Decoder, der zwei
Convolutional-Layer-Upsampling-Layer-Paaren umfasst.
Um die jeweils neue Schicht zur vorherigen Schicht zu verbinden, gibt man die
Vorherige in Klammern am Ende des Statements an. \\
Der ganze Graph wird durch folgenden Code gebildet:
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  # Encoder
  input_data = tf.keras.Input(shape=(28, 28, 1))
  econv0 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same',
    activation='relu', kernel_initializer='glorot_normal')(input_data)
  emaxpool0 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='same')(econv0)
  econv1 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same',
    activation='relu', kernel_initializer='glorot_normal')(emaxpool0)
  emaxpool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='same')(econv1)

  # Flaschenhals der Form (7, 7, 32)
  encoded =  emaxpool1

  # Decoder
  dconv0 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same',
    activation='relu', kernel_initializer='glorot_normal')(encoded)
  dupsample0 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='nearest')(dconv0)
  dconv1 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same',
    activation='relu', kernel_initializer='glorot_normal')(dupsample0)
  dupsample1 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='nearest')(dconv1)
  dconv2 = tf.keras.layers.Conv2D(filters=1, kernel_size=(3, 3), strides=(1, 1), padding='same',
    activation='sigmoid', kernel_initializer='glorot_normal')(dupsample1)
  decoded = dconv2
\end{minted}
Auffallend im Code ist, dass der Decoder nicht etwa zwei Convolutional-Layers
besitzt, sondern sogar drei. Die letzte Schicht wird benötigt, damit
die Daten wieder die richtige Formatierung aufweisen. Das heisst einerseits,
dass die Outputs wieder in der Ursprungsform $(28 \times 28)$ vorliegen müssen.
Andererseits sollen die Outputs wieder im Intervall
$[0,1]$ befinden. Mithilfe der Sigmoid-Funktion als Aktivierungsfunktion werden
sie genau auf dieses Intervall abgebildet. \\
Für alle anderen Schichten wurde die ReLU-Aktivierungsfunktion gewählt.
\para{}
Nun muss noch eine Modellvariable deklariert werden, indem man die Input- \code{input\_data} und
die Outputschicht \code{decoded} angibt.
Danach ist das Modell zu kompilieren. Bei der Kompilierung kann das
Optimierungsverfahren und die Kostenfunktion gewählt werden. Für die Optimierung
wird das Stochastische-Gradientenverfahren SGD verwendet.
Als Kostenfunktion wird der Mittlere-Quadratische-Fehler $C_{\text{MSE}}$ gewählt.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  autoencoder = tf.keras.Model(input_data, decoded)
  autoencoder.compile(optimizer='sgd', loss='mean_squared_error') # SGD und MSE
\end{minted}
Mithilfe von \code{tf.keras.Model.summary} kann man eine Zusammenfassung des
Modells in Textform erhalten. So können Informationen bezüglich der Form der
verschiedenen Schichten abgerufen werden, um zu überprüfen, ob alles stimmig ist.
Nachfolgende Summary gilt für unser Modell:
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{text}
  _________________________________________________________________
  Layer (type)                 Output Shape              Param #
  =================================================================
  input_1 (InputLayer)         [(None, 28, 28, 1)]       0
  _________________________________________________________________
  conv2d (Conv2D)              (None, 28, 28, 32)        320
  _________________________________________________________________
  max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0
  _________________________________________________________________
  conv2d_1 (Conv2D)            (None, 14, 14, 32)        9248
  _________________________________________________________________
  max_pooling2d_1 (MaxPooling2 (None, 7, 7, 32)          0
  _________________________________________________________________
  conv2d_2 (Conv2D)            (None, 7, 7, 32)          9248
  _________________________________________________________________
  up_sampling2d (UpSampling2D) (None, 14, 14, 32)        0
  _________________________________________________________________
  conv2d_3 (Conv2D)            (None, 14, 14, 32)        9248
  _________________________________________________________________
  up_sampling2d_1 (UpSampling2 (None, 28, 28, 32)        0
  _________________________________________________________________
  conv2d_4 (Conv2D)            (None, 28, 28, 1)         289
  =================================================================
  Total params: 28,353
  Trainable params: 28,353
  Non-trainable params: 0
  _________________________________________________________________
\end{minted}
Es zeigt sich, dass das Modell gesamthaft circa 28'000 Parameter umfasst,
welche zu trainieren sind.

\subsection{Modell trainieren}
Nun gilt es das definierte Modell zu trainieren.
Mit Keras kann dies mit einem einzigen Funktionsaufruf durchgeführt werden.
Diese Funktion erwartet einige Argumente. Zuerst sind die Features und Labels anzugeben. Die
Features wurden in der Variable \code{x\_train\_noisy} gespeichert. Die Labels
sind die unverrauschten MNIST-Bilder, welche in der Variable \code{x\_train} enthalten sind.
Desweiteren wird die Grösse eines Mini-Batches als 128 Sampels spezifiziert. Das
Training erfolgt über 100 Epochen.
Wichtig ist auch, dass die Trainingssampels vor dem Training druchmischt werden, damit keine Mustern
innerhalb der Anordnung der Samples erlernt werden. Als letzen Argument wird noch
der Testdatensatz angegeben.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  autoencoder.fit(x=x_train_noisy, y=x_train, batch_size=128, epochs=100, shuffle=True,
    validation_data=(x_test_noisy, x_test))
\end{minted}
Sofern TensorFlow eine geeignete Nvidia-Grafikkarte vorfindet und auch die
CUDA-Version von TF installiert ist, erfolgt das Training mithilfe von GPU-Hardwarebeschleunigung.
Dadurch sollte das Trainings innherhalb einiger Minuten abgeschlossen sein.
Während des Trainings schreibt TensorFlow Informationen zum aktuellen Fortschritt in die
Kommandozeile. Diese Infos weisen nachstehenden Charakter auf und geben Auskunft über die aktuelle Epoche und die
Werte der Kostenfunktion. Die Kosten für den Trainingsdatensatz werden mit
\code{loss} bezeichnet und die für den Testdatensatz mit \code{val\_loss}.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{text}
  Epoch 3/100
  60000/60000 [==============================] - 4s 60us/sample - loss: 0.0911 - val_loss: 0.0772
\end{minted}
Das Kostenfunktion hat nach dem Training einene Wert \code{val\_los} von $0.0180$,
bezüglich dem Testdatensatz.
\para{}
Jetzt, da das Modell trainiert wurde, ist es sinnvoll es auf dem Computer als
Modelldatei abzuspeichern.
So muss das es nicht jedesmal wieder neu trainieren werden, sondern kann über
die Modelldatei eingelesen werden. Für dieses Einlesen, wird folgender Code benötigt.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  autoencoder.save('denoiser.model')
\end{minted}
Um das Modell wieder einzulesen verwendet man folgenden Code:
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  autoencoder = tf.keras.models.load_model('denoiser.model')
\end{minted}
Mit der Summary-Funktion kann man überprüfen, ob das richtige Modell geladen wurde.

\subsection{Modell ausführen}
Da das Modell nun trainiert ist, kann es nun für seinen eigentlichen Zweck
genutzt werden.
Dieser besteht darin, Bilder zu entrauschen. Dafür gelangen die
Bilder aus dem Testdatensatz zur Anwendung. Damit ist garantiert, dass diese
unbenutzten Daten vom Modell nicht auswenig gelernt werden konnten.
Mit der Funktion \code{tf.keras.model.predict}
erhält man alle Vorhersagen des Autoencoders zum gesamten Testdatensatz.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  denoised_imgs = autoencoder.predict(x_test)
\end{minted}
Mithilfe von der Matplotlib können die entrauschten Bilder mit den
ursprünglichen MNIST-Bilder direkt verglichen werden.
So kann mit blossem Auge beurteilt werden, wie gut die Ergebnisse tatsächlich sind.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  n = 10 # jeweils 10 Bilder
  plt.figure()
  for i in range(n):
    # verrauschte Bilder
    ax = plt.subplot(3, n, 1+i)
    plt.imshow(x_test_noisy[i].reshape(28, 28))
    plt.gray()

    # entrauschte Bilder
    ax = plt.subplot(3, n, 1+n+i)
    plt.imshow(decoded_imgs[i].reshape(28, 28))
    plt.gray()

    # Original-Bilder
    ax = plt.subplot(3, n, 1+2*n+i)
    plt.imshow(x_test[i].reshape(28, 28))
    plt.gray()
  plt.show()
\end{minted}
In der ersten Zeile der Grafik sind die verrauschten Bilder dargestellt.
Darunter folgen die Rekonstuktionen als Ergebniss des Modells und ganz unten
sind die Original-Bilder zu sehen.
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\textwidth]{denoised.png}
  \caption{Verrauschte Bilder, Rekonstruktionen und Orignalbilder}
\end{figure}
Wirkliche Unterschiede zwischen der zweiten und der dritten Zeile sind kaum zu erkennen.

\subsection{Hyperparameter einstellen}
Um die Hyperparameter einzustellen, müssen lediglich verschiedene Werte
ausprobiert werden. Die Applikation wird insofern modifiziert, als dass
verschiedene Modelle konstruiert werden, welche unterschiedliche
Hyperparametern verwenden.
Beim Training werden mithilfe von TensorBoard
Daten zum Lernfortschritt und den Kosten der verschiedenen Modelle erfasst. Auf
diese Weise kann eine Auswertung erstellt werden, um die besten Hyperparameter
zu identifizieren.
\para{}
Zuerst werden einige Arrays definiert, welche die zu testenden Hyperparameter beinhalten.
Ein Hyperparameter ist die Anzahl Epochen. Diesen werden wir aber nicht testen,
da es offensichtlich ist, dass mit einer längeren Trainingszeit sich auch die
Resultate verbessern.
Alle Modelle werden für 20 Epochen traniert.
\para{}
Folgende Mini-Batch-Grössen sollen ausprobiert werden: 16, 32, 64, 128, 265.
Die Vermutung liegt nahe, dass mit kleineren Mini-Batches die
Gradientapproximationen besser mit dem reellen Gradienten übereinstimmt und
damit die Resultate besser werden. \\
Als zu testende Anzahl von Filtern werden definiert: 16, 32, 64, 128, 256.
Für die Aktivierungsfunktionen werden Sigmoid und ReLU ausprobiert und gelangen
für das ganze Modell zur Anwendung.
Jedoch wird für die letzte Schicht immer die
Sigmoid-Aktivierungsfunktion verwendet, damit die Outputs im Intervall $[0,1]$
liegen.

\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  EPOCHS = 20

  BATCH_SIZES = [ 16, 32, 64, 128, 256 ]
  FILTER_NUMS = [ 16, 32, 64, 128, 256 ]
  ACTIVATIONFUNCTIONS = [ 'sigmoid', 'relu' ]
\end{minted}
Nun werden wir den gesamten Code zum Bauen des Graphens, zur Kompilierung und
zum Training des Modells in verschachtele For-Loops stecken, welche alle
Kombinationen an Hyperparameter ausprobiert.

\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  for activation_function in ACTIVATIONFUNCTIONS:
    for batch_size in BATCH_SIZES:
      for num_filters in FILTER_NUMS:
        # ganzer Modell-Code
\end{minted}
Nun muss der Code zur Definition des Modells insofern modiziert werden, dass
die Variabeln \code{activation\_function}, \code{batch\_size} und
\code{num\_filters} an den entsprechenden Stellen anstatt der Konstanten benutzt werden.

\subsubsection{TensorBoard konfigurieren}
Um den Lernprozess und die Leistungen des Modells zu analysieren,
wird nun nun TensorBoard konfiguriert.
Dafür müssen die Methoden \code{compile} und \code{fit} angepasst werden.
Zuerst müssen wir in der \code{compile} Methode die zu erfassenden Metriken
angeben. Von Interesse ist die \code{'accuracy'}.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  autoencoder.compile(optimizer='sgd', loss='mean_squared_error', metrics=['accuracy'])
\end{minted}
Nun muss eine TensorFlow-Variable definiert werden. Hierbei ist anzugeben, wo
die erfassten Daten gespeichert werden sollen. Dabei ist es sinnvoll den Dateien
der einzelnen Modelle aussagekräftige Namen zu geben. Deshalb benennen wir die
Dateien in folgendem Format: ``denoiser-(Aktivierungsfunktion)-(Mini-Batch
Grösse)-batches-(Anzahl Filter)-filters''.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  NAME = 'denoiser-{}-{}-batches-{}-filters'.format(activation_function, batch_size, num_filters)
  tensorboard = tf.keras.callbacks.TensorBoard(log_dir='logs/{}'.format(NAME))
\end{minted}
Zu letzt muss noch die TensorBoard-Variable als \code{callback} in der
\code{fit}-Methode spezifiziert werden.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{python}
  autoencoder.fit(x=x_train_noisy, y=x_train, batch_size=batch_size, epochs=EPOCHS, shuffle=True,
    validation_data=(x_test_noisy,x_test), callbacks=[tensorboard])
\end{minted}

\subsubsection{TensorBoard-Analyse}
Um nun die besten Hyperparameter mithilfe von TensorBoard herauszufinden, muss
zuerst der TensorBoard-Webserver gestartet werden. Dafür navigiert man zuerst
mit der Kommandozeile in das Verzeichniss, welches die Logdateien enthält.
Dort kann dann der Webserver mithilfe folgendem Kommando gestartet werden.
\begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,bgcolor=lightgray,fontsize=\footnotesize,linenos]{text}
tensorboard --logdir .
\end{minted}
Nun kann mit einem WebBrowser zur angegebenen Webadresse navigieren. Meistens
ist diese: \code{http://localhost:6006/}.
Der Graph, welcher die Kosten bezüglich dem Testdatensatz aufzeichnet ist von
Interesse. Dieser sieht folgendermassen aus:
\para{}
\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{tb_results.png}
  \caption{Kosten bezüglich dem Testdatensatz}
\end{figure}
\para{}
Dank dem interaktivem Interface, kann dem Graphen entnommen werden, das folgende
Modelle mit den angegebenen Hyperparametern, die besten Resultate liefern:

\para{}
\begin{table}[h!]
  \centering
  \begin{tabular}{ |c|c|c|c|c| }
    \hline
    Rang & Aktivierungsfunktion & Minibatch-Grösse & Anzahl Filter & Kosten \\
    \hline
    1 & ReLU & 16 & 265 & 0.015 \\
    2 & ReLU & 16 & 128 & 0.01521 \\
    3 & ReLU & 16 & 64 & 0.01588 \\
    4 & ReLU & 16 & 32 & 0.0167 \\
    $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
    50 & Sigmoid & 256 & 16 & 0.09438 \\
    \hline
  \end{tabular}
  \caption{Rangliste der besten Modelle mit ihren Hyperparametern}
\end{table}
\para{}


\subsection{Diskussion des Modells}
2 bis drei Seiten.


Diese Ergebnisse stimmen ziemlich gut mit den Erwartungen überein. Wie bereits
in der Theorie behauptet, liefern die ReLU-Aktivierungsfunktion bessere Ergebnisse
in CNNs als die Sigmoid-Aktivierungsfunktion. Deshalb finden sich auf den ersten
Plätzen nur Modelle mit der ReLU-Aktivierungsfunktion. \\
Auch, dass die kleinste Minibatch-Grösse die besten Resultate liefert ist
naheliegend. Durch die kleine Grösse sind die Fehler bei der Approximation des
wahren Gradienten kleiner als bei grossen Batches. \\
Die Anzahl Filter haben den geringsten Einfluss, jedoch gibt es trotzdem
minimale Vorteile von einer grösseren Anzahl davon.


Auf Modell selber bezogen:
Echt gutes Modell.
\subsubsection{Maximale Leistung}
Wie hoch kann man den Noise-Factor stellen?

\subsubsection{Mögliche Verbesserungen}
Es gibt konkrete Verbesserungsmöglichkeiten, welche nicht umgesetzt wurden bei
der Entwicklung des Denoisers. Es wäre vermutlich möglich gewesen bessere
Resulate zu erziehlen, wenn verschiedene Topologien durchprobiert werden wären.
Denn die Anzahl Zwischenschichten stellt auch einen Hyperparameter dar. Jedoch
hätte dies das Training noch länger gemacht. \\
Eine Modifikation, welche wahrscheinlich das Modell deutlich verbessert hätte,
wäre eine alternative Kostenfunktion gewesen: die Kreuzentropy-Kostenfunktion.
Diese lässt die Neuronen eines Netzes weniger schnell saturieren und erzeugt so
bessere Resultate.
\para{}
Es ist darauf hinzuweisen, dass das hier konstruierte Modell bei weitem nicht
das eigentliche Potenzial von TensorFlow und Keras ausschöpft.


%%% Local Variables:
%%% TeX-command-extra-options: "--shell-escape"
%%% TeX-master: "../main"
%%% End:

% LocalWords:  gelabelt Unüberwachtes unsupervised Inputdaten Grossteil Vector
% LocalWords:  unüberwachtem Hauptkomponentenanalysen Adversial Networks bzw
% LocalWords:  Trainingssample Inputsvektor Labelvektor Overfitting Machines
% LocalWords:  Trainingsdaten Testdatensatz Trainingsdatensatz gewissermassen
% LocalWords:  Hypothesenfunktion Klassifizierungsprobleme Regressionsprobleme
% LocalWords:  Regressionsmodell Regressionsproblem Modellparameter Machine
% LocalWords:  Hyperparameter Regressionsgerade Kostenfunktionen BEISPIELMODELL
% LocalWords:  Verlustfunktionen Verlustfunktion Label Kostenfunktion Ouputs
